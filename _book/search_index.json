[["index.html", "Landscape Genetic Data Analysis with R 1 Introduction", " Landscape Genetic Data Analysis with R Editor: Helene Wagner (University of Toronto) 2026-01-21 1 Introduction This is a web-interface to the teaching materials for the lab course ‘Landscape Genetic Data Analysis with R’ associated with the distributed graduate course ‘DGS Landscape Genetics’. The Landscape Genetics Distributed Graduate Seminar (DGS) is an international collaboration that provides a unique opportunity for interdisciplinary graduate training. The course draws on experts from around the world to deliver an in-depth introduction and overview of the field of landscape genetics. The course caters to students in both basic and applied ecology, conservation/population genetics, landscape ecology and conservation biology. Every other year, several hundred students, post-docs and faculty from around the world participate in this course. For more information about the course, and to sign up to be notified when registration opens, please visit the course website: Link to DGS Landscape Genetics The online book compiles the teaching materials for the optional computer lab component of the DGS Landscape Genetics. The materials included in this online resource are also included in the R package LandGenCourse available on Github. Next sections: How to use this Book List of R Packages by Vignette License: Landscape Genetic Data Analysis with R by Helene Wagner is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],["how-to-use-this-book.html", "1.1 How to use this Book", " 1.1 How to use this Book 1. Book Structure This book has weekly chapters that correspond to course modules, with three parts: a) Getting Started Review of R Skills: check whether you need to build or brush up your R skills before starting the course: Basic R Programming: introduction to R objects and functions. Prerequisite. R Graphics: learn to create figures with base R and with ggplot2. Optional. b) Basic Topics These 8 weekly modules are streamlined to build the necessary R skills and brush up statistics knowledge. Best complete these in sequence. Each module has the following components: Video: introduces the R and stats topics Interactive Tutorial: swirl course to practice R programming Worked Example: worked example by the weekly experts from the ‘DGS Landscape Genetics’ course. Bonus Materials: some weeks include bonus vignettes with optional advanced topics. c) Advanced Topics These weekly modules build on the skills developed in Basic Topics. You may pick and choose from the Advanced Topics according to your interests. Each weekly modules contains: Worked Example: worked example by the weekly experts from the ‘DGS Landscape Genetics’ course. Bonus Materials: some weeks include bonus vignettes with optional advanced topics. 2. Find what is relevant for you a) How to use the labs Weeks 1 - 8: Beginners: watch video do tutorial to build basic R programming skills read worked example, focus on content Intermediate: watch video check weekly tutorial functions: do tutorial to brush up? understand worked example, focus on content and code do R exercise to practice coding Advanced: check video slides: watch video to brush up on concepts? R exercise? adapt worked example to own data By Week 9: you will be at least at ‘intermediate’ level if not a pro! b) Check contents Videos: Check first weekly slide, browse slides for each week Interactive tutorials: check weekly list of commands for each chapter Worked examples: check List of R Packages by Vignette; check introduction section for each worked example (or bonus vignette) 3. Course R package ‘LandGenCourse’ a) Practice good R hygiene Please update or install the following before installing ‘LandGenCourse’: Windows: R: https://cran.r-project.org/ RStudio: https://www.rstudio.com/products/rstudio/download/#download If asked to install Rtools during package installation: accept Mac: please follow the order below exactly! Xquartz: https://www.xquartz.org/ (re-install after each MacOS upgrade) R: https://cran.r-project.org/ RStudio: https://www.rstudio.com/products/rstudio/download/#download If asked to install Command Line Tools during package installation: accept If asked which packages to install, select ‘All’. If needed, manually install Command Line Tools. Instructions: http://osxdaily.com/2014/02/12/install-command-line-tools-mac-os-x/ b) How to install (or update) ‘LandGenCourse’ if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;hhwagner1/LandGenCourse&quot;) library(LandGenCourse) Hints: Make sure you load the package (library(LandGenCourse)) after installing it! Check whether the add-ins 0 - 4 are available: in the RStudio menu, click on the drop-down menu Addins. If the add-ins 0 - 4 don’t appear, rerun the code above (install and load package LandGenCourse). Sometimes this seems to be necessary. c) How to install dependencies The previous step only installed the absolutely required dependencies (i.e., R packages that LandGenCourse needs so that it can be installed). However, the weekly chapters will require additional R packages. There are two ways to install them: Recommended: Bulk install Use the Addin 0. Install Packages to bulk install most or all packages needed to run the worked examples. Simply click on the add-in 0. Install Packages (alternatively, you could run this code: installDGS()) If asked whether to install some package from source (Do you want to install from sources the package which needs compilation? (Yes/no/cancel)), you may enter: no (this is faster) If the installation stops with an error API rate limit exceeded, the best thing to do is to wait 30 min and run the Addin 0. Install Packages again. Don’t worry, this won’t start the installation process again, it will resume where it broke off. If you often install packages from Github, you may want to follow up on the advice printed with the error message: Use usethis::create_github_token() to create a Personal Access Token. Use usethis::edit_r_environ() and add the token as GITHUB_PAT. If you do so, the second line above will open the file .Renviron in RStudio. Enter GITHUB_PAT=\"myPAT\" on the last line (replace myPAT by the PAT you generated using the first line). Make sure to include an empty line after it! Save the file and restart R. Alternative: Install as you go This option is not suitable if you are planning to do the interactive swirl tutorials! Use bulk installation instead (see above). Newer versions of RStudio will warn you when a R notebook (.Rmd) that you opened requires R packages that are not installed. The warning contains a link Install, click on it to install the missing packages. d) How to use the R package LandGenCourse The package installs four more Addins in RStudio. Each will provide you with some dropdown menu choices. 1. Watch Course Video: opens a video resource from course “Landscape Genetic Data Analysis with R”. 2. Start Tutorial: installs swirl course “Landscape_Genetics_R_Course” and prints instructions. 3. Choose Worked Example: opens vignette file (.html, .Rmd, or .R) with a worked example from course “Landscape Genetic Data Analysis with R”. 4. Open Cheat Sheet: opens selected R cheat sheet. e) Video instructions for beginners This video walks through the process of installing devtools, the course package, and using the RStudio Add-Ins. Intro_LandGenCourse_small.mp4 "],["list-of-packages.html", "1.2 List of R Packages by Vignette", " 1.2 List of R Packages by Vignette Scroll through the list to find which R packages are used in which vignette (i.e., weekly worked example or bonus vignette). This may be helpful for finding relevant materials if you are interested in a specific type of analysis. Notes: ‘B’ and ‘G’ refer to the Review of R Skills section (B = Basic R, G = Graphics) Lowercase letters refer to bonus vignettes. E.g., 2a is the (first) bonus vignette of Week 2. B G 1 2 2a 3 4 5 6 7 7a 8 8a 9 10 10a 11 12 13 14 ade4 X X adegenet X X X car X X corMLPE X X cowplot X X data.table X X dplyr X X X X X X X e1071 X EcoGenetics X X effsize X fields X foreach X gdistance X GeNetIt X ggeffects X ggplot2 X X X X X X gridExtra X gstudio X X X X X here X X X X hierfstat X igraph X knitr X X landscapemetrics X lattice X LEA X lfmm X mapplots X MASS X microbenchmark X mmod X X MuMIn X X X nlme X X X pegas X PopGenReport X X X poppr X X predictmeans X X profvis X purrr X X pwr X QstFstComp X qvalue X radish X raster X X RColorBrewer X X X readr X rio X secr X sf X X X X X X X X sfnetworks X sp X spatialEco X X spatialreg X spdep X X X X spmoran X terra X X X tmap X X X X X X vcfR X vegan X X X "],["review-of-r-skills.html", "2 Review of R Skills", " 2 Review of R Skills This part contains two optional vignettes that will help you brush up your basic R programming skills and get a head-start on R graphics. Basic R Programming R Graphics Further R Resources "],["basic-r.html", "2.1 Basic R Programming", " 2.1 Basic R Programming Rodney Dyer (worked example) and Helene Wagner (adaptation) 1. Overview This worked example is adapted from “Applied Population Genetics” by Rodney Dyer. The entire book is available here: http://dyerlab.github.io/applied_population_genetics/index.html R is a statistical programming language, and it thus requires users to work with code. This can be intimidating at first. Working through this document will help you get up to speed with basic R concepts and notation. Whether you are new to R or need a refresher, this worked example will get you to the level expected for the ‘Landscape Genetics with R’ lab course. The main topics covered here are: R data types (how data are stored in R: numeric, character, etc.) R containers (how data are organized: vectors, data frames, etc.) R functions (how to tell R what to do) See also video “Week 0: Intro to R Notebooks” for options how to work this this document as .html or .Rmd. Install packages needed for this worked example. Note: popgraph needs to be installed before installing gstudio. if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) { install.packages(c(&quot;RgoogleMaps&quot;, &quot;geosphere&quot;, &quot;proto&quot;, &quot;sampling&quot;, &quot;seqinr&quot;, &quot;spacetime&quot;, &quot;spdep&quot;), dependencies=TRUE) remotes::install_github(&quot;dyerlab/popgraph&quot;) } if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) 2. Data Types The data we work with comes in many forms—integers, stratum, categories, genotypes, etc.—all of which we need to be able to work with in our analyses. In this chapter, the basic data types we will commonly use in population genetic analyses. This section covers some of the basic types of data we will use in R. These include numbers, character, factors, and logical data types. We will also introduce the locus object from the gstudio library and see how it is just another data type that we can manipulate in R. The very first hurdle you need to get over is the oddness in the way in which R assigns values to variables. variable &lt;- value Yes that is a less-than and dash character. This is the assignment operator that historically has been used and it is the one that I will stick with. In some cases you can use the ‘=’ to assign variables instead but then it takes away the R-ness of R itself. For decision making, the equality operator (e.g., is this equal to that) is the double equals sign ‘==’. We will get into that below where we talk about logical types and later in decision making. If you are unaware of what type a particular variable may be, you can always use the type() function and R will tell you. class( variable ) R also has a pretty good help system built into itself. You can get help for any function by typing a question mark in front of the function name. This is a particularly awesome features because at the end of the help file, there is often examples of its usage, which are priceless. Here is the documentation for the ‘help’ function as given by: ?help There are also package vignettes available (for most packages you download) that provide additional information on the routines, data sets, and other items included in these packages. You can get a list of vignettes currently installed on your machine by: vignette() and vignettes for a particular package by passing the package name as an argument to the function itself. 2.1. Numeric Data Types a. Numeric data The quantitative measurements we make are often numeric, in that they can be represented as as a number with a decimal component (think weight, height, latitude, soil moisture, ear wax viscosity, etc.). The most basic type of data in R, is the numeric type and represents both integers and floating point numbers (n.b., there is a strict integer data type but it is often only needed when interfacing with other C libraries and can for what we are doing be disregarded). Assigning a value to a variable is easy x &lt;- 3 x ## [1] 3 By default, R automatically outputs whole numbers numbers within decimal values appropriately. y &lt;- 22/7 y ## [1] 3.142857 If there is a mix of whole numbers and numbers with decimals together in a container such as c(x,y) ## [1] 3.000000 3.142857 then both are shown with decimals. The c() part here is a function that combines several data objects together into a vector and is very useful. In fact, the use of vectors are are central to working in R and functions almost all the functions we use on individual variables can also be applied to vectors. A word of caution should be made about numeric data types on any computer. Consider the following example. x &lt;- .3 / 3 x ## [1] 0.1 which is exactly what we’d expect. However, the way in which computers store decimal numbers plays off our notion of significant digits pretty well. Look what happens when I print out x but carry out the number of decimal places. print(x, digits=20) ## [1] 0.099999999999999991673 Not quite 0.1 is it? Not that far away from it but not exact. That is a general problem, not one that R has any more claim to than any other language and/or implementation. Does this matter much, probably not in the realm of the kinds of things we do in population genetics, it is just something that you should be aware of. You can make random sets of numeric data by using using functions describing various distributions. For example, some random numbers from the normal distribution are: rnorm(10) ## [1] -0.79157820 0.64983440 -1.32962235 -0.50246365 1.08349866 0.12816213 ## [7] 0.88600990 0.92590182 0.01727144 -0.51566101 from the normal distribution with designated mean and standard deviation: rnorm(10,mean=42,sd=12) ## [1] 38.58979 38.13407 60.11725 48.01066 42.69456 48.54586 53.59720 45.01358 ## [9] 45.73550 40.32697 A poisson distribution with mean 2: rpois(10,lambda = 2) ## [1] 6 3 2 1 2 5 1 2 1 2 and the \\(\\chi^2\\) distribution with 1 degree of freedom: rchisq(10, df=1) ## [1] 0.01094670 0.00732484 0.14994298 2.10658821 0.07749988 0.43336323 ## [7] 3.72981901 1.53207955 0.49904789 0.14133199 There are several more distributions that if you need to access random numbers, quantiles, probability densities, and cumulative density values are available. b. Coercion to Numeric All data types have the potential ability to take another variable and coerce it into their type. Some combinations make sense, and some do not. For example, if you load in a CSV data file using read_csv(), and at some point a stray non-numeric character was inserted into one of the cells on your spreadsheet, R will interpret the entire column as a character type rather than as a numeric type. This can be a very frustrating thing, spreadsheets should generally be considered evil as they do all kinds of stuff behind the scenes and make your life less awesome. Here is an example of coercion of some data that is initially defined as a set of characters x &lt;- c(&quot;42&quot;,&quot;99&quot;) x ## [1] &quot;42&quot; &quot;99&quot; and is coerced into a numeric type using the as.numeric() function. y &lt;- as.numeric( x ) y ## [1] 42 99 It is a built-in feature of the data types in R that they all have (or should have if someone is producing a new data type and is being courteous to their users) an as.X() function. This is where the data type decides if the values asked to be coerced are reasonable or if you need to be reminded that what you are asking is not possible. Here is an example where I try to coerce a non-numeric variable into a number. x &lt;- &quot;The night is dark and full of terrors...&quot; as.numeric( x ) ## Warning: NAs introduced by coercion ## [1] NA By default, the result should be NA (missing data/non-applicable) if you ask for things that are not possible. 2.2. Characters a. Character data A collection of letters, number, and or punctuation is represented as a character data type. These are enclosed in either single or double quotes and are considered a single entity. For example, my name can be represented as: prof &lt;- &quot;Rodney J. Dyer&quot; prof ## [1] &quot;Rodney J. Dyer&quot; In R, character variables are considered to be a single entity, that is the entire prof variable is a single unit, not a collection of characters. This is in part due to the way in which vectors of variables are constructed in the language. For example, if you are looking at the length of the variable I assigned my name to you see length(prof) ## [1] 1 which shows that there is only one ‘character’ variable. If, as is often the case, you are interested in knowing how many characters are in the variable prof, then you use the nchar(prof) ## [1] 14 function instead. This returns the number of characters (even the non-printing ones like tabs and spaces. nchar(&quot; \\t &quot;) ## [1] 3 As all other data types, you can define a vector of character values using the c() function. x &lt;- &quot;I am&quot; y &lt;- &quot;not&quot; z &lt;- &#39;a looser&#39; terms &lt;- c(x,y,z) terms ## [1] &quot;I am&quot; &quot;not&quot; &quot;a looser&quot; And looking at the length() and nchar() of this you can see how these operations differ. length(terms) ## [1] 3 nchar(terms) ## [1] 4 3 8 b. Concatenation of Characters Another common use of characters is concatenating them into single sequences. Here we use the function paste() and can set the separators (or characters that are inserted between entities when we collapse vectors). Here is an example, entirely fictional and only provided for instructional purposes only. paste(terms, collapse=&quot; &quot;) ## [1] &quot;I am not a looser&quot; paste(x,z) ## [1] &quot;I am a looser&quot; paste(x,z,sep=&quot; not &quot;) ## [1] &quot;I am not a looser&quot; c. Coercion to Characters A character data type is often the most basal type of data you can work with. For example, consider the case where you have named sample locations. These can be kept as a character data type or as a factor (see below). There are benefits and drawbacks to each representation of the same data (see below). By default (as of the version of R I am currently using when writing this book), if you use a function like read_table() to load in an external file, columns of character data will be treated as factors. This can be good behavior if all you are doing is loading in data and running an analysis, or it can be a total pain in the backside if you are doing more manipulative analyses. Here is an example of coercing a numeric type into a character type using the as.character() function. x &lt;- 42 x ## [1] 42 y &lt;- as.character(x) y ## [1] &quot;42&quot; 2.3. Factors a. Factor vs. Character A factor is a categorical data type. If you are coming from SAS, these are class variables. If you are not, then perhaps you can think of them as mutually exclusive classifications. For example, an sample may be assigned to one particular locale, one particular region, and one particular species. Across all the data you may have several species, regions, and locales. These are finite, and defined, sets of categories. One of the more common headaches encountered by people new to R is working with factor types and trying to add categories that are not already defined. Since factors are categorical, it is in your best interest to make sure you label them in as descriptive as a fashion as possible. You are not saving space or cutting down on computational time to take shortcuts and label the locale for Rancho Santa Maria as RSN or pop3d or 5. Our computers are fast and large enough, and our programmers are cleaver enough, to not have to rename our populations in numeric format to make them work (hello STRUCTURE I’m calling you out here). The only thing you have to loose by adopting a reasonable naming scheme is confusion in your output. To define a factor type, you use the function factor() and pass it a vector of values. region &lt;- c(&quot;North&quot;,&quot;North&quot;,&quot;South&quot;,&quot;East&quot;,&quot;East&quot;,&quot;South&quot;,&quot;West&quot;,&quot;West&quot;,&quot;West&quot;) region &lt;- factor( region ) region ## [1] North North South East East South West West West ## Levels: East North South West When you print out the values, it shows you all the levels present for the factor. If you have levels that are not present in your data set, when you define it, you can tell R to consider additional levels of this factor by passing the optional levels= argument as: region &lt;- factor( region, levels=c(&quot;North&quot;,&quot;South&quot;,&quot;East&quot;,&quot;West&quot;,&quot;Central&quot;)) region ## [1] North North South East East South West West West ## Levels: North South East West Central If you try to add a data point to a factor list that does not have the factor that you are adding, it will give you an error (or ‘barf’ as I like to say). region[1] &lt;- &quot;Bob&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 1, value = &quot;Bob&quot;): invalid factor level, NA ## generated Now, I have to admit that the Error message in its entirety, with its “[&lt;-.factor(*tmp*, 1, value = “Bob”)“` part is, perhaps, not the most informative. Agreed. However, the “invalid factor level” does tell you something useful. Unfortunately, the programmers that put in the error handling system in R did not quite adhere to the spirit of the “fail loudly” mantra. It is something you will have to get good at. Google is your friend, and if you post a questions to (http://stackoverflow.org) or the R user list without doing serious homework, put on your asbestos shorts! Unfortunately, the error above changed the first element of the region vector to NA (missing data). I’ll turn it back before we move too much further. region[1] &lt;- &quot;North&quot; Factors in R can be either unordered (as say locale may be since locale A is not &gt;, =, or &lt; locale B) or they may be ordered categories as in Small &lt; Medium &lt; Large &lt; X-Large. When you create the factor, you need to indicate if it is an ordered type (by default it is not). If the factors are ordered in some way, you can also create an ordination on the data. If you do not pass a levels= option to the factors() function, it will take the order in which they occur in data you pass to it. If you want to specify an order for the factors specifically, pass the optional levels= and they will be ordinated in the order given there. region &lt;- factor( region, ordered=TRUE, levels = c(&quot;West&quot;, &quot;North&quot;, &quot;South&quot;, &quot;East&quot;) ) region ## [1] North North South East East South West West West ## Levels: West &lt; North &lt; South &lt; East b. Missing Levels in Factors There are times when you have a subset of data that do not have all the potential categories. subregion &lt;- region[ 3:9 ] subregion ## [1] South East East South West West West ## Levels: West &lt; North &lt; South &lt; East table( subregion ) ## subregion ## West North South East ## 3 0 2 2 2.4. Logical Types A logical type is either TRUE or FALSE, there is no in-between. It is common to use these types in making decisions (see if-else decisions) to check a specific condition being satisfied. To define logical variables you can either use the TRUE or FALSE directly canThrow &lt;- c(FALSE, TRUE, FALSE, FALSE, FALSE) canThrow ## [1] FALSE TRUE FALSE FALSE FALSE or can implement some logical condition stable &lt;- c( &quot;RGIII&quot; == 0, nchar(&quot;Marshawn&quot;) == 8) stable ## [1] FALSE TRUE on the variables. Notice here how each of the items is actually evaluated as to determine the truth of each expression. In the first case, the character is not equal to zero and in the second, the number of characters (what nchar() does) is indeed equal to 8 for the character string “Marshawn”. It is common to use logical types to serve as indices for vectors. Say for example, you have a vector of data that you want to select some subset from. data &lt;- rnorm(20) data ## [1] -0.9348844 0.8203106 -1.6352834 0.1502384 0.6057883 -0.4257914 ## [7] 2.1458429 -2.0645585 0.4874247 -0.5602449 -0.6403197 -2.4885485 ## [13] 1.3607995 -0.5153664 0.7724645 1.0403607 -0.3148372 0.6555180 ## [19] -1.5640644 -0.1097257 Perhaps you are on interested in the non-negative values data[ data &gt; 0 ] ## [1] 0.8203106 0.1502384 0.6057883 2.1458429 0.4874247 1.3607995 0.7724645 ## [8] 1.0403607 0.6555180 If you look at the condition being passed to as the index data &gt; 0 ## [1] FALSE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE FALSE FALSE FALSE ## [13] TRUE FALSE TRUE TRUE FALSE TRUE FALSE FALSE you see that individually, each value in the data vector is being evaluated as a logical value, satisfying the condition that it is strictly greater than zero. When you pass that as indices to a vector it only shows the indices that are TRUE. You can coerce a value into a logical if you understand the rules. Numeric types that equal 0 (zero) are FALSE, always. Any non-zero value is considered TRUE. Here I use the modulus operator, %%, which provides the remainder of a division. 1:20 %% 2 ## [1] 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 which used as indices give us data[ (1:20 %% 2) &gt; 0 ] ## [1] -0.9348844 -1.6352834 0.6057883 2.1458429 0.4874247 -0.6403197 ## [7] 1.3607995 0.7724645 -0.3148372 -1.5640644 You can get as complicated in the creation of indices as you like, even using logical operators such as OR and AND. I leave that as an example for you to play with. 3. Data Containers We almost never work with a single datum1, rather we keep lots of data. Moreover, the kinds of data are often heterogeneous, including categorical (Populations, Regions), continuous (coordinates, rainfall, elevation), imagry (hyperspectral, LiDAR), and perhaps even genetic. R has a very rich set of containers into which we can stuff our data as we work with it. Here these container types are examined and the restrictions and benefits associated with each type are explained. 3.1. Vectors We have already seen several examples of several vectors in action (see the introduction to Numeric data types for example). A vector of objects is simply a collection of them, often created using the c() function (c for combine). Vectorized data is restricted to having homogeneous data types—you cannot mix character and numeric types in the same vector. If you try to mix types, R will either coerce your data into a reasonable type x &lt;- c(1,2,3) x ## [1] 1 2 3 y &lt;- c(TRUE,TRUE,FALSE) y ## [1] TRUE TRUE FALSE z &lt;- c(&quot;I&quot;,&quot;am&quot;,&quot;not&quot;,&quot;a&quot;,&quot;looser&quot;) z ## [1] &quot;I&quot; &quot;am&quot; &quot;not&quot; &quot;a&quot; &quot;looser&quot; or coearce them into one type that is amenable to all the types of data that you have given it. In this example, a Logical, Character, Constant, and Function are combined resulting in a vector output of type Character. w &lt;- c(TRUE, &quot;1&quot;, pi, ls()) w ## [1] &quot;TRUE&quot; &quot;1&quot; &quot;3.14159265358979&quot; &quot;canThrow&quot; ## [5] &quot;data&quot; &quot;prof&quot; &quot;region&quot; &quot;rmd_file&quot; ## [9] &quot;stable&quot; &quot;subregion&quot; &quot;terms&quot; &quot;x&quot; ## [13] &quot;y&quot; &quot;yml_metadata&quot; &quot;z&quot; class(w) ## [1] &quot;character&quot; Accessing elements within a vector are done using the square bracket [] notation. All indices (for vectors and matrices) start at 1 (not zero as is the case for some languages). Getting and setting the components within a vector are accomplished using numeric indices with the assignment operators just like we do for variables containing a single value. x ## [1] 1 2 3 x[1] &lt;- 2 x[3] &lt;- 1 x ## [1] 2 2 1 x[2] ## [1] 2 A common type of vector is that of a sequences. We use sequences all the time, to iterate through a list, to counting generations, etc. There are a few ways to generate sequences, depending upon the step sequence. For a sequence of whole numbers, the easiest is through the use of the colon operator. x &lt;- 1:6 x ## [1] 1 2 3 4 5 6 This provides a nice shorthand for getting the values X:Y from X to Y, inclusive. It is also possible to go backwards using this operator, counting down from X to Y as in: x &lt;- 5:2 x ## [1] 5 4 3 2 The only constraint here is that we are limited to a step size of 1.0. It is possible to use non-integers as the bounds, it will just count up by 1.0 each time. x &lt;- 3.2:8.4 x ## [1] 3.2 4.2 5.2 6.2 7.2 8.2 If you are interested in making a sequence with a step other than 1.0, you can use the seq() function. If you do not provide a step value, it defaults to 1.0. y &lt;- seq(1,6) y ## [1] 1 2 3 4 5 6 But if you do, it will use that instead. z &lt;- seq(1,20,by=2) z ## [1] 1 3 5 7 9 11 13 15 17 19 It is also possible to create a vector of objects as repetitions using the rep() (for repeat) function. rep(&quot;Beetlejuice&quot;,3) ## [1] &quot;Beetlejuice&quot; &quot;Beetlejuice&quot; &quot;Beetlejuice&quot; If you pass a vector of items to rep(), it can repeat these as either a vector being repeated (the default value) x &lt;- c(&quot;No&quot;,&quot;Free&quot;,&quot;Lunch&quot;) rep(x,time=3) ## [1] &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; or as each item in the vector repeated. rep(x,each=3) ## [1] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Free&quot; &quot;Free&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;Lunch&quot; &quot;Lunch&quot; 3.2. Matrices A matrix is a 2- or higher dimensional container, most commonly used to store numeric data types. There are some libraries that use matrices in more than two dimensions (rows and columns and sheets), though you will not run across them too often. Here I restrict myself to only 2-dimensional matrices. You can define a matrix by giving it a set of values and an indication of the number of rows and columns you want. The easiest matrix to try is one with empty values: matrix(nrow=2, ncol=2) ## [,1] [,2] ## [1,] NA NA ## [2,] NA NA Perhaps more useful is one that is pre-populated with values. matrix(1:4, nrow=2 ) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Notice that here, there were four entries and I only specified the number of rows required. By default the ‘filling-in’ of the matrix will proceed down column (by-column). In this example, we have the first column with the first two entries and the last two entries down the second column. If you want it to fill by row, you can pass the optional argument matrix(1:4, nrow=2, byrow=TRUE) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 and it will fill by-row. When filling matrices, the default size and the size of the data being added to the matrix are critical. For example, I can create a matrix as: Y &lt;- matrix(c(1,2,3,4,5,6),ncol=2,byrow=TRUE) Y ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 or X &lt;- matrix(c(1,2,3,4,5,6),nrow=2) X ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 and both produce a similar matrix, only transposed. X == t(Y) ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE In the example above, the number of rows (or columns) was a clean multiple of the number of entries. However, if it is not, R will fill in values. X &lt;- matrix(c(1,2,3,4,5,6),ncol=4, byrow=TRUE) ## Warning in matrix(c(1, 2, 3, 4, 5, 6), ncol = 4, byrow = TRUE): data length [6] ## is not a sub-multiple or multiple of the number of columns [4] Notice how you get a warning from the interpreter. But that does not stop it from filling in the remaining slots by starting over in the sequence of numbers you passed to it. X ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 1 2 The dimensionality of a matrix (and data.frame as we will see shortly) is returned by the dim() function. This will provide the number of rows and columns as a vector. dim(X) ## [1] 2 4 Accessing elements to retrieve or set their values within a matrix is done using the square brackets just like for a vector but you need to give [row,col] indices. Again, these are 1-based so that X[1,3] ## [1] 3 is the entry in the 1st row and 3rd column. You can also use ‘slices’ through a matrix to get the rows X[1,] ## [1] 1 2 3 4 or columns X[,3] ## [1] 3 1 of data. Here you just omit the index for the entity you want to span. Notice that when you grab a slice, even if it is a column, is given as a vector. length(X[,3]) ## [1] 2 You can grab a sub-matrix using slices if you give a range (or sequence) of indices. X[,2:3] ## [,1] [,2] ## [1,] 2 3 ## [2,] 6 1 If you ask for values from a matrix that exceed its dimensions, R will give you an error. X[1,8] ## Error in X[1, 8] : subscript out of bounds ## Calls: &lt;Anonymous&gt; ... handle -&gt; withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval ## Execution halted There are a few cool extensions of the rep() function that can be used to create matrices as well. They are optional values that can be passed to the function. times=x: This is the default option that was occupied by the ‘3’ in the example above and represents the number of times that first argument will be repeated. each=x This will take each element in the first argument are repeat them each times. length.out=x: This make the result equal in length to x. In combination, these can be quite helpful. Here is an example using numeric sequences in which it is necessary to find the index of all entries in a 3x2 matrix. To make the indices, I bind two columns together using cbind(). There is a matching row binding function, denoted as rbind() (perhaps not so surprisingly). What is returned is a matrix indices &lt;- cbind( rep(1:2, each=3), rep(1:3,times=2), rep(5,length.out=6) ) indices ## [,1] [,2] [,3] ## [1,] 1 1 5 ## [2,] 1 2 5 ## [3,] 1 3 5 ## [4,] 2 1 5 ## [5,] 2 2 5 ## [6,] 2 3 5 3.3. Lists A list is a type of vector but is indexed by ‘keys’ rather than by numeric indices. Moreover, lists can contain heterogeneous types of data (e.g., values of different class), which is not possible in a vector type. For example, consider the list theList &lt;- list( x=seq(2,40, by=2), dog=LETTERS[1:5], hasStyle=logical(5) ) summary(theList) ## Length Class Mode ## x 20 -none- numeric ## dog 5 -none- character ## hasStyle 5 -none- logical which is defined with a numeric, a character, and a logical component. Each of these entries can be different in length as well as type. Once defined, the entries may be observed as: theList ## $x ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 ## ## $dog ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; ## ## $hasStyle ## [1] FALSE FALSE FALSE FALSE FALSE Once created, you can add variables to the list using the $-operator followed by the name of the key for the new entry. theList$my_favoriate_number &lt;- 2.9 + 3i or use double brackets and the name of the variable as a character string. theList[[&quot;lotto numbers&quot;]] &lt;- rpois(7,lambda=42) The keys currently in the list are given by the names() function names(theList) ## [1] &quot;x&quot; &quot;dog&quot; &quot;hasStyle&quot; ## [4] &quot;my_favoriate_number&quot; &quot;lotto numbers&quot; Getting and setting values within a list are done the same way using either the $-operator theList$x ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 theList$x[2] &lt;- 42 theList$x ## [1] 2 42 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 or the double brackets theList[[&quot;x&quot;]] ## [1] 2 42 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 or using a numeric index, but that numeric index is looks to the results of names() to figure out which key to use. theList[[2]] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; The use of the double brackets in essence provides a direct link to the variable in the list whose name is second in the names() function (dog in this case). If you want to access elements within that variable, then you add a second set of brackets on after the double ones. theList[[1]][3] ## [1] 6 This deviates from the matrix approach as well as from how we access entries in a data.frame (described next). It is not a single square bracket with two indices, that gives you an error: theList[1,3] ## Error in theList[1, 3] : incorrect number of dimensions ## Calls: &lt;Anonymous&gt; ... handle -&gt; withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval ## Execution halted List are rather robust objects that allow you to store a wide variety of data types (including nested lists). Once you get the indexing scheme down, it they will provide nice solutions for many of your computational needs. 3.4. Data Frames a. Data Frames as spreadsheets The data.frame is the default data container in R. It is analogous to both a spreadsheet, at least in the way that I have used spreadsheets in the past, as well as a database. If you consider a single spreadsheet containing measurements and observations from your research, you may have many columns of data, each of which may be a different kind of data. There may be factors representing designations such as species, regions, populations, sex, flower color, etc. Other columns may contain numeric data types for items such as latitude, longitude, dbh, and nectar sugar content. You may also have specialized columns such as dates collected, genetic loci, and any other information you may be collecting. On a spreadsheet, each column has a unified data type, either quantified with a value or as a missing value, NA, in each row. Rows typically represent the sampling unit, perhaps individual or site, along which all of these various items have been measured or determined. A data.frame is similar to this, at least conceptually. You define a data.frame by designating the columns of data to be used. You do not need to define all of them, more can be added later. The values passed can be sequences, collections of values, or computed parameters. For example: df &lt;- data.frame( ID=1:5, Names=c(&quot;Bob&quot;,&quot;Alice&quot;,&quot;Vicki&quot;,&quot;John&quot;,&quot;Sarah&quot;), Score=100 - rpois(5,lambda=10)) df ## ID Names Score ## 1 1 Bob 93 ## 2 2 Alice 94 ## 3 3 Vicki 89 ## 4 4 John 91 ## 5 5 Sarah 88 You can see that each column is a unified type of data and each row is equivalent to a record. Additional data columns may be added to an existing data.frame as: df$Passed_Class &lt;- c(TRUE,TRUE,TRUE,FALSE,TRUE) Since we may have many (thousands?) of rows of observations, a summary() of the data.frame can provide a more compact description. summary(df) ## ID Names Score Passed_Class ## Min. :1 Length:5 Min. :88 Mode :logical ## 1st Qu.:2 Class :character 1st Qu.:89 FALSE:1 ## Median :3 Mode :character Median :91 TRUE :4 ## Mean :3 Mean :91 ## 3rd Qu.:4 3rd Qu.:93 ## Max. :5 Max. :94 We can add columns of data to the data.frame after the fact using the $-operator to indicate the column name. Depending upon the data type, the summary will provide an overview of what is there. b. Indexing Data Frames You can access individual items within a data.frame by numeric index such as: df[1,3] ## [1] 93 You can slide indices along rows (which return a new data.frame for you) df[1,] ## ID Names Score Passed_Class ## 1 1 Bob 93 TRUE or along columns (which give you a vector of data) df[,3] ## [1] 93 94 89 91 88 or use the $-operator as you did for the list data type to get direct access to a either all the data or a specific subset therein. df$Names[3] ## [1] &quot;Vicki&quot; Indices are ordered just like for matrices, rows first then columns. You can also pass a set of indices such as: df[1:3,] ## ID Names Score Passed_Class ## 1 1 Bob 93 TRUE ## 2 2 Alice 94 TRUE ## 3 3 Vicki 89 TRUE It is also possible to use logical operators as indices. Here I select only those names in the data.frame whose score was &gt;90 and they passed popgen. df$Names[df$Score &gt; 90 &amp; df$Passed_Class==TRUE] ## [1] &quot;Bob&quot; &quot;Alice&quot; This is why data.frame objects are very database like. They can contain lots of data and you can extract from them subsets that you need to work on. This is a VERY important feature, one that is vital for reproducible research. Keep you data in one and only one place. 4. Programming One of the strengths of R as an analysis platform is that it is a language rather than a program. With programs, such as SPSS &amp; JMP, you are limited by the functionality that the designers thought would be necessary to meet the broadest audience. In R, you can rely upon simple functions or you can create entire analysis and simulation programs de novo. To do this, we need to dig into flow control and decision making processes, both of which you need for doing more in-depth programming. 4.1. Function Writing Here we look at how to create an R function. Writing small functions like this is a huge benefit to you as an analyst and this is a great place to start. A function in R is defined as: function_name &lt;- function( arguments ) { Stuff you want the function to do } You define a function name for whatever you like and assign it the stuff to the right. In R, the function named function() is a special one, it tells R that you are about to create a little routine and you want that set of code to be available to you for later use under the name of whatever you named it. This allows a tremendous amount of flexibility as you develop your own set of routines and analyses for your work. The part that actually does stuff is after the function call. It may be that the function that you create does need some data (those are the arguments) or you may not need any input in to the function (in which you pass no arguments). It all depends upon what you are creating. The key to understanding functions is that they are encapsulations of code—a shortcut for a sequence of instructions if you will not have to type over and over again. The less typing you do, the lower the probability that you will have errors (and all code has errors). Here is an example of some code that I’m going to develop into a function. This function will allow me to determine if one genotype could possibly be the offspring of the other genotype. library(gstudio) loc1 &lt;- locus( c(128,130) ) loc2 &lt;- locus( c(128,128) ) cat( loc1, loc2 ) ## 128:130 128:128 We start out with two loci, a 128:130 heterozygote and a 128:128 homozygote. These may represent repeat motifs at a microsatellite locus or some other co-dominant genotype. First, I’ll break the locus into a vector of genotypes. off.alleles &lt;- alleles( loc1 ) off.alleles ## [1] &quot;128&quot; &quot;130&quot; mom.alleles &lt;- alleles( loc2 ) mom.alleles ## [1] &quot;128&quot; &quot;128&quot; To be a valid potential offspring there should be at least one of the alleles in the parent that matches the allele in the offspring. The intersect() function returns the set of values common to both vectors. shared &lt;- intersect( off.alleles, mom.alleles ) shared ## [1] &quot;128&quot; If it has at least one of the alleles present (it could have both if parent and offspring are both the same heterozygote) then you cannot exclude this individual as a potential offspring. If there are no alleles in common, then the value returned is an empty vector. loc3 &lt;- locus( c(132,132)) dad.alleles &lt;- alleles( loc3 ) intersect( mom.alleles, dad.alleles ) ## character(0) This logic can be shoved into a function. You have to wrap it into a set of curly brackets. I use the length of the result from the intersect() to return from the function. Potential values for potential_offspring &lt;- function( parent, offspring ) { off &lt;- alleles( offspring ) par &lt;- alleles( loc2 ) shared &lt;- intersect( off, par ) return( length( shared ) &gt; 0 ) } Now, you can call this function anytime you need, just passing it two genotypes. If they can be offspring it returns TRUE, as in the comparison between 128:130 and 128:128 genotypes. potential_offspring(loc1, loc2) ## [1] TRUE And it returns FALSE for the comparison between 128:128 and 132:132. potential_offspring(loc2, loc3) ## [1] FALSE 4.2. Variable Scope There is a lot more information on writing functions and we will get into that as we progress through the text. However, it is important that I bring this up now. The value assigned to a variable is defined by its scope. Consider the following code x &lt;- 10 and the function defined as do_it &lt;- function( x ) { x &lt;- x + 10 return( x ) } When I call the function, the variable x that is the argument of the function is not the same variable that is in the environment that I assigned a value of 10. The x in the function argument is what we call “local to that function” in that within the curly brackets that follow (and any number of curly brackets nested within those, the value of x is given whatever was passed to the function. 4.3. Decision Making We interact with our data in many ways and introspection of the values we have in the variables we are working with are of prime importance. Decision making in your code is where you evaluate your data and make a choice of outcomes based upon some criteria. Here is some example data that we can use as we explore the basics of if(), if(){} else{}, and if(){} elif(){} else{} coding patterns. a. The if Pattern The most basic version of decision making is asking a single question and if the answer is TRUE then do something. The if(){} function does this and has the form if( CRITERIA ) { DO_SOMETHING } You pass a logical statement (or something that can be coerced into a logical type) to the function as the CRITERIA and if it evaluates to TRUE, then the contents of the DO_SOMETHING are executed. If the value of CRITERIA is not TRUE the DO_SOMETHING is skipped entirely—it is not even seen by the interpreter. Here we can test this out using the loci defined above along with the is_heterozygote() function. This function takes one or more locus objects and returns TRUE/FALSE if they are or are not a heterozygote. is_heterozygote( c(loc1, loc2) ) ## [1] TRUE FALSE If we shove that function into the if() parameters we can use its evaluation of the heterozygous state of the locus to do something interesting, say tell us it is a heterozygote—it is admittedly a contrived example, but hey you try to make easy examples, it is not easy. if( is_heterozygote(loc1) ){ print(&quot;It&#39;s a het!&quot;) } ## [1] &quot;It&#39;s a het!&quot; If the is_heterozygote() function returns a value of FALSE, then the contents of the if() function (the stuff within the curly brackets is skipped entirely. if( is_heterozygote(loc2) ){ print(&quot;It&#39;s a het!&quot;) } Notice, there was no indication of any of that code inside the curly brackets. The if-else Pattern If there are more than on thing you want to potentially do when making a decision, you can add an else clause after the if pattern. Here if is_heterozygote() returns FALSE, the contents of the else{} clause will be executed. Here is the heterozygote example if( is_heterozygote(loc1) ) { cat(loc1, &quot;is a heterozygote&quot;) } else { cat(loc1, &quot;is a homozygote&quot;) } ## 128:130 is a heterozygote and the homozygote one if( is_heterozygote(loc2) ) { cat(loc2, &quot;is a heterozygote&quot;) } else { cat(loc2, &quot;is a homozygote&quot;) } ## 128:128 is a homozygote There is a slightly shorter version of this that is available for the lazy programmer and lets be honest, all programmers are lazy and the more you can accomplish with fewer strokes on the keyboard the better (this is how we got emacs and vim). I generally don’t teach the shortcuts up front, but this one is short and readily apparent so it may be more helpful than confusing. The ifelse() function has three parts, the condition, the result if TRUE, and the result if FALSE. ans &lt;- ifelse( is_heterozygote( c(loc1, loc2)) , &quot;heterozygote&quot;, &quot;Not&quot;) ans ## [1] &quot;heterozygote&quot; &quot;Not&quot; So iterating through the x vector, the condition x&gt;0 is evaluated and if TRUE the sqrt() of the value is returned, else the NA is given. It is compact and easy to use so you may run into it often. b. The if-else Pattern It is possible to test many conditions in a single sequence by stringing together else-if conditions. The point that is important here is that the first condition that evaluates to TRUE will be executed and all remaining ones will be skipped, even if they also are logically TRUE. This means that it is important to figure out the proper order of asking your conditions. Here is an example function that determines if none, one, or both of the genotypes passed to it are heterozygotes. By default, I step through every one of the potential options of available on this comparison. 1. The first is a heterozygote and the second one isn’t 2. The first one isn’t and the second one is 3. Both are heterozygotes 4. The last state (both are not) Here is the function. which_is_het &lt;- function( A, B) { if( is_heterozygote(A) &amp; !is_heterozygote(B) ) { print(&quot;First is heterozygote&quot;) } else if( !is_heterozygote(A) &amp; is_heterozygote(B) ){ print(&quot;Second is heterozygote&quot;) } else if( is_heterozygote(A) &amp; is_heterozygote(B) ){ print(&quot;Both are heterozygotes&quot;) } else { print( &quot;Neither are heterozygotes&quot;) } } It is possible that the order of these CRITERIA could be changed, the important thing to remember is that the sequence of if - else if - else if etc. will terminate the very first time one of the CRITERIA is evaluated to be TRUE. 4.4. Flow Control Flow control is the process of iterating across objects and perhaps doing operations on those objects. The R language has several mechanisms that you can use to control the flow of a script or bit of code. a. The for() Loop x &lt;- c(3,8,5,4,6) x ## [1] 3 8 5 4 6 You can iterate through this vector using a for() loop. This is a simple function that has the form: for( SOME_SEQUENCE ){ DO_SOMETHING } Where the SOME_SEQUENCE component is a sequence of values either specified OR calculated and the DO_SOMETHING is the thing you want to do with each of the values in the sequence. Usually, there is a variable defined in the SOME_SEQUENCE component and the value of that variable is used. Here are a few examples. The first goes through the existing vector directly and assigns (in sequential order) the entries of ‘x’ to the variable val. We can then do whatever we want with the value in val (though if we change it, nothing happens to the original x vector). for( val in x ){ print(val) } ## [1] 3 ## [1] 8 ## [1] 5 ## [1] 4 ## [1] 6 We can also specify a sequence directly and then use it as an index. Here I use an index variable named i to take on the integer seqeunce equal in length to the length of the original x variable. Then I can iterate through the original vector and use that index variable to grab the value I want. for( i in 1:length(x)){ print( x[i] ) } ## [1] 3 ## [1] 8 ## [1] 5 ## [1] 4 ## [1] 6 Both give us the same output, namely a way to go through the variable x. However, there may be a need to use the latter approach in your calculations. For example, perhaps I want to do some other operation on the values. In this very contrived example that follows, I want to perform operations on the values in x depending on if they are even or odd. For the odd ones, I add the corresponding value in y and if not I subtract it. Sure, this is totally contrived and I cannot think of a reason why I would be doing this, but if I need to know what index (row, column or whatever) an entry is during the iteration process, then I need to use this approach over the for( val in x) approach. y &lt;- 1:5 for( i in 1:length(x)){ if( x[i] %% 2) print( x[i] + y[i]) else print( x[i] - y[i] ) } ## [1] 4 ## [1] 6 ## [1] 8 ## [1] 0 ## [1] 1 b. Short Circuiting the Loop It is possible to short circuit the looping process using the keywords next and break, though in my programming style, I consider their use in my source files as evidence of inelegant code. That said, you may need them on occasion. The next keyword basically stops all commands after that during the current iteration of the loop. It does not terminate the loop itself, it just stops the commands that follow it this time through. Here is an example that uses the modulus operator, %% (e.g., the remainder after division), to print out only those numbers that are divisible by three. for( i in 1:20 ){ if( i %% 3 ) next cat(&quot;The value of i =&quot;,i,&quot;\\n&quot;) } ## The value of i = 3 ## The value of i = 6 ## The value of i = 9 ## The value of i = 12 ## The value of i = 15 ## The value of i = 18 The use of break to exit the loop entirely is perhaps more commonly encountered. When this keyword is encountered, the loop terminates immediately, as if it reached the send of the sequence. for( i in 1:10){ if( i &gt; 2 ) break cat(&quot;The value of i=&quot;,i,&quot;\\n&quot;) } ## The value of i= 1 ## The value of i= 2 The word data is plural, datum is singular↩︎ "],["r-graphics.html", "2.2 R Graphics", " 2.2 R Graphics Rodney Dyer (worked example) and Helene Wagner (adaptation) 1. Overview This worked example is adapted from “Applied Population Genetics” by Rodney Dyer. The entire book is available here: http://dyerlab.github.io/applied_population_genetics/index.html One of the most critical features of data analysis is the ability to present your results in a logical and meaningful fashion. R has built-in functions that can provide you graphical output that will suffice for your understanding and interpretations. However, there are also third-party packages that make some truly beautiful output. In this section, both built-in graphics and graphical output from the ggplot2 library are explained and highlighted. We are going to use the venerable iris dataset that was used by Anderson (1935) and Fisher (1936). These data are measurements of four morphological variables (Sepal Length, Sepal Width, Petal Length, and Petal Width) measured on fifty individual iris plants from three recognized species. Here is a summary of this data set. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## I will also provide examples using two different plotting approaches. R has a robust set of built-in graphical routines that you can use. However, the development community has also provided several additional graphics libraries available for creating output. The one I prefer is ggplot2 written by Hadley Wickham. His approach in designing this library a philosophy of graphics depicted in Leland Wilkson’s (2005) book The Grammar of Graphics. As I understand it, the idea is that a graphical display consists of several layers of information. These layers may include: The underlying data. Mapping of the data onto one or more axes. Geometric representations of data as points, lines, and/or areas. Transformations of the axes into different coordinate spaces (e.g., cartesian, polar, etc.) or the data onto different scales (e.g., logrithmic) Specification of subplots. In the normal plotting routines discussed before, configuration of these layers were specified as arguments passed to the plotting function (plot(), boxplot(), etc.). The ggplot2 library takes a different approach, allowing you to specify these components separately and literally add them together like components of a linear model. Required packages: library(ggplot2) library(RColorBrewer) 2. Univariate plots 2.1. Barplots and histograms Univariate data can represent either counts (e.g., integers) of items or vectors of data that have decimal components (e.g., frequency distributions, etc.). If the sampling units are discrete, then a the barplot() function can make a nice visual representation. Here is an example using some fictions data. x &lt;- c(2, 3, 6, 3, 2, 4) names(x) &lt;- c(&quot;Bob&quot;, &quot;Alice&quot;, &quot;Jane&quot;, &quot;Fred&quot;, &quot;Barney&quot;, &quot;Lucy&quot;) x ## Bob Alice Jane Fred Barney Lucy ## 2 3 6 3 2 4 From this, a barplot can be constructed where the x-axis has discrete entities for each name and the y-axis represents the magnitude of whatever it is we are measuring. barplot(x, xlab = &quot;People&quot;, ylab = &quot;Thinn-A-Ma-Jigs&quot;) Notice here that I included labels for both the x- and y-axes. You do this by inserting these arguments into the parameters passed to the barplot() (it is the same for all built-in plotting as we will see below). To use ggplot for displaying this, you have to present the data in a slightly different way. There are shortcuts using the qplot() function but I prefer to use the more verbose approach. Here is how we would plot the same output using ggplot. library(ggplot2) df &lt;- data.frame(Thinn_A_Ma_Jigs=x,names=names(x)) ggplot( df, aes(x=names,y=Thinn_A_Ma_Jigs)) + geom_bar(stat=&quot;identity&quot;) There are a couple of things to point out about this. This is a compound statement, the plotting commands are literally ‘added together’. The ggplot() function specifies a data.frame from which your data will be extracted. The aes() component within the ggplot() function describes the aesthetics of the plot. This is how you designate which variable columns in the data.frame will be on the x- and y- axes, colors, plot shapes, categories, etc. To the basic description of the data and aesthetics, a geometry is added, that of a barplot. If necessary, you can pass additional information to the geom_bar() function. In this case, I had to tell it that you use the raw data stat='identity' instead of trying to summarize many observations. The axis labels are taken directly from the names of the columns in the data.frame. If the data on the x-axis is not discrete but measured on a single variable, then the hist() function can take the data and categorize it into groups based upon the density of the underlying data. h &lt;- hist(iris$Sepal.Length) The hist() function itself returns a bit of information that may be of interest. It is not necessary that you capture these data, R will ignore it if you do not assign it to a variable. What is of interest though is that it returns a list() object with the following keys: names(h) ## [1] &quot;breaks&quot; &quot;counts&quot; &quot;density&quot; &quot;mids&quot; &quot;xname&quot; &quot;equidist&quot; The data within these keys can be accessed directly as: h ## $breaks ## [1] 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 ## ## $counts ## [1] 5 27 27 30 31 18 6 6 ## ## $density ## [1] 0.06666667 0.36000000 0.36000000 0.40000000 0.41333333 0.24000000 0.08000000 ## [8] 0.08000000 ## ## $mids ## [1] 4.25 4.75 5.25 5.75 6.25 6.75 7.25 7.75 ## ## $xname ## [1] &quot;iris$Sepal.Length&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; and provide the basic information on how to construct the histogram as shown below. The values defined for the break locations for each of the bars and the midpoints are the default ones for the hist() function. If you wish, you can provide the number of breaks in the data, the location of the breaks, including the lowest and highest terms, plot as a count versus a frequency, etc. The corresponding ggplot approach is ggplot( iris, aes(x=Sepal.Length)) + geom_bar() which also takes a few liberties with the default binning parameters. By default, both the R and ggplot approches make a stab at creating enough of the meta data around the plot to make it somewhat useful. However, it is helpful if we add a bit to that and create graphics that suck just a little bit less. There are many options that we can add to the standard plot command to make it more informative and/or appropriate for your use. The most common ones include: xlab Change the label on the x-axis ylab Change the lable on the y-axis main Change the title of the graph col Either a single value, or a vector of values, indicating the color to be plot. xlim/ylim The limits for the x- and y-axes. Additional options are given in tabular form in the next section. Here is another plot of the same data but spiffied up a bit. x &lt;- hist( iris$Sepal.Length,xlab=&quot;Sepal Length (cm)&quot;, ylab=&quot;Count&quot;, main=&quot;&quot;, col=&quot;lightblue&quot;) A few things to notice: The main title over the top of the image was visually removed by assigning an empty characters string to it. The color parameter refers to the fill color of the bars, not the border color. In ggplot we add those additional components either to the geometry where it is defined (e.g., the color in the geom_bar) or to the overall plot (as in the addition of labels on the axes). ggplot( iris, aes(x=Sepal.Length)) + geom_bar(fill=&quot;lightblue&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Count&quot;) 2.2. Plotting Parameters There are several parameters common to the various plotting functions used in the basic R plotting functions. Here is a short table of the most commonly used ones. Command Usage Description bg bg=“white” Sets the background color for the entire figure. bty bty=“n” Sets the style of the box type around the graph. Useful values are “o” for complete box (the default), “l”, “7”, “c”, “u”, “]” which will make a box with sides around the plot area resembling the upper case version of these letters, and “n” for no box. cex cex=2 Magnifies the default font size by the corresponding factor. cex.axis cex.axis=2 Magnifies the font size on the axes. col col=“blue” Sets the plot color for the points, lines, etc. fg fg=“blue” Sets the foreground color for the image. lty lty=1 Sets the type of line as 0-none, 1-solid, 2-dashed, 3-dotted, etc. lwd lwd=3 Specifies the width of the line. main main=“title” Sets the title to be displayed over the top of the graph. mfrow mfrow=c(2,2) Creates a ‘matrix’ of plots in a single figure. pch pch=16 Sets the type of symbol to be used in a scatter plot. sub sub=“subtitle” Sets the subtitle under the main title on the graph. type type=“l” Specifies the type of the graph to be shown using a generic plot() command. Types are “p” for points (default), “l” for lines, and “b” for both. xlab xlab=“Size (m)” Sets the label attached to the x-axis. ylab ylab=“Frequency” Sets the label attached to the y-axis. 2.3. Density Plots Another way of looking at the distribution of univariate data is through the use of the density() function. This function takes a vector of values and creates a probability density function of it returning an object of class “density”. d &lt;- density( iris$Sepal.Length) attributes(d) ## $names ## [1] &quot;x&quot; &quot;y&quot; &quot;bw&quot; &quot;n&quot; &quot;old.coords&quot; ## [6] &quot;call&quot; &quot;data.name&quot; &quot;has.na&quot; ## ## $class ## [1] &quot;density&quot; The density() function takes the univariate data and fits its density (internally represented by d$y) along the range of values (in d$x). A plot can be produced using d as the variables as follows (with a bit of extra plotting parameters as depicted in the previous table). plot( d, col = &quot;red&quot;, lwd = 2, xlab = &quot;Value of X&quot;, ylab = &quot;Frequency&quot;, bty = &quot;n&quot;, main = &quot;Density Plot&quot;) The corresponding ggplot approach is: ggplot( iris, aes(x=Sepal.Length)) + geom_density() 3. Bivariate plots 3.1. Overlaying Plots by grouping The data in the previous density plot represents the sepal lengths across all three iris species. It may be more useful to plot them as the density of each species instead of combined. Overlaying plots is a pretty easy feature built into the default R plotting functionalities. Lets look at the data and see if the mean length differs across species. I do this using the by() command, which takes three parameters; the first parameter is the raw data you want to examine, the second one is the way you would like to partition that data, and the third one is the function you want to call on those groupings of raw data. This general form by( data, grouping, function ) mixing both data and functions to be used, is pretty common in R and you will see it over and over again. The generic call below asks to take the sepal data and partition it by species and estimate the mean. by( iris$Sepal.Length, iris$Species, mean) ## iris$Species: setosa ## [1] 5.006 ## ------------------------------------------------------------ ## iris$Species: versicolor ## [1] 5.936 ## ------------------------------------------------------------ ## iris$Species: virginica ## [1] 6.588 So there may be differences. Lets pull the data out and create density plots for each. d.setosa &lt;- iris$Sepal.Length[ iris$Species==&quot;setosa&quot; ] d.versicolor &lt;- iris$Sepal.Length[ iris$Species==&quot;versicolor&quot; ] d.virginica &lt;- iris$Sepal.Length[ iris$Species==&quot;virginica&quot; ] d.se &lt;- density( d.setosa ) d.ve &lt;- density( d.versicolor ) d.vi &lt;- density( d.virginica ) I can now plot the densities independently. After the first plot() function is called, you can add to it by using lines() or points() function calls. They will overlay the subsequent plots over the first one. One of the things you need to be careful of is that you need to make sure the x- and y-axes are properly scaled so that subsequent calls to points() or lines() does not plot stuff outside the boundaries of your initial plot. Here I plot the setosa data first, specify the xlim (limits of the x-axis), set the color, and labels. On subsequent plotting calls, I do not specify labels but set alternative colors. Then a nice legend is placed on the graph, the coordinates of which are specified on the values of the x- and y-axis (I also dislike the boxes around graphic so I remove them as well with bty=\"n\"). plot(d.se,xlim=c(4,8),col=&quot;red&quot;, lwd=2, bty=&quot;n&quot;, xlab=&quot;Sepal Length (cm)&quot;, main=&quot;Sepal Lengths&quot;) lines( d.ve, xlim=c(4,8), col=&quot;green&quot;,lwd=2, bty=&quot;n&quot;) lines( d.vi, xlim=c(4,8), col=&quot;blue&quot;, lwd=2, bty=&quot;n&quot;) legend( 6.5,1.1,c(&quot;I. setosa&quot;, &quot;I. versicolor&quot;, &quot;I. virginica&quot;), col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;), lwd=2,bty=&quot;n&quot;) A lot of that background material is unnecessary using geom_density() because we can specify to the plotting commands that the data are partitioned by the values contained in the Species column of the data.frame. This allows us to make this plot ggplot(iris, aes(Sepal.Length,color=Species)) + geom_density() or a correpsonding one using fill=Species instead (I set the alpha transparency to allow you to see the plots in the background). ggplot(iris, aes(Sepal.Length,fill=Species)) + geom_density( alpha=0.8) We can use the barplot() function here as well and either stack or stagger the density of sepal lengths using discrete bars. Here I make a matrix of bins using the hist() function with a specified set of breaks and then use it to plot discrete bin counts using the barplot() function. I include this example here because there are times when you want to produce a stacked bar plot (rarely) or a staggered barplot (more common) for some univariate data source and I always forget how to specifically do that. breaks &lt;- seq(4,8,by=0.2) h.se &lt;- hist(d.setosa, breaks=breaks, plot = FALSE) h.ve &lt;- hist(d.versicolor, breaks=breaks, plot=FALSE) h.vi &lt;- hist(d.virginica, breaks=breaks, plot=FALSE) vals &lt;- rbind( h.se$counts, h.ve$counts, h.vi$counts ) rownames(vals) &lt;- levels(iris$Species) colnames(vals) &lt;- breaks[1:20] vals ## 4 4.2 4.4 4.6 4.8 5 5.2 5.4 5.6 5.8 6 6.2 6.4 6.6 6.8 7 7.2 7.4 7.6 ## setosa 0 4 5 7 12 11 6 2 3 0 0 0 0 0 0 0 0 0 0 ## versicolor 0 0 0 0 3 2 1 10 8 6 6 5 3 4 2 0 0 0 0 ## virginica 0 0 0 0 1 0 0 1 4 3 4 11 4 7 3 4 2 1 4 ## 7.8 ## setosa 0 ## versicolor 0 ## virginica 1 The matrix of data barplot(vals,xlab=&quot;Sepal Length&quot;, ylab=&quot;Frequency&quot;) Stacked barplots may or may not be that informative, depending upon the complexity of your underlying data. It is helpful though to be able to stagger them. In the basic barplot() function allows you to specify the bars to be spaced beside each other as: barplot(vals,xlab=&quot;Sepal Length&quot;, ylab=&quot;Frequency&quot;, col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;), beside=TRUE) legend(60, 10, c(&quot;I. setosa&quot;, &quot;I. versicolor&quot;, &quot;I. virginica&quot;), fill = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), bty=&quot;n&quot;) For plotting onto a barplot object, the x-axis number is not based upon the labels on the x-axis. It is an integer that relates to the number of bar-widths and separations. In this example, there are three bars for each category plus one separator (e.g., the area between two categories). So I had to plot the legend at 60 units on the x-coordinate, which puts it just after the 15th category (e.g., 15*4=60). We can do the same thing with geom_histogram(), though again with a bit less typing involved. Here is the raw plot (which by default stacks just like in the barplot() example) ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value `binwidth`. and the correspondingly staggered plot with bars positioned next to eachother. ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_histogram(position=&quot;dodge&quot;) ## `stat_bin()` using `bins = 30`. Pick better value `binwidth`. Notice how we add the fill color to the aes() function because the categories for assigning colors will be extracted from the data.frame, whereas when we just wanted to set them all light blue, the color is specified outside the aes() function. Also notice when we specify something to the aes() command in this way, we do not quote the name of the column, we call it just as if it were a normal variable. 3.2. Boxplots Boxplots are a display of distributions of continuous data within categories, much like what we displayed in the previous staggered barplot. It is often the case that boxplots depict differences in mean values while giving some indication of the dispersal around that mean. This can be done in the staggered barplot as above but not quite as specifically. The default display for boxplots in R provides the requires a factor and a numeric data type. The factor will be used as the categories for the x-axis on which the following visual components will be displayed: The median of the data for each factor, An optional 5% confidence interval (shown by passing notch=TRUE) around the mean. The inner 50th percentile of the data, Outlier data points. Here is how the sepal length can be plot as a function of the Iris species. boxplot(Sepal.Length ~ Species, data=iris, notch=TRUE, xlab=&quot;Species&quot;, ylab=&quot;Sepal Length (cm)&quot;, frame.plot=FALSE) A couple of things should be noted on this plot call. First, I used a ‘functional’ form for the relationship between the continuous variable (Sepal.Length) and the factor (Species) with the response variable indicated on the left side of the tilde and the predictor variables on the right side. It is just as reasonable to use boxplot(Species,Sepal.Length, ...) in the call as x- and y- variables in the first two positions. However, it reads a bit better to do it as a function, like a regression. Also, it should be noted that I added the optional term, data=iris, to the call. This allowed me to reference the columns in the iris data.frame without the dollar sign notation. Without that I would have had to write boxplot( data$Sepal.Length ~ data$Species, ...), a much more verbose way of doing it but most programmers are relatively lazy and anything they can do to get more functionality out of less typing… Finally, I set frame.plot=FALSE to remove the box around the plot since the bty=\"n\" does not work on boxplots (big shout out for consistency!). I don’t know why these are different, I just hate the box. The corresponding ggplot approach is the same as the boxplot() one, we just have to specify the notch=TRUE in the geom_boxplot() function. ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch = TRUE) + xlab(&quot;Iris Species&quot;) + ylab(&quot;Sepal Width (cm)&quot;) 3.3. Scatter Plots With two sets of continuous data, we can produce scatter plots. These consist of 2- (or 3-) coordinates where we can plot our data. With the addition of alternative colors (col=) and/or plot shapes (pch=) passed to the generic plot() command, we can make really informative graphical output. Here I plot two characteristics of the iris data set, sepal length and width, and use the species to indicate alternative symbols. plot( iris$Sepal.Length, iris$Sepal.Width, pch=as.numeric(iris$Species), bty=&quot;n&quot;, xlab=&quot;Sepal Length (cm)&quot;, ylab=&quot;Sepal Width (cm)&quot;) legend( 6.75, 4.3, c(&quot;I. setosa&quot;, &quot;I. versicolor&quot;, &quot;I. virginica&quot;), pch=1:3, bty=&quot;n&quot;) The call to pch= that I used coerced a factor into a numeric data type. By default, this will create a numeric sequence (starting at 1) for all levels of the factor, including ones that may not be in the data you are doing the conversion on. The parameter passed to pch is an integer that determines the shape of the symbol being plot. I typically forget which numbers correspond to which symbols (there are 25 of them in total ) and have to look them up when I need them. One of the easiest ways is just to plot them as: plot(1:25,1:25,pch=1:25, bty=&quot;n&quot;, xlab=&quot;pch&quot;, ylab=&quot;pch&quot;) which produces the following figure where you can choose the appropriate symbols for your plotting needs. A scatter plot in ggplot is created using the geom_point() layer. ggplot(iris,aes(x=Sepal.Length,y=Sepal.Width, shape=Species)) + geom_point() and the shapes are specified automatically. 4. Advanced plotting 4.1. Multiple Plots The last figure was a bit confusing. You can see a few points where the three species have the same measurements for both sepal length and width—both I. versicolor and I. virginica have examples with sepal widths of 3.0cm and lengths of 6.7cm. For some of these it would be difficult to determine using a graphical output like this if there were more overlap. In this case, it may be a more informative approach to make plots for each species rather than using different symbols. In ggplot, there is a facet geometry that can be added to a plot that can pull apart the individual species plots (in this case) and plot them either next to each other (if you are looking at y-axis differences), on top of each other (for x-axis comparisons), or as a grid (for both x- and y- axis comparisons). This layer is added to the plot using facet_grid() and take a functional argument (as I used for the boxplot example above) on which factors will define rows and columns of plots. Here is an example where I stack plots by species. ggplot( iris, aes(x=Sepal.Width, y=Sepal.Length ) ) + geom_point() + facet_grid(Species~.) + xlab(&quot;Sepal Width (cm)&quot;) + ylab(&quot;Sepal Length&quot;) The Species ~ . means that the rows will be defined by the levels indicatd in iris$Species and all the plotting (the period part) will be done as columns. If you have more than one factor, you can specify RowFactor ~ ColumnFactor and it will make the corresponding grid. I would argue that this is a much more intuitive display of differences in width than the previous plot. You can achieve a similar effect using built-in plotting functions as well. To do this, we need to mess around a bit with the plotting attributes. These are default parameters (hence the name par) for plotting that are used each time we make a new graphic. Here are all of them (consult with the previous Table for some of the meanings). names( par() ) ## [1] &quot;xlog&quot; &quot;ylog&quot; &quot;adj&quot; &quot;ann&quot; &quot;ask&quot; &quot;bg&quot; ## [7] &quot;bty&quot; &quot;cex&quot; &quot;cex.axis&quot; &quot;cex.lab&quot; &quot;cex.main&quot; &quot;cex.sub&quot; ## [13] &quot;cin&quot; &quot;col&quot; &quot;col.axis&quot; &quot;col.lab&quot; &quot;col.main&quot; &quot;col.sub&quot; ## [19] &quot;cra&quot; &quot;crt&quot; &quot;csi&quot; &quot;cxy&quot; &quot;din&quot; &quot;err&quot; ## [25] &quot;family&quot; &quot;fg&quot; &quot;fig&quot; &quot;fin&quot; &quot;font&quot; &quot;font.axis&quot; ## [31] &quot;font.lab&quot; &quot;font.main&quot; &quot;font.sub&quot; &quot;lab&quot; &quot;las&quot; &quot;lend&quot; ## [37] &quot;lheight&quot; &quot;ljoin&quot; &quot;lmitre&quot; &quot;lty&quot; &quot;lwd&quot; &quot;mai&quot; ## [43] &quot;mar&quot; &quot;mex&quot; &quot;mfcol&quot; &quot;mfg&quot; &quot;mfrow&quot; &quot;mgp&quot; ## [49] &quot;mkh&quot; &quot;new&quot; &quot;oma&quot; &quot;omd&quot; &quot;omi&quot; &quot;page&quot; ## [55] &quot;pch&quot; &quot;pin&quot; &quot;plt&quot; &quot;ps&quot; &quot;pty&quot; &quot;smo&quot; ## [61] &quot;srt&quot; &quot;tck&quot; &quot;tcl&quot; &quot;usr&quot; &quot;xaxp&quot; &quot;xaxs&quot; ## [67] &quot;xaxt&quot; &quot;xpd&quot; &quot;yaxp&quot; &quot;yaxs&quot; &quot;yaxt&quot; &quot;ylbias&quot; To create multiple plots on one graphic, we need to modify either the mfrow or mfcol property (either will do). They represent the number of figures to plot as a 2-integer vector for rows and columns. By default, it is set to par()$mfrow ## [1] 1 1 because there is only 1 row and 1 column in the plot. To change this, we simply pass a new value to the par() command and then do the plotting. In the following example, I plot the results of a linear model (lm()) function call. This returns four plots looking at the normality of the data, residuals, etc. This time, instead of seeing them one at a time, I’m going to plot all four into one figure. par(mfrow=c(2,2)) plot( lm( Sepal.Length ~ Sepal.Width, data=iris)) 4.2. Color Palettes The default plotting colors in R are black and white. However, there is a rich set of colors available for your plotting needs. The easiest set are named colors. At the time of this writing, there are length(colors()) ## [1] 657 different colors available for your use. Not all are distinct as some overlap. However the benefit of these colors is that they have specific names, making it easier for you to remember than RGB or hex representations. Here are a random set of 20 color names. colors()[ sample.int( length(colors()), size=20) ] ## [1] &quot;gray75&quot; &quot;orchid4&quot; &quot;grey62&quot; &quot;khaki1&quot; ## [5] &quot;darkslategray2&quot; &quot;gray61&quot; &quot;grey39&quot; &quot;gray45&quot; ## [9] &quot;gray43&quot; &quot;grey94&quot; &quot;darkcyan&quot; &quot;grey58&quot; ## [13] &quot;plum1&quot; &quot;deepskyblue1&quot; &quot;grey40&quot; &quot;indianred3&quot; ## [17] &quot;ivory1&quot; &quot;darkslategray4&quot; &quot;burlywood2&quot; &quot;darkseagreen3&quot; To use these colors you can call them by name in the col= option to a plot. Here is an example where I define three named colors and then coerce the iris$Species variable into an integer to select the color by species and plot it in a scatter plot (another version of the pch= example previously). colors &lt;- c(&quot;royalblue1&quot;, &quot;orange1&quot;, &quot;green3&quot;) cols &lt;- colors[ iris$Species ] plot( Sepal.Width ~ Sepal.Length, data=iris, col=cols, xlab=&quot;Sepal Length (cm)&quot;, ylab=&quot;Sepal Width (cm)&quot;, bty=&quot;n&quot;, pch=16) There is a lot of colors to choose from, and you are undoubtedly able to fit any color scheme on any presentation you may encounter. In addition to named colors, there are color palettes available for you to grab the hex value for individual color along a pre-defined gradient. These colors ramps are: rainbow(): A palette covering the visible spectrum heat.colors(): A palette ranging from red, through orange and yellow, to white. terrain.colors(): A palette for plotting topography with lower values as green and increasing through yellow, brown, and finally white. topo.colors(): A palette starting at dark blue (water) and going through green (land) and yellow to beige (mountains). cm.colors(): A palette going from light blue through white to pink. and are displayed in the following figure. The individual palette functions return the hex value for an equally separated number of colors along the palette, you only need to ask for the number of colors you are requesting. Here is an example from the rainbow() function. rainbow(10) ## [1] &quot;#FF0000&quot; &quot;#FF9900&quot; &quot;#CCFF00&quot; &quot;#33FF00&quot; &quot;#00FF66&quot; &quot;#00FFFF&quot; &quot;#0066FF&quot; ## [8] &quot;#3300FF&quot; &quot;#CC00FF&quot; &quot;#FF0099&quot; 4.3. RColorBrewer The ggplot2 library has a slightly more interesting color palette, but it too is often a bit lacking. You have total control over the colors you produce and can specify them as RGB, hex (e.g. the way the web makes colors), or CMYK. You can also define your own color palettes. To start, the RColorBrewer library provides several palettes for plotting either quantitative, qualitative, or divergent data. These three groups and the names of individual color palettes can be viewed as: library(RColorBrewer) display.brewer.all() In ggplot, we can use these palettes as follows. You can change the default palette in ggplot output adding the scale_color_brewer() (and scale_fill_brewer() if you are doing fill colors like in a barplot) to the plot. Here is an example where I change the default palette to the 6th divergent palette. p &lt;- ggplot(iris,aes(x=Sepal.Length,y=Sepal.Width, color=Species)) + geom_point() p &lt;- p + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) p + scale_color_brewer(type=&quot;div&quot;, palette=6) 4.4. Saving Imagery Creating a graphic for display on your screen is only the first step. For it to be truly useful, you need to save it to a file, often with specific parameters such as DPI or image size, so that you can use it outside of R. For normal R graphics, you can copy the current graphical device to a file using dev.copy() or you can create a graphic file object and plot directly to it instead of the display. In both cases, you will need to make sure that you have your graphic formatted the way you like. Once you have the file configured the way you like, decide on the image format you will want. Common ones are jpg, png, tiff and pdf. Here are the function arguments for each of the formats. Notes: png is preferred over jpg because jpg compression results in loss. The command ‘pdf’ does not take a ‘units’ argument, it assumes that the unit is inches. Only the most commonly used arguments are listed here, for further arguments, see the help file for each function. jpeg(filename = &quot;Rplot%03d.jpeg&quot;, width = 480, height = 480, units = &quot;px&quot;, pointsize = 12) png(filename = &quot;Rplot%03d.png&quot;, width = 480, height = 480, units = &quot;px&quot;, pointsize = 12, bg = &quot;white&quot;, res = 300) tiff(filename = &quot;Rplot%03d.tiff&quot;, width = 480, height = 480, units = &quot;px&quot;, pointsize = 12, compression = &quot;none&quot;, bg = &quot;white&quot;) pdf(file = &quot;Rplot%03d.pdf&quot;, width = 6, height = 6, pointsize = 12, bg = &quot;white&quot;) These are the default values. If you need a transparent background, it is easiest to use the png file and set bg=\"transparent\". If you are producing an image for a publication, you will most likely be given specific parameters about the image format in terms of DPI, pointsize, dimensions, etc. They can be set here. To create the image, call the appropriate function above, and a file will be created for your plotting. You must then call the plotting functions to create your graphic. Instead of showing up in a graphical display window, they will instead be plot to the file. When done you must tell R that your plotting is now finished and it can close the graphics file. This is done by calling dev.off(). Here is an example workflow saving an image of a scatter plot with a smoothed line through it using ggplot. png( filename = &quot;MyCoolGraphic.png&quot;, quality=300, bg=&quot;transparent&quot;, width=1080, height=1080) ggplot( df, aes(x=PredictorVar, y=ResponseVar)) + geom_point() + stat_smooth(method=&quot;loess&quot;) + theme_bw() dev.off() This will create a file MyCoolGraphic.png saved in the same folder as the current working directory. To use dev.copy() instead, you first make the graphic and then copy the current graphic to a file. For this example, it would be: ggplot( df, aes(x=PredictorVar, y=ResponseVar)) + geom_point() + stat_smooth(method=&quot;loess&quot;) + theme_bw() dev.copy(device=png,file=&quot;MyDevCopyGraphic.png&quot;) dev.off() In general, I prefer the first method as it allows me to specify the specifics of the file in an easier fashion than the second approach. If you are using ggplot2 graphics, there is a built-in function ggsave() that can also be used to save your currently displaying graphic (or others if you so specify) to file. Here are the specifics on that function call. ggsave(filename = default_name(plot), plot = last_plot(), device = default_device(filename), path = NULL, scale = 1, width = par(&quot;din&quot;)[1], height = par(&quot;din&quot;)[2], units = c(&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;), dpi = 300, limitsize = TRUE, ...) Either approach will allow you to produce publication quality graphics. 4.5. Interactive Graphics There are several other libraries available for creating useful plots in R. The most dynamical ones are based on javascript and as such are restricted to web or ebook displays. Throughout this book, I will be showing how you can use various libraries to create interactive content as well as providing some ‘widgets’ that will demonstrate how some feature of popualtion or landscape structure changes under a specified model. These widgets are also written in R and are hosted online as a shiny application. I would encourage you to look into creating dynamcial reports using these techniques, they can facilitate a better understanding of the underlying biology than static approaches. "],["resources.html", "2.3 Further R resources", " 2.3 Further R resources Links to some excellent external resources: Applied Population Genetics bz Rodney Dyer Efficient R Programming by Colin Gillespie and Robin Lovelace "],["github-for-group-projects.html", "3 GitHub for Group Projects", " 3 GitHub for Group Projects If you joined a group project in the DGS Landscape Genetics and want to collaborate with your team on GitHub, this part is for you! It contains an introductory video and a series of tutorials to help get you started with GitHub and collaborative coding. Why use GitHub? Installation Workflow "],["github.html", "3.1 Why use GitHub?", " 3.1 Why use GitHub? Helene Wagner 1. Goals This section explains the basics of version control: why it is important, and how it works. This will help you better understand the tutorials on installation and workflow. Here you can: Watch course video Preview and download the slides 2. Watch course video: Version Control 101 Note: the video mentions a worked example. This has been replaced by the external tutorials listed above. External link: Week 0 video Transcript: Download transcript Embedded video: iframe not supported 3. Preview slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. 4. Next steps Continue to the Installation page. "],["install.html", "3.2 Installation", " 3.2 Installation Helene Wagner 1. Goals This section provides step-by-step video instructions how to install git and make it interact with RStudio and GitKraken. Video tutorials walk you through a typical installation process. They assume that you have received an invitation to join a GitHub project, e.g., for a DGS Landscape Genetics group project. Happy Git with R - this external tutorial is more general and can be very useful for trouble shooting during installation. Joining your group’s project on GitHub is explained with screenshots in a brief tutorial (Google Doc). Setting up a group project as a collaborative GitHub repo is explained in an external tutorial. There are various ways to install git, and various ways to interact with it. While many tutorials available online rely heavily on git commands in the Terminal, here we avoid using the Terminal as far as reasonably possible (and when we use it, we demonstrate it in a video). 3.2.1 2. How to get GitHub and GitKraken for free For students: GitHub Student Developer Pack For teachers: GitHub Teacher Toolbox https://education.github.com/pack?utm_source=github+gitkraken How to get GitKraken through these packs 3. Happy Git with R This excellent external resource explains the installation process and contains useful information for trouble-shooting: Happy Git and Github for the useR The videos below refer to numbered sections of this tutorial. Refer to the tutorial if you run into issues with the installation, as not everything could be covered in the videos. 4. Video tutorials: installation These videos show the installation process step-by-step, focusing on the most important numbered sections of the Happy Git with R tutorial listed above. If you run into problems, please consult the tutorial. Register GitHub account: 4 Register GitHub account Install Git and GitKraken: 6-8 Install Git and GitKraken Connect to GitHub with PAT: 9-11 Connect to GitHub with PAT Embedded videos: 5. Joining your group’s project on GitHub There are different ways to start a project in RStudio that uses GitHub for version control. Here we focus on the case where you have been invited to join an existing GitHub project. The following tutorial explains how to get a copy (clone) of your team’s GitHub project (repo = repository) into RStudio on your local computer. Tutorial: How to join a collaborative Github project: clone a repo In addition, the Happy Git with R tutorial provides instructions for a wider range of use cases: New project, GitHub first Existing project, GitHub first (the case covered by the above tutorial on cloning a repo) Existing project, GitHub last 6. Setting up a group repo This is for the folks who want to start a project and invite others to it. External tutorial: The Ultimate Github Collaboration Guide 7. Next steps Continue to the Workflows page. "],["workflow.html", "3.3 Workflows", " 3.3 Workflows Helene Wagner 1. Goals The main goal of this section is to help you get over the most intimidating first steps and become familiar with workflows that will get you started on a collaborative research project using R and GitHub. Master a basic workflow (fine if working alone) Embrace a safe workflow (recommended for collaboration) Learn to resolve a merge conflict The tutorials provide step-by-step instructions, illustrated with screenshots. In addition, they introduce key concepts and provide some recommendations to get you, and your group, off to a safe start. The focus lies on workflows that use RStudio’s built-in git functionality and complement it with GitKraken. We thus avoid using git commands in the Terminal. 2. Step-by-step tutorials These tutorials build on each other and should be worked through in sequence, unless you have prior experience with these topics. Basic workflow tutorial: First steps with your cloned repo: pull, commit, push Safe workflow tutorial: A safe workflow for collaboration: branch, pull request, merge, delete Merge conflict tutorial: Surviving your first merge conflicts These tutorials are currently in a beta version. Please use the “Comment” feature in the Google docs to suggests changes that could help other users. Thanks! 3. Further resources GitKraken is providing a series of brief videos that explain relevant concepts and show specific actions: Git Tutorials by GitKraken These videos are organized in three lists: Beginner videos: what is a repository, what is a commit, how to commit, etc. Intermediate videos: how to merge, how to stash, how to cherry pick, etc. Advanced videos: how to resolve a merge conflict, etc. "],["Week1.html", "4 Lab 1: Importing Genetic Data", " 4 Lab 1: Importing Genetic Data In this first regular computer lab of the course, we will explore different ways of importing genetic data into R. Along the way, we will learn about data models in R and how to avoid common errors. View Course Video Interactive Tutorial 1 Worked Example R Exercise Week 1 Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_1.html", "4.1 View Course Video", " 4.1 View Course Video 1. Embedded Video External link: Week 1 video Transcript: Download transcript iframe not supported 2. Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_1.html", "4.2 Interactive Tutorial 1", " 4.2 Interactive Tutorial 1 1. List of R commands covered this week Function Package data utils View utils write.csv utils read.csv utils head utils class base lapply base as.character base read_csv readr read_population gstudio df2genind adegenet dim base 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: the LandGenCourse package must be installed instructions and video are available on [Github] (https://github.com/hhwagner1/LandGenCourse) type: library(LandGenCourse) In RStudio, click on Addins (top menu bar) and select 2. Start Tutorial alternatively you can type: LandGenCourse:::startTutorialAddin() video instructions are [available] (https://www.dropbox.com/s/598kwim7x09m47t/Intro_LandGenCourse_small.mp4?dl=0) Following the To start a tutorial instructions that pop up in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) note: some users will require updates to packages and to restart this process before proceeding b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_1.html", "4.3 Worked Example", " 4.3 Worked Example Helene Wagner 1. Overview of Worked Example a. Goals This worked example shows: How microsatellite data may be coded in a spreadsheet. How such a file is imported into R. How the data are imported into a ‘genind’ object using package ‘adegenet’. How to view information stored in a ‘genind’ object. How to import the data with the ‘gstudio’ package. How to import SNP data Try modifying the code to import your own data! b. Data set This code imports genetic data from 181 individuals of Colombia spotted frogs (Rana luteiventris) from 12 populations. The data are a subsample of the full data set analyzed in Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set. ralu.loci: Data frame with populations and genetic data (181 rows x 9 columns). Included in package LandGenCourse. c. Required R packages Install some packages needed for this worked example. Note: popgraph needs to be installed before installing gstudio. if(!require(&quot;adegenet&quot;)) install.packages(&quot;adegenet&quot;) if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) { install.packages(c(&quot;RgoogleMaps&quot;, &quot;geosphere&quot;, &quot;proto&quot;, &quot;sampling&quot;, &quot;seqinr&quot;, &quot;spacetime&quot;, &quot;spdep&quot;), dependencies=TRUE) remotes::install_github(&quot;dyerlab/popgraph&quot;) } if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) Note: the function library will always load the package, even if it is already loaded, whereas require will only load it if it is not yet loaded. Either will work. library(adegenet) library(gstudio) library(LandGenCourse) library(tibble) library(here) library(vcfR) library(pinfsc50) library(utils) 2. Import data from .csv file a. Preparation Create a new R project for this lab: File &gt; New Project &gt; New Directory &gt; New Project. This will automatically set your working directory to the new project folder. That’s where R will look for files and save anything you export. Note: R does not distinguish between single quotes and double quotes, they are treated as synonyms, but the opening and closing symbols need to match. The dataset ralu.loci is available in the package ‘LandGenCourse’ and can be loaded with data(ralu.loci). data(ralu.loci) Below, however, we’ll import it from a comma separated file (.csv), as this is the recommended format for importing your own data (e.g. from Excel). b. Excel file First we need to copy the file ralu.loci.csv to the downloads folder inside your project folder. The first line checks that the folder downloads exists and creates it if needed. if(!dir.exists(paste0(here(),&quot;/downloads&quot;))) dir.create(paste0(here(),&quot;/downloads&quot;)) file.copy(system.file(&quot;extdata&quot;, &quot;ralu.loci.csv&quot;, package = &quot;LandGenCourse&quot;), paste0(here(), &quot;/downloads/ralu.loci.csv&quot;), overwrite=FALSE) ## [1] FALSE Check that the file ralu.loci.csv is now in the folder downloads within your project folder. From your file system, you can open it with Excel. Note that in Excel, the columns with the loci must be explicitly defined as ‘text’, otherwise Excel is likely to interpret them as times. Excel instructions: Opening an existing .csv file with genetic data: Excel menu: ‘File &gt; New Workbook’. Excel menu: ‘File &gt; Import &gt; CSV File &gt; Import’. Navigate to file. Text Import Wizard: select ‘Delimited’, click ‘Next’. Delimiters: check ‘comma’. Click ‘Next’. Use the ‘Shift’ key to select all columns with loci A - H. Select ‘text’. Click ‘Finish &gt; OK’. Creating a new Excel file to enter genetic data: Label the loci columns. Select all loci columns by clicking on the column headers (hold the ‘Shift’ key to select multiple adjacent columns). Define the data format: ‘Format &gt; Cell &gt; Text’. Enter genetic data, separating the alleles by a colon (e.g., ‘1:3’). Saving Excel file for import in R: File Format: ‘Comma Separated Values (.csv)’ Here is a screenshot of the first few lines of the Excel file with the genetic data: c. Import with function ‘read.csv’ As the file is saved in csv format, we use the function ‘read.csv’ to import it into an R object called Frogs. Display the first few rows and columns to check whether the data have been imported correctly: Frogs &lt;- read.csv(paste0(here(), &quot;/downloads/ralu.loci.csv&quot;), header=TRUE) as_tibble(Frogs) ## # A tibble: 181 × 10 ## SiteName Pop A B C D E F G H ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 AirplaneLake Airplane 2:2 1:1 NA:NA 1:1 1:1 NA:NA 2:2 NA:NA ## 3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 AirplaneLake Airplane 1:1 1:1 NA:NA 2:2 1:2 NA:NA NA:NA NA:NA ## 5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 AirplaneLake Airplane 1:2 1:1 1:1 3:1 1:1 1:1 1:2 4:5 ## 7 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 8 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 9 AirplaneLake Airplane 3:1 1:1 1:1 1:1 1:7 1:1 1:1 3:5 ## 10 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:7 1:1 1:1 3:3 ## # ℹ 171 more rows The data frame Frogs contains 181 rows and 11 columns. Only the first few are shown. The file has a header row with column names. Notice the abbreviations for the R data type under each variable name: &lt;fctr&gt; means that the variable has been coded as a factor. Each row is one individual. Note that for this data set, there is no column with ID or names of individuals. The first two columns indicates the site in long (“SiteName”) and short (“Pop”) format. There are 8 columns with loci, named A - H. Each locus has two alleles, coded with numbers and separated by a colon (:). This coding suggests that the markers are codominant and that the species is diploid. The code for missing values is NA, (which shows up as NA:NA for missing genetic data). There seem to be quite a few missing values. d. Create ID variable We’ll add unique ID’s for the frogs (to show how these would be imported). Here we take, for each frog, a sub-string of Pop containing the first three letters, add the row number, and separate them by a period. Frogs &lt;- data.frame(FrogID = paste(substr(Frogs$Pop, 1, 3), row.names(Frogs), sep=&quot;.&quot;), Frogs) as_tibble(Frogs) ## # A tibble: 181 × 11 ## FrogID SiteName Pop A B C D E F G H ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Air.1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 Air.2 AirplaneLake Airplane 2:2 1:1 NA:NA 1:1 1:1 NA:NA 2:2 NA:NA ## 3 Air.3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 Air.4 AirplaneLake Airplane 1:1 1:1 NA:NA 2:2 1:2 NA:NA NA:NA NA:NA ## 5 Air.5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 Air.6 AirplaneLake Airplane 1:2 1:1 1:1 3:1 1:1 1:1 1:2 4:5 ## 7 Air.7 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 8 Air.8 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 9 Air.9 AirplaneLake Airplane 3:1 1:1 1:1 1:1 1:7 1:1 1:1 3:5 ## 10 Air.10 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:7 1:1 1:1 3:3 ## # ℹ 171 more rows e. Export with function ‘write.csv’ If you want to export (and import) your own files, adapt this code. Note: when using R Notebooks, problems can arise if R looks for files in a different place (more on this in Week 8 Bonus material). Here, we use a safe work-around, with the function here, that will help make your work reproducible and transferable to other computers. For this to work properly, you must work within an R project, and you must have writing permission for the project folder. The function here from the package here finds the path to your project folder. Remove the hashtags and run the two lines to see the path to your project folder, and the path to the (to be created) output folder. The function paste0 simply pastes strings of text together, without any separator (hence the ‘0’ in the function name). here() ## [1] &quot;/Users/helene/Desktop/Github_Projects/LandGenCourse_book&quot; paste0(here(),&quot;/output&quot;) ## [1] &quot;/Users/helene/Desktop/Github_Projects/LandGenCourse_book/output&quot; The following line checks whether an output folder exists in the project folder, and if not, creates one. if(!dir.exists(paste0(here(),&quot;/output&quot;))) dir.create(paste0(here(),&quot;/output&quot;)) Now we can write our data into a csv file in the output folder: write.csv(ralu.loci, paste0(here(),&quot;/output/ralu.loci.csv&quot;), quote=FALSE, row.names=FALSE) To import the file again from the output folder, you could do this: ralu.loci.2 &lt;- read.csv(paste0(here(),&quot;/output/ralu.loci.csv&quot;), header=TRUE) 3. Create ‘genind’ Object a. Read the helpfile We use the function df2genind of the adegenet package to import the loci into a genind object named Frogs.genind. Check the help file for a definition of all arguments and for example code. ?df2genind b. Set all arguments of function ‘df2genind’ Some explanations: X: this is the data frame (or matrix) containing the loci only. Hence we need columns 4 - 11. sep: need to specify the separator between alleles (here: colon) ncode: not needed here because we used a separator, defined under sep. ind.names: here we should indicate the column FrogID. loc.names: by default this will use the column names of the loci. pop: here we should indicate either the column Pop or SiteName. NA.char: how were missing values coded? Here: NA. ploidy: this species is diploid, hence 2. type: marker type, either \"codom\" for codominant markers like microsatellites, or \"PA\" for presence-absence, such as SNP or AFLP markers. Note: you can use either “” or ’’. strata: one way of defining hierarchical sampling levels. We will add this in Week 3. hierarchy: another way of defining hierarchical sampling levels. We’ll skip it for now. Frogs.genind &lt;- df2genind(X=Frogs[,c(4:11)], sep=&quot;:&quot;, ncode=NULL, ind.names= Frogs$FrogID, loc.names=NULL, pop=Frogs$Pop, NA.char=&quot;NA&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) c. Check imported data Printing the genind object’s name lists the number of individuals, loci and alleles, and lists all slots (prefaced by @). Frogs.genind ## /// GENIND OBJECT ///////// ## ## // 181 individuals; 8 loci; 39 alleles; size: 55.2 Kb ## ## // Basic content ## @tab: 181 x 39 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 39 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: df2genind(X = Frogs[, c(4:11)], sep = &quot;:&quot;, ncode = NULL, ind.names = Frogs$FrogID, ## loc.names = NULL, pop = Frogs$Pop, NA.char = &quot;NA&quot;, ploidy = 2, ## type = &quot;codom&quot;, strata = NULL, hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 7-23) d. Summarize ‘genind’ object There is a summary function for genind objects. Notes: Group here refers to the populations, i.e., Group sizes means number of individuals per population. It is a good idea to check the percentage of missing values here. If it is 0%, this may indicate that the coding for missing values was not recognized. If the % missing is very high, on the other hand, there may be some other importing problem. summary(Frogs.genind) ## ## // Number of individuals: 181 ## // Group sizes: 21 8 14 13 7 17 9 20 19 13 17 23 ## // Number of alleles per locus: 3 4 4 4 9 3 4 8 ## // Number of alleles per group: 21 21 20 22 20 19 19 25 18 14 18 26 ## // Percentage of missing data: 10.64 % ## // Observed heterozygosity: 0.1 0.4 0.09 0.36 0.68 0.02 0.38 0.68 ## // Expected heterozygosity: 0.17 0.47 0.14 0.59 0.78 0.02 0.48 0.74 4. View information stored in ‘genind’ object The summary lists each attribute or slot of the genind object and summarizes its content. Technically speaking, genind objects are S4 objects in R, which means that their slots are accessed with the @ sign, rather than the $ sign for attributes of the more commonly used S3 objects. Interestingly, the object does not store the data table that was imported, but converts it to a table of allele counts. a. Slot @tab Table with allele counts, where each column is an allele. Allele name A.1 means “locus A, allele 1”. For a codominant marker (e.g. microsatellite) and a diploid species, allele counts per individual can be 0, 1, 2, or NA=missing. Here are the first few lines: as_tibble(Frogs.genind@tab) ## # A tibble: 181 × 39 ## A.1 A.2 A.3 B.1 B.3 B.2 B.4 C.1 C.2 C.4 C.5 D.1 D.2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 0 0 2 0 0 0 2 0 0 0 2 0 ## 2 0 2 0 2 0 0 0 NA NA NA NA 2 0 ## 3 2 0 0 2 0 0 0 2 0 0 0 2 0 ## 4 2 0 0 2 0 0 0 NA NA NA NA 0 2 ## 5 1 1 0 1 1 0 0 2 0 0 0 2 0 ## 6 1 1 0 2 0 0 0 2 0 0 0 1 0 ## 7 0 2 0 1 1 0 0 2 0 0 0 2 0 ## 8 0 2 0 1 1 0 0 2 0 0 0 2 0 ## 9 1 0 1 2 0 0 0 2 0 0 0 2 0 ## 10 0 2 0 1 1 0 0 2 0 0 0 2 0 ## # ℹ 171 more rows ## # ℹ 26 more variables: D.3 &lt;int&gt;, D.4 &lt;int&gt;, E.1 &lt;int&gt;, E.2 &lt;int&gt;, E.3 &lt;int&gt;, ## # E.7 &lt;int&gt;, E.6 &lt;int&gt;, E.8 &lt;int&gt;, E.5 &lt;int&gt;, E.4 &lt;int&gt;, E.10 &lt;int&gt;, ## # F.1 &lt;int&gt;, F.2 &lt;int&gt;, F.3 &lt;int&gt;, G.1 &lt;int&gt;, G.2 &lt;int&gt;, G.3 &lt;int&gt;, ## # G.5 &lt;int&gt;, H.4 &lt;int&gt;, H.5 &lt;int&gt;, H.3 &lt;int&gt;, H.2 &lt;int&gt;, H.1 &lt;int&gt;, ## # H.6 &lt;int&gt;, H.8 &lt;int&gt;, H.7 &lt;int&gt; b. Slot @loc.n.all Number of alleles per locus, across all populations. Frogs.genind@loc.n.all ## A B C D E F G H ## 3 4 4 4 9 3 4 8 c. Slot @loc.fac This is a factor that indicates for each allele (column in @tab) which locus it belongs to. The levels of the factor correspond to the loci. Frogs.genind@loc.fac ## [1] A A A B B B B C C C C D D D D E E E E E E E E E F F F G G G G H H H H H H H ## [39] H ## Levels: A B C D E F G H d. Slot @all.names List of allele names, separately for each locus. Note: the alleles are treated as text (character), even if they are coded as numbers. They are sorted in order of occurrence in data set. Frogs.genind@all.names ## $A ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## ## $B ## [1] &quot;1&quot; &quot;3&quot; &quot;2&quot; &quot;4&quot; ## ## $C ## [1] &quot;1&quot; &quot;2&quot; &quot;4&quot; &quot;5&quot; ## ## $D ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## ## $E ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;7&quot; &quot;6&quot; &quot;8&quot; &quot;5&quot; &quot;4&quot; &quot;10&quot; ## ## $F ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## ## $G ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;5&quot; ## ## $H ## [1] &quot;4&quot; &quot;5&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;6&quot; &quot;8&quot; &quot;7&quot; e. Further slots The following slots are automatically filled with default values unless specified by user during import into genind object: Slot @ploidy: ploidy for each individual. Slot @type: codominant (microsatellites) or presence/absence (SNP, AFLP) The following slots may be empty unless specified during or after import into ‘genind’ object: Slot @other: placeholder for non-genetic data, e.g. spatial coordinates of individuals. Slot @pop: vector indicating the population that each individual belongs to. Slot @strata: stratification of populations, e.g. within regions or treatments. Slot @hierarchy: optional formula defining hierarchical levels in strata. 5. Import data with ‘gstudio’ package a. Function ‘read_population’ While genind objects are used by many functions for analyzing genetic data, the gstudio package provides an interface for additional analysis. We use the function read_population to import the data. ?read_population b. Set arguments Some explanations: path: This is the path to the .csv file. If it is in the current working directory, then the filename is sufficient. type: here we have ‘separated’ data, as the alleles at each locus are separated by a colon. locus.columns: this would be columns 3 - 10 in this case, because we are importing from the .csv file directly, which does not contain the FrogID variable. We can add that later. phased: default is fine here. sep: here this refers to the .csv file! The separater is a comma. header: yes we have column names. Working directory confusion: If you are working within an R project folder, then that folder is automatically set as your working directory. If you are working in an R session outside of an R project, then you need to actively set the working directory in the R Studio menu under: Session &gt; Set working directory &gt; Choose Directory. Frogs.gstudio &lt;- read_population(path=system.file(&quot;extdata&quot;, &quot;ralu.loci.csv&quot;, package = &quot;LandGenCourse&quot;), type=&quot;separated&quot;, locus.columns=c(3:10), phased=FALSE, sep=&quot;,&quot;, header=TRUE) c. Check imported data Check the column types: SiteName and Pop are now character vectors, and the loci columns A - H are now of class locus (it is indicated that ‘locus’ is a S3 object type). str(Frogs.gstudio) ## &#39;data.frame&#39;: 181 obs. of 10 variables: ## $ SiteName: chr &quot;AirplaneLake&quot; &quot;AirplaneLake&quot; &quot;AirplaneLake&quot; &quot;AirplaneLake&quot; ... ## $ Pop : chr &quot;Airplane&quot; &quot;Airplane&quot; &quot;Airplane&quot; &quot;Airplane&quot; ... ## $ A : &#39;locus&#39; chr &quot;1:1&quot; &quot;2:2&quot; &quot;1:1&quot; &quot;1:1&quot; ... ## $ B : &#39;locus&#39; chr &quot;1:1&quot; &quot;1:1&quot; &quot;1:1&quot; &quot;1:1&quot; ... ## $ C : &#39;locus&#39; chr &quot;1:1&quot; &quot;&quot; &quot;1:1&quot; &quot;&quot; ... ## $ D : &#39;locus&#39; chr &quot;1:1&quot; &quot;1:1&quot; &quot;1:1&quot; &quot;2:2&quot; ... ## $ E : &#39;locus&#39; chr &quot;1:2&quot; &quot;1:1&quot; &quot;3:3&quot; &quot;1:2&quot; ... ## $ F : &#39;locus&#39; chr &quot;1:1&quot; &quot;&quot; &quot;1:1&quot; &quot;&quot; ... ## $ G : &#39;locus&#39; chr &quot;1:1&quot; &quot;2:2&quot; &quot;1:1&quot; &quot;&quot; ... ## $ H : &#39;locus&#39; chr &quot;4:5&quot; &quot;&quot; &quot;3:3&quot; &quot;&quot; ... d. Add FrogID Frogs.gstudio is a data frame, and we can manipulate like any other data frame. Here we will add the variable FrogID as the first column. Frogs.gstudio &lt;- data.frame(FrogID=Frogs$FrogID, Frogs.gstudio) head(Frogs.gstudio) ## FrogID SiteName Pop A B C D E F G H ## 1 Air.1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 Air.2 AirplaneLake Airplane 2:2 1:1 1:1 1:1 2:2 ## 3 Air.3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 Air.4 AirplaneLake Airplane 1:1 1:1 2:2 1:2 ## 5 Air.5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 Air.6 AirplaneLake Airplane 1:2 1:1 1:1 1:3 1:1 1:1 1:2 4:5 e. Convert to ‘genind’ object We can use the dataframe with the locus objects from gstudio to import the data into a genind object. In this case, we set sep=\":\", because the locus object in gstudio stores the alleles at each locus separated by a colon, and NA.char=\"\", because gstudio stores missing values as empty cells. Frogs.genind2 &lt;- adegenet::df2genind(X=Frogs.gstudio[,c(4:11)], sep=&quot;:&quot;, ncode=NULL, ind.names=Frogs.gstudio$FrogID, loc.names=NULL, pop=Frogs.gstudio$Pop, NA.char=&quot;&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) Frogs.genind2 ## /// GENIND OBJECT ///////// ## ## // 181 individuals; 8 loci; 39 alleles; size: 55.5 Kb ## ## // Basic content ## @tab: 181 x 39 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 39 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::df2genind(X = Frogs.gstudio[, c(4:11)], sep = &quot;:&quot;, ## ncode = NULL, ind.names = Frogs.gstudio$FrogID, loc.names = NULL, ## pop = Frogs.gstudio$Pop, NA.char = &quot;&quot;, ploidy = 2, type = &quot;codom&quot;, ## strata = NULL, hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 7-23) 6. Importing SNP data a. Import from text file First, we’ll import a relatively small SNP dataset from a text file. This dataset will be analyzed in the Week 6 Worked Example. The data are stored in a tab-delimited text file. Let’s have a look at the first few rows and columns: family: these are ID’s of the genotyped tree individuals (from which seeds were grown in a common-garden experiment, hence they are called “family” in the dataset). population: the trees were sampled from a set of populations. snpXXX.Plmn: each column is a diploid SNP. To import your own data, replace infile by the path to the text file with your SNP data. Change settings as necessary (check whether data are separated by tabs or commas!). infile &lt;- system.file(&quot;extdata&quot;, &quot;WWP_SNP_genotypes.txt&quot;, package = &quot;LandGenCourse&quot;) Trees &lt;- read.table(infile, header = TRUE, sep = &quot;\\t&quot;) Trees[1:6, 1:6] ## family population snp102.Plmn snp106.Plmn snp108.Plmn snp110.Plmn ## 1 59 blk cyn CC AC GG GG ## 2 60 blk cyn AC AA AG GG ## 3 61 blk cyn CC AA AG GG ## 4 63 blk cyn CC AA GG GG ## 5 64 blk cyn AC AA AG GG ## 6 65 blk cyn AC AA &lt;NA&gt; GG We should replace the SNP column names: if they contain a period ., then the function df2genind thinks that the first part indicates the locus and the second part the allele. The following line of code splits each column name by the period and only retains the first part. names(Trees) &lt;- unlist(lapply(names(Trees), function(x) strsplit(x, &quot;[.]&quot;)[[1]][1])) To import these data frame into a genind object, we need to tell R how the SNP data are coded (check the df2genind help file!) ncode = 1: each allele (A, C, G, T) is coded with a single character. sep = \"\": the two alleles per individual and locus are not separated by any symbol. NA.char= \"NA\": missing values are coded as NA. Trees.genind &lt;- adegenet::df2genind(X=Trees[,-c(1:2)], sep=&quot;&quot;, ncode=1, ind.names=Trees$family, loc.names=NULL, pop=Trees$population, NA.char=&quot;NA&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) Trees.genind ## /// GENIND OBJECT ///////// ## ## // 157 individuals; 164 loci; 324 alleles; size: 302 Kb ## ## // Basic content ## @tab: 157 x 324 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 1-2) ## @loc.fac: locus factor for the 324 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::df2genind(X = Trees[, -c(1:2)], sep = &quot;&quot;, ncode = 1, ## ind.names = Trees$family, loc.names = NULL, pop = Trees$population, ## NA.char = &quot;NA&quot;, ploidy = 2, type = &quot;codom&quot;, strata = NULL, ## hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 9-22) b. Import from vcf file to genind object A common data format for genomic data is vcf. Here’s some background information (all but the first are vignettes from the vcfR package): PDF defining the data format: https://samtools.github.io/hts-specs/VCFv4.3.pdf Tutorial on data format: https://cran.r-project.org/web/packages/vcfR/vignettes/vcf_data.html Tutorial on quality testing: https://cran.r-project.org/web/packages/vcfR/vignettes/intro_to_vcfR.html Tutorial on converting vcf data: https://cran.r-project.org/web/packages/vcfR/vignettes/converting_data.html The R package vcfR makes it easy to import vcf files and convert to other formats. Here we use it to convert the data into a genind object. The following code is adapted from the above listed vcfR vignettes. We’ll import a file that is distributed with the pinfsc50 package. Note that vcf files are commonly compressed in gz archives. To import your own data, replace vcf_file by the path to your own gz archive. vcf_file &lt;- system.file(&quot;extdata&quot;, &quot;pinf_sc50.vcf.gz&quot;, package = &quot;pinfsc50&quot;) vcf &lt;- read.vcfR( vcf_file, verbose = FALSE ) Note that the package vcfR can import three types of genomic data files listed below. We will only use the .vcf file here. .vcf: SNPs .fasta: genomic reference sequence data .gff: annotation Convert to genind object: SNP_genind &lt;- vcfR2genind(vcf) SNP_genind ## /// GENIND OBJECT ///////// ## ## // 18 individuals; 22,031 loci; 44,137 alleles; size: 16.4 Mb ## ## // Basic content ## @tab: 18 x 44137 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 1-5) ## @loc.fac: locus factor for the 44137 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::df2genind(X = t(x), sep = sep) ## ## // Optional content ## - empty - c. Import from vcf file to genlight object This dataset has 18 individuals genotyped at 44,137. It may be better to use a genlight object instead of genind. This is an alternative object type from the package adegenet that can store large genomic data much more efficiently. Note: the current implementation can only store two alleles per locus, loci with more alleles will be dropped and a warning is printed. SNP_genlight &lt;- vcfR2genlight(vcf) ## Warning in vcfR2genlight(vcf): Found 312 loci with more than two alleles. ## Objects of class genlight only support loci with two alleles. ## 312 loci will be omitted from the genlight object. SNP_genlight ## /// GENLIGHT OBJECT ///////// ## ## // 18 genotypes, 21,719 binary SNPs, size: 2.2 Mb ## 31239 (7.99 %) missing data ## ## // Basic content ## @gen: list of 18 SNPbin ## ## // Optional content ## @ind.names: 18 individual labels ## @loc.names: 21719 locus labels ## @chromosome: factor storing chromosomes of the SNPs ## @position: integer storing positions of the SNPs ## @other: a list containing: elements without names Note that instead of 44,137 alleles, we now have 21,719 binary SNPs. This is one of the ways by which the genlight object is more efficient than genind. Overall, the size has been reduced from 16.4 Mb to 2.2 Mb! "],["r-exercise-week-1.html", "4.4 R exercise Week 1", " 4.4 R exercise Week 1 R Notebook: Create a new R Notebook for each weekly exercise. Watch the course video “Week 0: Intro to R Notebooks” as needed. At the end, “knit” it to an html file and view it in your browser. Good news: if you can knit the file, the code can stand by itself (it does not depend on what dyou did in your R session before) and runs without errors. This is a good check. If there are error messages, check the ‘R Markdown’ tab for the code line number and try to fix it. Task: Import the data set pulsatilla_genotypes, which we’ll use in a later lab (Week 14), into gstudio and convert it to a genind object. Data: This file contains microsatellite data for adults and seeds of the herb Pulsatilla vulgaris sampled at seven sites. Reference: DiLeo et al. (2018), Journal of Ecology 106:2242-2255. https://besjournals.onlinelibrary.wiley.com/doi/10.1111/1365-2745.12992 The following code copies the file into the ‘downloads’ folder in your R project folder: file.copy(system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;), paste0(here(), &quot;/downloads/pulsatilla_genotypes.csv&quot;), overwrite=FALSE) ## [1] FALSE Variables: ID: Family ID (i.e., mother and her offspring have the same ID) OffID: ‘0’ for adults, seeds from the same mom are numbered 1, 2, etc. Population: site ID Coordinates: X and Y coordinates (Projection info: EPSG Projection 31468) Loci: seven diploid microsatellites, each with two columns (1 allele per column) Hints: Load packages: Make sure the following packages are loaded: gstudio, here,tibble and adegenet. View data file: Adapt the code from section 2.c to import the raw data set. The file has a header row. View it. How are the genetic data coded? Import data into gstudio: Adapt the code from section 5.b to import the genetic data with ‘gstudio’. The loci are in columns 6 - 19. What setting for type is appropriate for this data set? Check the help file for read_population. Check imported data: Use str or as_tibble to check the imported data. Does each variable have the correct data type? Note: there should be 7 variables of type locus. Check variable types: Create a bulleted list, like the one above, with the variables. For each variable, list their R data type (e.g., numeric, integer, character, logical, factor, locus). Check the cheatsheet “R markdown language” as needed. Convert to genind object: Modify the code from section 5.e to convert the data from gstudio to a genind object. Ignore the warning about duplicate labels. Print a summary of the genind object. Question: What is the range of the number of genotyped individuals per population in this dataset? "],["Week2.html", "5 Lab 2: Spatial Data", " 5 Lab 2: Spatial Data In this second regular computer lab of the course, we will learn about how to analyze land cover with landscape metrics in R. Along the way, we will learn about how to handle spatial data in R. View Course Video Interactive Tutorial 2 Worked Example R Exercise Week 2 Bonus Vignette: ‘sf’ package, plotting categorical maps Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_2.html", "5.1 View Course Video", " 5.1 View Course Video 1. Embedded Video External link: Week 2 video Transcript: Download transcript iframe not supported 2. Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_2.html", "5.2 Interactive Tutorial 2", " 5.2 Interactive Tutorial 2 1. List of R commands covered this week Function Package %&gt;% dplyr group_by dplyr summarize dplyr n dplyr filter dplyr left_join dplyr ==, &gt;, &lt;= base != base sum base merge base names base st_coordinates sf st_drop_geometry sf st_as_sf sf st_crs sf extract terra 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_2.html", "5.3 Worked Example", " 5.3 Worked Example Helene Wagner and Max Hesselbarth 1. Overview of Worked Example 5.3.0.1 a. Background on spatial packages in R There has been a lot of development recently in R regarding object types for geospatial data. We are currently in a transition period, where some packages still expect the “old” object types (based e.g. on packages raster and sp), others expect the “new” object types (based e.g. on packages terra and sf), and some accept both. This vignette uses the newer packages terra and sf. The bonus vignette includes code for converting between sf and sp, and between terra and raster. Further resources: Intro GIS with R: to learn about GIS functionality using R, https://bookdown.org/michael_bcalles/gis-crash-course-in-r/data.html#vector-data R Tutorial: For an introductory tutorial on spatial data analysis with R, which explains object types in more detail than this worked example, see: https://rpubs.com/jguelat/fdsfd Advanced: For a thorough resource on spatial data analysis with R, see this excellent Gitbook: https://geocompr.robinlovelace.net/index.html. b. Goals This worked example shows: How to import spatial coordinates and site attributes as spatially referenced data. How to plot raster data in R and overlay sampling locations. How to calculate landscape metrics. How to extract landscape data at sampling locations and within a buffer around them. Try modifying the code to import your own data! c. Data set This code builds on data and code from the GeNetIt package by Jeff Evans and Melanie Murphy. Landscape metrics will be calculated with the landscapemetrics package described in: Hesselbarth et al. (2019), Ecography 42: 1648-1657. This code uses landscape data and spatial coordinates from 30 locations where Colombia spotted frogs (Rana luteiventris) were sampled for the full data set analyzed by Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set. ralu.site: sf object with UTM coordinates (zone 11) and 17 site variables for 31 sites. The data are included in the ‘GeNetIt’ package, for meta data type: ?ralu.site We will extract values at sampling point locations and within a local neighborhood (buffer) from six raster maps (see Murphy et al. 2010 for definitions), which are included with the GeNetIt package as a SpatialPixelsDataFrame called ‘rasters’: cti: Compound Topographic Index (“wetness”) err27: Elevation Relief Ratio ffp: Frost Free Period gsp: Growing Season Precipitation hli: Heat Load Index nlcd: USGS Landcover (categorical map) d. Required R packages Install some packages needed for this worked example. if(!requireNamespace(&quot;GeNetIt&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/GeNetIt&quot;) library(LandGenCourse) library(here) library(landscapemetrics) library(dplyr) library(sf) library(terra) library(GeNetIt) library(tibble) library(tmap) library(RColorBrewer) 2. Import site data from .csv file a. Import data into an sf object The site data are already in an sf object named ralu.site that comes with the package GeNetIt. Use data(ralu.site) to load it. This will create an object ralu.site. data(ralu.site) class(ralu.site) ## [1] &quot;sf&quot; &quot;data.frame&quot; To demonstrate how to create an sf object from two data frames (one with the coordinates and one with the attribute data), we’ll extract these data frames from the sf object ralu.site and then recreate the sf object (we’ll call it Sites) from the two data frames. We can extract the coordinates with the function st_coordinates: Coordinates &lt;- st_coordinates(ralu.site) head(Coordinates) ## X Y ## [1,] 688816.6 5003207 ## [2,] 688494.4 4999093 ## [3,] 687938.4 5000223 ## [4,] 689732.8 5002522 ## [5,] 690104.0 4999355 ## [6,] 688742.5 4997481 Question: What are the variable names for the spatial coordinates? Similarly, we can drop the geometry (spatial information) from the sf object to reduce it to a data frame: Data &lt;- st_drop_geometry(ralu.site) class(Data) ## [1] &quot;data.frame&quot; Now we can create an sf object again. Here, we: combine the two data frames Data and Coordinates into a single data frame with function data.frame, use the function st_as_sf from the sf package to convert this data frame to an sf object, tell R that the variables with the coordinates are called “X” and “Y”. Sites &lt;- data.frame(Data, Coordinates) Sites.sf &lt;- st_as_sf(Sites, coords=c(&quot;X&quot;, &quot;Y&quot;)) head(Sites.sf) ## Simple feature collection with 6 features and 17 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 687938.4 ymin: 4997481 xmax: 690104 ymax: 5003207 ## CRS: NA ## SiteName Drainage Basin Substrate ## 1 AirplaneLake ShipIslandCreek Sheepeater Silt ## 2 BachelorMeadow WilsonCreek Skyhigh Silt ## 3 BarkingFoxLake WaterfallCreek Terrace Silt ## 4 BirdbillLake ClearCreek Birdbill Sand ## 5 BobLake WilsonCreek Harbor Silt ## 6 CacheLake WilsonCreek Skyhigh Silt ## NWI AREA_m2 PERI_m Depth_m TDS FISH ACB AUC ## 1 Lacustrine 62582.2 1142.8 21.64 2.5 1 0 0.411 ## 2 Riverine_Intermittent_Streambed 225.0 60.0 0.40 0.0 0 0 0.000 ## 3 Lacustrine 12000.0 435.0 5.00 13.8 1 0 0.300 ## 4 Lacustrine 12358.6 572.3 3.93 6.4 1 0 0.283 ## 5 Palustrine 4600.0 321.4 2.00 14.3 0 0 0.000 ## 6 Palustrine 2268.8 192.0 1.86 10.9 0 0 0.000 ## AUCV AUCC AUF AWOOD AUFV geometry ## 1 0 0.411 0.063 0.063 0.464 POINT (688816.6 5003207) ## 2 0 0.000 1.000 0.000 0.000 POINT (688494.4 4999093) ## 3 0 0.300 0.700 0.000 0.000 POINT (687938.4 5000223) ## 4 0 0.283 0.717 0.000 0.000 POINT (689732.8 5002522) ## 5 0 0.000 0.500 0.000 0.500 POINT (690104 4999355) ## 6 0 0.000 0.556 0.093 0.352 POINT (688742.5 4997481) Question: Where and how are the spatial coordinates shown in the sf object Sites.sf? To illustrate importing spatial data from Excel, here we export the combined data frame as a csv file, import it again as a data frame, then convert it to an sf object. First we create a folder output if it does not yet exist. Note: to run the code below, remove all the hashtags # at the beginning of the lines to un-comment them. This part assumes that you have writing permission on your computer. Alternatively, try setting up your R project folder on an external drive where you have writing permission. The code below does the following: Line 1: Load package here that helps with file paths. Line 2: Check if folder output exists, and if not, create it. Line 3: Export the combined data frame as a .csv file. Line 4: Re-imports the .csv file as a data.frame object Sites. Line 5: Create sf object Sites.sf from df`. #require(here) #if(!dir.exists(here(&quot;output&quot;))) dir.create(here(&quot;output&quot;)) #write.csv(data.frame(Data, Coordinates), file=here(&quot;output/ralu.site.csv&quot;), quote=FALSE, row.names=FALSE) #Sites &lt;- read.csv(here(&quot;output/ralu.site.csv&quot;), header=TRUE) #Sites.sf &lt;- st_as_sf(df, coords=c(&quot;X&quot;, &quot;Y&quot;)) The sf object Sites.sf contains 17 attribute variables and one variable geometry that contains the spatial information. Now R knows these are spatial data and knows how to handle them. b. Add spatial reference data Before we can combine the sampling locations with other spatial datasets, such as raster data, we need to tell R where on Earth these locations are (georeferencing). This is done by specifying the “Coordinate Reference System” (CRS). For a great explanation of projections, see: https://michaelminn.net/tutorials/gis-projections/index.html For more information on CRS, see: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf We know that these coordinates are UTM zone 11 (Northern hemisphere) coordinates. We define it here by its EPSG code (32611). You can search for EPSG codes here: https://epsg.io/32611. Here we call the function and the package simultaneously (this is good practice, as it helps keep track of where the functions in your code come from). st_crs(Sites.sf) &lt;- 32611 Question: Print Sites.sf and check what CRS is listed in the header. Important: the code above, using function st_crs, only declares the existing projection, it does not change the coordinates to that projection! However, the ralu.site dataset uses a slightly different definition of the projection (the difference is in the datum argument WGS84 vs. NAD83). Hence it may be safer to simply copy the crs information from ralu.site to Sites.sf. Again, this does not change the projection but declares that Sites.sf has the same projection as ralu.site. R even prints a warning to remind us of this: st_crs(Sites.sf) &lt;- st_crs(ralu.site) ## Warning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for ## that c. Change projection In case we needed to transform the projection, e.g., from UTM zone 11 to longitude/latitude (EPSG code: 4326), we could create a new sf object Sites.sf.longlat. We use the function st_transform to change the projection from the projection of the old object Sites.sf to the “longlat” coordinate system, which we define by the argument crs. With Sites.sf.longlat &lt;- st_transform(Sites.sf, crs = 4326) head(Sites.sf.longlat) ## Simple feature collection with 6 features and 17 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -114.61 ymin: 45.1056 xmax: -114.5828 ymax: 45.15708 ## Geodetic CRS: WGS 84 ## SiteName Drainage Basin Substrate ## 1 AirplaneLake ShipIslandCreek Sheepeater Silt ## 2 BachelorMeadow WilsonCreek Skyhigh Silt ## 3 BarkingFoxLake WaterfallCreek Terrace Silt ## 4 BirdbillLake ClearCreek Birdbill Sand ## 5 BobLake WilsonCreek Harbor Silt ## 6 CacheLake WilsonCreek Skyhigh Silt ## NWI AREA_m2 PERI_m Depth_m TDS FISH ACB AUC ## 1 Lacustrine 62582.2 1142.8 21.64 2.5 1 0 0.411 ## 2 Riverine_Intermittent_Streambed 225.0 60.0 0.40 0.0 0 0 0.000 ## 3 Lacustrine 12000.0 435.0 5.00 13.8 1 0 0.300 ## 4 Lacustrine 12358.6 572.3 3.93 6.4 1 0 0.283 ## 5 Palustrine 4600.0 321.4 2.00 14.3 0 0 0.000 ## 6 Palustrine 2268.8 192.0 1.86 10.9 0 0 0.000 ## AUCV AUCC AUF AWOOD AUFV geometry ## 1 0 0.411 0.063 0.063 0.464 POINT (-114.5977 45.15708) ## 2 0 0.000 1.000 0.000 0.000 POINT (-114.6034 45.12016) ## 3 0 0.300 0.700 0.000 0.000 POINT (-114.61 45.13047) ## 4 0 0.283 0.717 0.000 0.000 POINT (-114.5864 45.15067) ## 5 0 0.000 0.500 0.000 0.500 POINT (-114.5828 45.12208) ## 6 0 0.000 0.556 0.093 0.352 POINT (-114.6008 45.1056) Question: What has changed in the summary? d. Map sampling sites on world map Where on earth is this? You could enter the coordinates from the “longlat” projection in Google maps. Note that Google expects the Latitude (Y coordinate) first, then the Longitude (X coordinate). In the variable geometry, longitude (e.g., -114.5977 for the first data point) is listed before latitude (45.15708 for the first data point). Thus, to locate the first site in Google maps, you will need to enter 45.15708, -114.5977. There is a much easier way to find out where on Earth the sampling points are located. And we don’t even need to change the coordinates to longitude/latitude - R will do this for us internally. We load the R package tmap (which stands for “thematic maps”) With tmap_mode(\"view\"), we indicate that we want to create an interactive map. With tm_shape(Sites.sf) we select the data set (Sites.sf`) to be displayed. For now, we only plot the location, no attribute information. With tm_sf, we specify how the points should be displayed (color, size). require(tmap) tmap_mode(&quot;view&quot;) ## ℹ tmap modes &quot;plot&quot; - &quot;view&quot; ## ℹ toggle with `tmap::ttm()` tm_shape(Sites.sf) + tm_sf(col=&quot;red&quot;, size=1) Question: Try changing the basemap by clicking on the Layers symbol and selecting a different map. Which map is most informative for these data? Zoom in and out to find out where on Earth the sampling points are located. Note: see this week’s bonus materials to learn more about creating maps with tmap. 3. Display raster data and overlay sampling locations, extract data a. Display raster data The raster data for this project are already available in the package GeNetIt. They are stored as a SpatRaster object from package terra. RasterMaps &lt;- rast(system.file(&quot;extdata/covariates.tif&quot;, package=&quot;GeNetIt&quot;)) class(RasterMaps) ## [1] &quot;SpatRaster&quot; ## attr(,&quot;package&quot;) ## [1] &quot;terra&quot; Printing the name of the SpatRaster object displays a summary. A few explanations: dimensions: number of rows (nrow), number of columns (ncol), number of layers (nlyr). So we see there are 6 layers (i.e., variables). resolution: cell size is 30 m both in x and y directions (typical for Landsat-derived remote sensing data) extent: the bounding box that defines the spatial extent of the raster data. coord.ref: projected in UTM zone 11, with ‘datum’ (NAD83). source: the file from which the data were imported. names, min values, max values: summary of the layers (variables). RasterMaps ## class : SpatRaster ## size : 426, 358, 6 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 11N (EPSG:26911) ## source : covariates.tif ## names : cti, err27, ffp, gsp, hli, nlcd ## min values : 0.8429851, 0.03906551, 0, 227.0000, 1014, 11 ## max values : 23.7147598, 0.76376426, 51, 338.0697, 9263, 95 Now we can use plot, which knows what to do with a raster SpatRaster object. Note: layer nlcd is a categorical map of land cover types. See this week’s bonus materials for how to better display a categorical map in R. plot(RasterMaps) Some layers seem to show a similar pattern. It is easy to calculate the correlation between quantitative raster layers. Here, the last layer ncld, is in fact categorical (land cover type), and it’s correlation here is meaningless. layerCor(RasterMaps, &quot;pearson&quot;, na.rm=TRUE)$pearson ## NULL Questions: What does each value in this table represent? Which layers are most strongly correlated? What do these variables (layers) represent? Is the correlation positive or negative? b. Change color ramp, add sampling locations We can specify a color ramp by setting the col argument. The default is terrain.colors(255). Here we change it to rainbow(9), a rainbow color palette with 9 color levels. Note: To learn about options for the plot function for SpatRaster objects, access the help file by typing ?plot and select Make a map. We can add the sampling locations (if we plot only a single raster layer). Here we use rev to reverse the color ramp for plotting raster layer ffp, and add the sites as white circles with black outlines. par(mar=c(3,3,1,2)) plot(RasterMaps, &quot;ffp&quot;, col=rev(rainbow(9))) points(Sites.sf, pch=21, col=&quot;black&quot;, bg=&quot;white&quot;) Question: Recall that ‘ffp’ stands for frost free period (in days). What do you think is the average length of the frost free period at theses sampling sites? c. Extract raster values at sampling locations The following code adds six variables to Sites.sf. However, it automatically changes the object type to a SpatVector object, which we’ll call Sites.terra to remind us of the object type. Technically we combine the columns of the existing data frame Sites.sf with the new columns in a new data frame with the same name. Sites.terra &lt;- terra::extract(RasterMaps, Sites.sf, bind=TRUE) We can convert back to an sf object: Sites.sf &lt;- sf::st_as_sf(Sites.terra) Sites.sf ## Simple feature collection with 31 features and 23 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 686908.5 ymin: 4994089 xmax: 690890.1 ymax: 5004435 ## Projected CRS: PROJCRS[&quot;unknown&quot;, ## BASEGEOGCRS[&quot;unknown&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6269]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8901]]], ## CONVERSION[&quot;UTM zone 11N&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-117, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]], ## ID[&quot;EPSG&quot;,16011]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] ## First 10 features: ## SiteName Drainage Basin Substrate ## 1 AirplaneLake ShipIslandCreek Sheepeater Silt ## 2 BachelorMeadow WilsonCreek Skyhigh Silt ## 3 BarkingFoxLake WaterfallCreek Terrace Silt ## 4 BirdbillLake ClearCreek Birdbill Sand ## 5 BobLake WilsonCreek Harbor Silt ## 6 CacheLake WilsonCreek Skyhigh Silt ## 7 DoeLake WilsonCreek Skyhigh Silt ## 8 EggWhiteLake WilsonCreek Skyhigh Silt ## 9 ElenasLake ShipIslandCreek Sheepeater Sand ## 10 FawnLake WilsonCreek Skyhigh Silt ## NWI AREA_m2 PERI_m Depth_m TDS FISH ACB AUC ## 1 Lacustrine 62582.2 1142.8 21.64 2.5 1 0 0.411 ## 2 Riverine_Intermittent_Streambed 225.0 60.0 0.40 0.0 0 0 0.000 ## 3 Lacustrine 12000.0 435.0 5.00 13.8 1 0 0.300 ## 4 Lacustrine 12358.6 572.3 3.93 6.4 1 0 0.283 ## 5 Palustrine 4600.0 321.4 2.00 14.3 0 0 0.000 ## 6 Palustrine 2268.8 192.0 1.86 10.9 0 0 0.000 ## 7 Lacustrine 13034.9 463.2 6.03 10.0 1 0 0.415 ## 8 Palustrine 4544.5 291.9 3.30 2.4 0 0 0.000 ## 9 Palustrine 0.0 0.0 0.00 0.0 0 0 0.000 ## 10 Palustrine 3865.9 237.7 1.98 3.6 0 0 0.000 ## AUCV AUCC AUF AWOOD AUFV cti err27 ffp gsp hli nlcd ## 1 0.000 0.411 0.063 0.063 0.464 6.551698 0.2211860 8 286.0000 1438 42 ## 2 0.000 0.000 1.000 0.000 0.000 7.257386 0.3399709 8 286.0000 1718 71 ## 3 0.000 0.300 0.700 0.000 0.000 7.587409 0.2636720 9 283.0000 1308 42 ## 4 0.000 0.283 0.717 0.000 0.000 4.864036 0.3027736 5 294.1377 1416 12 ## 5 0.000 0.000 0.500 0.000 0.500 10.962278 0.5830295 6 290.0000 1685 42 ## 6 0.000 0.000 0.556 0.093 0.352 7.817918 0.4895256 11 274.0000 1277 42 ## 7 0.171 0.585 0.341 0.000 0.073 6.940942 0.1684043 11 273.0000 1306 42 ## 8 0.047 0.047 0.686 0.209 0.058 6.560551 0.4499388 10 279.0000 1244 42 ## 9 0.000 0.000 0.000 0.000 0.000 4.031181 0.3834549 2 308.0000 1169 71 ## 10 0.000 0.000 1.000 0.000 0.000 10.988009 0.1625656 11 273.0000 1296 42 ## geometry ## 1 POINT (688816.6 5003207) ## 2 POINT (688494.4 4999093) ## 3 POINT (687938.4 5000223) ## 4 POINT (689732.8 5002522) ## 5 POINT (690104 4999355) ## 6 POINT (688742.5 4997481) ## 7 POINT (688962.4 4996675) ## 8 POINT (688539.3 4998146) ## 9 POINT (688878.7 5004435) ## 10 POINT (688901.5 4996837) Let’s calculate the mean length of the frost free period for these sites: mean(Sites.sf$ffp) ## [1] 8.0963 What land cover type is assigned to the most sampling units? Let’s tabulate them. table(Sites.sf$nlcd) ## ## 11 12 42 52 71 90 ## 3 1 21 1 4 1 Note: the land cover types are coded by numbers. Check here what the numbers mean: https://www.mrlc.gov/data/legends/national-land-cover-database-2016-nlcd2016-legend Question: A total of 21 sites are classified as 42. What is this most frequent land cover type? 4. Calculate landscape metrics We are going to use the package landscapemetrics. It is an R package to calculate landscape metrics in a tidy workflow (for more information about tidy data see here). landscapemetrics is basically a reimplementation of ‘FRAGSTATS’, which allows an integration into larger workflows within the R environment. The core of the package are functions to calculate landscape metrics, but also several auxiliary functions exist. To facilitate an integration into larger workflows, landscapemetrics is based on the terra and stars packages. It expects a raster with integers that represent land cover classes. To check if a raster is suitable for landscapemetrics, run the check_landscape() function first. The function checks the coordinate reference system (and mainly if units are in meters) and if the raster values are discrete classes. If the check fails, the calculation of metrics is still possible, however, especially metrics that are based on area and distances must be used with caution. landscapemetrics::check_landscape(RasterMaps) ## Warning: Caution: Land-cover classes must be decoded as integer values. ## Warning: Caution: More than 30 land cover-classes - Please check if discrete ## land-cover classes are present. ## Warning: Caution: Land-cover classes must be decoded as integer values. ## Warning: Caution: More than 30 land cover-classes - Please check if discrete ## land-cover classes are present. ## Warning: Caution: Land-cover classes must be decoded as integer values. ## Warning: Caution: More than 30 land cover-classes - Please check if discrete ## land-cover classes are present. ## Warning: Caution: Land-cover classes must be decoded as integer values. ## Warning: Caution: More than 30 land cover-classes - Please check if discrete ## land-cover classes are present. ## Warning: Caution: More than 30 land cover-classes - Please check if discrete ## land-cover classes are present. ## layer crs units class n_classes OK ## 1 1 projected m non-integer 150973 ✖ ## 2 2 projected m non-integer 151161 ✖ ## 3 3 projected m non-integer 37266 ✖ ## 4 4 projected m non-integer 79335 ✖ ## 5 5 projected m integer 4776 ❓ ## 6 6 projected m integer 8 ✔ Question: Which raster layer(s) are suitable for calculating landscape metrics? Why are the others not suitable? There are three different levels of landscape metrics. Firstly, metrics can be calculated for each single patch (a patch is defined as neighbouring cells of the same class). Secondly, metrics can be calculated for a certain class (i.e. all patches belonging to the same class) and lastly for the whole landscape. All these levels are implemented and easily accessible in landscapemetrics. All functions to calculate metrics start with lsm_ (for landscapemetrics). The second part of the name specifies the level (patch - p, class - c or landscape - l). Lastly, the final part of the function name is the abbreviation of the corresponding metric (e.g. enn for the Euclidean nearest-neighbor distance). To list all available metrics, you can use the list_lsm() function. The function also allows to show metrics filtered by level, type or metric name. For more information about the metrics, please see either the corresponding helpfile(s) or https://r-spatialecology.github.io/landscapemetrics. List available diversity metrics: landscapemetrics::list_lsm(level = &quot;landscape&quot;, type = &quot;diversity metric&quot;) ## # A tibble: 9 × 5 ## metric name type level function_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 msidi modified simpson&#39;s diversity index diversity metric land… lsm_l_msidi ## 2 msiei modified simpson&#39;s evenness index diversity metric land… lsm_l_msiei ## 3 pr patch richness diversity metric land… lsm_l_pr ## 4 prd patch richness density diversity metric land… lsm_l_prd ## 5 rpr relative patch richness diversity metric land… lsm_l_rpr ## 6 shdi shannon&#39;s diversity index diversity metric land… lsm_l_shdi ## 7 shei shannon&#39;s evenness index diversity metric land… lsm_l_shei ## 8 sidi simpson&#39;s diversity index diversity metric land… lsm_l_sidi ## 9 siei simspon&#39;s evenness index diversity metric land… lsm_l_siei List available area metrics: landscapemetrics::list_lsm(metric = &quot;area&quot;) ## # A tibble: 7 × 5 ## metric name type level function_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 area patch area area and edge metric patch lsm_p_area ## 2 area_cv patch area area and edge metric class lsm_c_area_cv ## 3 area_mn patch area area and edge metric class lsm_c_area_mn ## 4 area_sd patch area area and edge metric class lsm_c_area_sd ## 5 area_cv patch area area and edge metric landscape lsm_l_area_cv ## 6 area_mn patch area area and edge metric landscape lsm_l_area_mn ## 7 area_sd patch area area and edge metric landscape lsm_l_area_sd List available aggregation metrics: landscapemetrics::list_lsm(level = c(&quot;class&quot;, &quot;landscape&quot;), type = &quot;aggregation metric&quot;, simplify = TRUE) ## [1] &quot;lsm_c_ai&quot; &quot;lsm_c_clumpy&quot; &quot;lsm_c_cohesion&quot; &quot;lsm_c_division&quot; ## [5] &quot;lsm_c_enn_cv&quot; &quot;lsm_c_enn_mn&quot; &quot;lsm_c_enn_sd&quot; &quot;lsm_c_iji&quot; ## [9] &quot;lsm_c_lsi&quot; &quot;lsm_c_mesh&quot; &quot;lsm_c_nlsi&quot; &quot;lsm_c_np&quot; ## [13] &quot;lsm_c_pd&quot; &quot;lsm_c_pladj&quot; &quot;lsm_c_split&quot; &quot;lsm_l_ai&quot; ## [17] &quot;lsm_l_cohesion&quot; &quot;lsm_l_contag&quot; &quot;lsm_l_division&quot; &quot;lsm_l_enn_cv&quot; ## [21] &quot;lsm_l_enn_mn&quot; &quot;lsm_l_enn_sd&quot; &quot;lsm_l_iji&quot; &quot;lsm_l_lsi&quot; ## [25] &quot;lsm_l_mesh&quot; &quot;lsm_l_np&quot; &quot;lsm_l_pd&quot; &quot;lsm_l_pladj&quot; ## [29] &quot;lsm_l_split&quot; a. Calculate patch-, class- and landscape level landscape metrics Note: This section explains different ways of calculating a selection of landscape metrics from a raster map with ‘landscapemetrics’. If this seems too technical for a first go, you may jump to section 4b. To calculate a single metric, just use the corresponding function. The result of all landscape metric functions is always an identically structured tibble (i.e. an advanced data.frame). The first coloumn is the layer id (only interesting for e.g. a RasterStack). The second coloumn specifies the level (‘patch’, ‘class’ or ‘landscape’). The third coloumn is the class ID (NA on landscape level) and the fourth coloumn is the patch ID (NA on class- and landscape level). Lastly, The fith coloumn is the abbreviation of the metric and finally the corresponding value in the last coloumn. ## c.lculate percentage of landscape of class percentage_class &lt;- lsm_c_pland(landscape = RasterMaps$nlcd) percentage_class ## # A tibble: 8 × 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 class 11 NA pland 0.948 ## 2 1 class 12 NA pland 0.441 ## 3 1 class 31 NA pland 0.394 ## 4 1 class 42 NA pland 59.1 ## 5 1 class 52 NA pland 11.0 ## 6 1 class 71 NA pland 28.1 ## 7 1 class 90 NA pland 0.0557 ## 8 1 class 95 NA pland 0.0210 Questions: What percentage of the overall landscape (total map) is evergreen forest (class 42)? What percentage of the landscape is classified as wetlands (classes 90 and 95)? Because the resulting tibble is type stable, you can easily row-bind (rbind) different metrics (even of different levels): metrics &lt;- rbind( landscapemetrics::lsm_c_pladj(RasterMaps$nlcd), landscapemetrics::lsm_l_pr(RasterMaps$nlcd), landscapemetrics::lsm_l_shdi(RasterMaps$nlcd) ) metrics ## # A tibble: 10 × 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 class 11 NA pladj 77.9 ## 2 1 class 12 NA pladj 52.5 ## 3 1 class 31 NA pladj 48.8 ## 4 1 class 42 NA pladj 89.9 ## 5 1 class 52 NA pladj 58.6 ## 6 1 class 71 NA pladj 81.6 ## 7 1 class 90 NA pladj 41.8 ## 8 1 class 95 NA pladj 32.8 ## 9 1 landscape NA NA pr 8 ## 10 1 landscape NA NA shdi 1.01 To calculate a larger set of landscape metrics, you can just use the wrapper calculate_lsm(). The arguments are similar to list_lsm(), e.g. you can specify the level or the type of metrics to calculate. Alternatively, you can also provide a vector with the function names of metrics to calculate to the what argument. However, watch out, for large rasters and many metrics, this can be rather slow (set progress = TRUE to get an progress report on the console). Also, we suggest to not just calculate all available metrics, but rather think about which ones might be actually suitable for your research question. Calculate all patch-level metrics using wrapper: nlcd_patch &lt;- landscapemetrics::calculate_lsm(landscape = RasterMaps$nlcd, level = &quot;patch&quot;) nlcd_patch ## # A tibble: 19,776 × 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 patch 11 1 area 0.45 ## 2 1 patch 11 2 area 0.45 ## 3 1 patch 11 3 area 41.7 ## 4 1 patch 11 4 area 0.72 ## 5 1 patch 11 5 area 6.12 ## 6 1 patch 11 6 area 0.9 ## 7 1 patch 11 7 area 0.9 ## 8 1 patch 11 8 area 1.89 ## 9 1 patch 11 9 area 0.09 ## 10 1 patch 11 10 area 1.17 ## # ℹ 19,766 more rows Show abbreviation of all calculated metrics: unique(nlcd_patch$metric) ## [1] &quot;area&quot; &quot;cai&quot; &quot;circle&quot; &quot;contig&quot; &quot;core&quot; &quot;enn&quot; &quot;frac&quot; &quot;gyrate&quot; ## [9] &quot;ncore&quot; &quot;para&quot; &quot;perim&quot; &quot;shape&quot; Calculate all aggregation metrics on landscape level: nlcd_landscape_aggr &lt;- landscapemetrics::calculate_lsm(landscape = RasterMaps$nlcd, level = &quot;landscape&quot;, type = &quot;aggregation metric&quot;) nlcd_landscape_aggr ## # A tibble: 14 × 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 landscape NA NA ai 84.1 ## 2 1 landscape NA NA cohesion 99.2 ## 3 1 landscape NA NA contag 62.7 ## 4 1 landscape NA NA division 0.804 ## 5 1 landscape NA NA enn_cv 218. ## 6 1 landscape NA NA enn_mn 129. ## 7 1 landscape NA NA enn_sd 281. ## 8 1 landscape NA NA iji 43.0 ## 9 1 landscape NA NA lsi 32.3 ## 10 1 landscape NA NA mesh 2694. ## 11 1 landscape NA NA np 1648 ## 12 1 landscape NA NA pd 12.0 ## 13 1 landscape NA NA pladj 83.7 ## 14 1 landscape NA NA split 5.09 Calculate specific metrics: nlcd_subset &lt;- landscapemetrics::calculate_lsm(landscape = RasterMaps$nlcd, what = c(&quot;lsm_c_pladj&quot;, &quot;lsm_l_pr&quot;, &quot;lsm_l_shdi&quot;)) nlcd_subset ## # A tibble: 10 × 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 class 11 NA pladj 77.9 ## 2 1 class 12 NA pladj 52.5 ## 3 1 class 31 NA pladj 48.8 ## 4 1 class 42 NA pladj 89.9 ## 5 1 class 52 NA pladj 58.6 ## 6 1 class 71 NA pladj 81.6 ## 7 1 class 90 NA pladj 41.8 ## 8 1 class 95 NA pladj 32.8 ## 9 1 landscape NA NA pr 8 ## 10 1 landscape NA NA shdi 1.01 The resulting tibble is easy to integrate into a workflow. For example, to get the ordered patch IDs of the 5% largest patches, the following code could be used. The pipe operator %&gt;% from the dplyr package passes the resulting object automatically to the next function as first argument. Note: the last step (pulling the id variable only) could be done by adding this to the pipe: %&gt;% dplyr::pull(id). Due to some package inconsistencies, this sometimes created an error. Here we extract the id variable in a separate step as a work-around. id_largest &lt;- nlcd_patch %&gt;% # previously calculated patch metrics dplyr::filter(metric == &quot;area&quot;) %&gt;% # only patch area dplyr::arrange(-value) %&gt;% # order by decreasing size dplyr::filter(value &gt; quantile(value, probs = 0.95)) ## g.t only patches larger than 95% quantile id_largest &lt;- id_largest$id ## g.t only patch id id_largest ## [1] 206 166 1265 1459 1549 1558 1421 427 1434 1385 226 3 386 1205 589 ## [16] 1441 426 1059 1206 324 433 1195 315 1225 712 1266 1377 389 753 1528 ## [31] 894 1510 286 563 240 1411 478 1364 435 1336 812 640 1376 1523 786 ## [46] 467 485 1559 1244 284 1537 1574 718 814 499 864 955 1015 1430 443 ## [61] 393 930 1250 548 851 885 1481 928 1554 43 164 281 1608 716 514 ## [76] 802 1614 729 977 1488 37 357 1353 Because the metric names are only abbreviated, there is also a way to include the full name in the results. For the wrapper, just set full_name = TRUE. For the rowbinded tibble, you can use the provided tibble called lsm_abbreviations_names that comes with the package and use e.g. dplyr::left_join() to combine it with your results. Add full metrics name to result: nlcd_subset_full_a &lt;- landscapemetrics::calculate_lsm(RasterMaps$nlcd, what = c(&quot;lsm_c_pladj&quot;, &quot;lsm_l_pr&quot;, &quot;lsm_l_shdi&quot;), full_name = TRUE) nlcd_subset_full_a ## # A tibble: 10 × 9 ## layer level class id metric value name type function_name ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 class 11 NA pladj 77.9 percentage of l… aggr… lsm_c_pladj ## 2 1 class 12 NA pladj 52.5 percentage of l… aggr… lsm_c_pladj ## 3 1 class 31 NA pladj 48.8 percentage of l… aggr… lsm_c_pladj ## 4 1 class 42 NA pladj 89.9 percentage of l… aggr… lsm_c_pladj ## 5 1 class 52 NA pladj 58.6 percentage of l… aggr… lsm_c_pladj ## 6 1 class 71 NA pladj 81.6 percentage of l… aggr… lsm_c_pladj ## 7 1 class 90 NA pladj 41.8 percentage of l… aggr… lsm_c_pladj ## 8 1 class 95 NA pladj 32.8 percentage of l… aggr… lsm_c_pladj ## 9 1 landscape NA NA pr 8 patch richness dive… lsm_l_pr ## 10 1 landscape NA NA shdi 1.01 shannon&#39;s diver… dive… lsm_l_shdi Add full metrics name to results calculated previously using left_join(): nlcd_subset_full_b &lt;- dplyr::left_join(x = nlcd_subset, y = lsm_abbreviations_names, by = c(&quot;metric&quot;, &quot;level&quot;)) nlcd_subset_full_b ## # A tibble: 10 × 9 ## layer level class id metric value name type function_name ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 class 11 NA pladj 77.9 percentage of l… aggr… lsm_c_pladj ## 2 1 class 12 NA pladj 52.5 percentage of l… aggr… lsm_c_pladj ## 3 1 class 31 NA pladj 48.8 percentage of l… aggr… lsm_c_pladj ## 4 1 class 42 NA pladj 89.9 percentage of l… aggr… lsm_c_pladj ## 5 1 class 52 NA pladj 58.6 percentage of l… aggr… lsm_c_pladj ## 6 1 class 71 NA pladj 81.6 percentage of l… aggr… lsm_c_pladj ## 7 1 class 90 NA pladj 41.8 percentage of l… aggr… lsm_c_pladj ## 8 1 class 95 NA pladj 32.8 percentage of l… aggr… lsm_c_pladj ## 9 1 landscape NA NA pr 8 patch richness dive… lsm_l_pr ## 10 1 landscape NA NA shdi 1.01 shannon&#39;s diver… dive… lsm_l_shdi b. Calculate patch-level landscape metrics for ‘Evergreen Forest’ To only get the results for class 42 (evergreen forest), you can just dplyr::filter() the tibble (or use any other subset method you prefer). forest_patch_metrics &lt;- dplyr::filter(nlcd_patch, class == 42) forest_patch_metrics ## # A tibble: 2,664 × 6 ## layer level class id metric value ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 patch 42 164 area 8.1 ## 2 1 patch 42 165 area 0.18 ## 3 1 patch 42 166 area 3180. ## 4 1 patch 42 167 area 0.27 ## 5 1 patch 42 168 area 3.51 ## 6 1 patch 42 169 area 1.71 ## 7 1 patch 42 170 area 0.18 ## 8 1 patch 42 171 area 0.09 ## 9 1 patch 42 172 area 0.09 ## 10 1 patch 42 173 area 1.98 ## # ℹ 2,654 more rows All functions make heavy use of connected components labeling to delineate patches (neighbouring cells of the same class). To get all patches of every class you can just use get_patches(). This returns a list of layers (one layer per input raster map, here only one) with a separate SpatRaster for each class within the layer. ## c.nnected components labeling of landscape cc_nlcd &lt;- landscapemetrics::get_patches(RasterMaps$nlcd, directions = 8) # summarize the SpatRaster for class 42: cc_nlcd$layer_1$class_42 ## class : SpatRaster ## size : 426, 358, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 11N (EPSG:26911) ## source(s) : memory ## name : lyr.1 ## min value : 164 ## max value : 385 To get only a certain class, just specify the class argument and the neighbourhood rule can be chosen between 8-neighbour rule or 4-neighbour rule with the argument directions. Note: although we only process a single class, the resulting object still is a list with the same structure as above. Thus, we access the SpatRaster for class 42 the same way: cc_forest &lt;- landscapemetrics::get_patches(RasterMaps$nlcd, class = 42) cc_forest$layer_1$class_42 ## class : SpatRaster ## size : 426, 358, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 11N (EPSG:26911) ## source(s) : memory ## name : lyr.1 ## min value : 164 ## max value : 385 To plot the patches you can use the show_patches() function. Here we show patches of class 42 (forest) and class 52 (shrubland). Note that the color indicates patch ID! show_patches(landscape = RasterMaps$nlcd, class = c(42, 52), labels = FALSE) ## $layer_1 It is also possible to visualize only the core area of each patch using show_cores(). The core area is defined as all cells that are further away from the edge of each patch than a specified edge depth (e.g. 5 cells). Here we show core area with edge depth = 5 for class 42; try edge_depth = 1 for comparison: show_cores(landscape = RasterMaps$nlcd, class = c(42), edge_depth = 5, labels = FALSE) ## $layer_1 Note: this may create a warning “no non-missing arguments to min; returning Inf” for each patch that does not have any core area. Here we suppressed the warnings for the chunk with the chunk option warning=FALSE. Lastly, you can plot the map and fill each patch with the corresponding metric value, e.g. patch size, using show_lsm(). Notice that there are two very large patches in class 42: show_lsm(landscape = RasterMaps$nlcd, class = c(42, 52), what = &quot;lsm_p_area&quot;, labels = FALSE) ## $layer_1 c. Extract forest patch size at sampling locations Let’s add forest patch size to the Sites.sf data. To extract landscape metrics of the patch in which each sampling point is located, use extract_lsm(). Which metrics are extracted can be specified by the what argument (similar to calculate_lsm()). However, only patch-level metrics are available. Please be aware that the resulting tibble now has a new column, namely the ID of the sampling point (in the same order as the input points). ## e.tract patch area of all classes: patch_size_sf &lt;- extract_lsm(landscape = RasterMaps$nlcd, y = Sites.sf, what = &quot;lsm_p_area&quot;) ## b.cause we are only interested in the forest patch size, we set all area of class != 42 to 0: patch_size_sf_forest &lt;- dplyr::mutate(patch_size_sf, value = dplyr::case_when(class == 42 ~ value, class != 42 ~ 0)) ## a.d data to sf object: Sites.sf$ForestPatchSize &lt;- patch_size_sf_forest$value Sites.sf$ForestPatchSize ## [1] 3179.88 0.00 3179.88 0.00 6.66 4539.33 4539.33 4539.33 0.00 ## [10] 4539.33 4539.33 23.76 23.76 0.00 0.00 4539.33 0.00 4539.33 ## [19] 3.60 3179.88 0.00 4539.33 3.87 4539.33 0.00 0.00 23.76 ## [28] 3179.88 0.00 4539.33 0.09 d. Plot a bubble map of forest patch size at sampling locations tmap_mode(&quot;view&quot;) ## ℹ tmap modes &quot;plot&quot; - &quot;view&quot; tm_shape(Sites.sf) + tm_bubbles(col=&quot;ForestPatchSize&quot;) #legend01 { background: #FFFFFF; opacity: 1} 5. Sample landscape metrics within buffer around sampling locations The package landscapemetrics has a built-in function sample_lsm to sample metrics in a buffer around sampling locations, which are provided with argument y. You can choose the shape of the buffer window (either a circle or a square) and, with the argument what, which metrics to sample (similar to calculate_lsm()). The argument size specifies the buffer size in map units (e.g., meters): radius for circles, half of the side length for squares. Here, the value size = 500 results in a square window of 1000 m x 1000 m centered at the sampling location. nlcd_sampled &lt;- landscapemetrics::sample_lsm(landscape = RasterMaps$nlcd, what = c(&quot;lsm_l_ta&quot;, &quot;lsm_c_np&quot;, &quot;lsm_c_pland&quot;, &quot;lsm_c_ai&quot;), shape = &quot;square&quot;, y = Sites.sf, size = 500) nlcd_sampled ## # A tibble: 493 × 8 ## layer level class id metric value plot_id percentage_inside ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 class 11 NA ai 88.6 1 98.0 ## 2 1 class 31 NA ai NA 1 98.0 ## 3 1 class 42 NA ai 93.3 1 98.0 ## 4 1 class 52 NA ai 50 1 98.0 ## 5 1 class 71 NA ai 65.9 1 98.0 ## 6 1 class 11 NA np 2 1 98.0 ## 7 1 class 31 NA np 1 1 98.0 ## 8 1 class 42 NA np 2 1 98.0 ## 9 1 class 52 NA np 4 1 98.0 ## 10 1 class 71 NA np 8 1 98.0 ## # ℹ 483 more rows The tibble now contains two additional columns. Firstly, the plot_id (in the same order as the input points) and secondly, the percentage_inside, i.e. what percentage of the buffer around the sampling location lies within the map. (In cases where the sampling location is on the edge of the landscape, the buffer around the sampling location could be only partly within the map). The value can also deviate from 100 % because the sampling locations are not necessarily in the cell center and the actually clipped cells lead to a slightly smaller or larger buffer area. A circular buffer shape increases this effect. It is also possible to get the clippings of the buffer around sampling locations as a RasterLayer. For this, just set return_raster = TRUE. # sample some metrics within buffer around sample location and returning sample # plots as raster nlcd_sampled_plots &lt;- landscapemetrics::sample_lsm(landscape = RasterMaps$nlcd, what = c(&quot;lsm_l_ta&quot;, &quot;lsm_c_np&quot;, &quot;lsm_c_pland&quot;, &quot;lsm_c_ai&quot;), shape = &quot;square&quot;, y = Sites.sf, size = 500, return_raster = TRUE) nlcd_sampled_plots ## # A tibble: 493 × 9 ## layer level class id metric value plot_id percentage_inside ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 class 11 NA ai 88.6 1 98.0 ## 2 1 class 31 NA ai NA 1 98.0 ## 3 1 class 42 NA ai 93.3 1 98.0 ## 4 1 class 52 NA ai 50 1 98.0 ## 5 1 class 71 NA ai 65.9 1 98.0 ## 6 1 class 11 NA np 2 1 98.0 ## 7 1 class 31 NA np 1 1 98.0 ## 8 1 class 42 NA np 2 1 98.0 ## 9 1 class 52 NA np 4 1 98.0 ## 10 1 class 71 NA np 8 1 98.0 ## # ℹ 483 more rows ## # ℹ 1 more variable: raster_sample_plots &lt;list&gt; The result will be a nested tibble containing the plot_id, the metrics and a RasterLayer with the clipped buffers (as a list). Attention: Because several metrics on class- and landscape-level the clipped buffers will be “repeated” several times. Here we show results for the first four sampling locations. We use a for loop to avoid repeating code. unique_plots &lt;- unique(nlcd_sampled_plots$raster_sample_plots)[1:4] par(mfrow = c(2,2)) for(i in 1:4) { plot(unique_plots[[i]], type=&quot;classes&quot;, main = paste(Sites.sf$SiteName[i])) } par(mfrow = c(1,1)) To use consistent colors for the land cover types, we need to tweak things a bit. Determine how many colors are needed (nColor): number of cover types in raster nlcd. Define a set of nColor colors, using a categorical color ramp from package RColorBrewer: Colors For each clipped buffer, we use the function is.element to determine for each land cover type if it is present in the buffer. The vector Present is of type logical (TRUE/FALSE). Then we specify the colors to be used as the subset of Colors for which Present is TRUE. (A shorter alternative would be: col=Colors[Present]). nColors &lt;- nrow(unique(RasterMaps$nlcd)) Colors &lt;- RColorBrewer::brewer.pal(n = nColors, name = &quot;Dark2&quot;) par(mfrow = c(2,2)) for(i in 1:4) { Present &lt;- is.element(unique(RasterMaps$nlcd)$nlcd, unique(unique_plots[[i]])$nlcd) plot(unique_plots[[i]], type=&quot;classes&quot;, col=Colors[Present==TRUE], main = paste(Sites.sf$SiteName[i])) } par(mfrow = c(1,1)) b. Extract landscape metric of choice for a single cover type (as vector) To extract a metrics you can just dplyr::filter() the resulting tibble and pull the value column. Here we filter the results for class == 42 (forest) and metric pland (percentage of landscape) and pull the results as a vector: percentage_forest_500_a &lt;- dplyr::pull(dplyr::filter(nlcd_sampled, class == 42, metric == &quot;pland&quot;), value) percentage_forest_500_a ## [1] 77.50230 39.57759 38.01653 31.40138 40.40404 83.30450 69.16221 86.80927 ## [9] 10.19284 72.81910 92.19467 34.04635 34.84848 27.09447 31.37255 56.23886 ## [17] 59.77961 58.73440 37.64922 60.12111 46.16756 32.32323 23.35640 50.44563 ## [25] 66.93405 30.57851 33.70064 26.53811 45.18717 38.05704 62.47772 As an alternative, here’s the same workflow again, but using a pipe: percentage_forest_500_b &lt;- nlcd_sampled %&gt;% dplyr::filter(class == 42, metric == &quot;pland&quot;) %&gt;% dplyr::pull(value) percentage_forest_500_b ## [1] 77.50230 39.57759 38.01653 31.40138 40.40404 83.30450 69.16221 86.80927 ## [9] 10.19284 72.81910 92.19467 34.04635 34.84848 27.09447 31.37255 56.23886 ## [17] 59.77961 58.73440 37.64922 60.12111 46.16756 32.32323 23.35640 50.44563 ## [25] 66.93405 30.57851 33.70064 26.53811 45.18717 38.05704 62.47772 c. Extract landscape metric of choice for all cover types (as data frame) To extract the landscape metric ‘prop.landscape’ for all cover types as a tibble, just filter dplyr::filter() the tibble again, but only use the metric as filter. ## f.lter for percentage of landscape percentage_forest_500_df &lt;- dplyr::filter(nlcd_sampled, metric == &quot;pland&quot;) percentage_forest_500_df ## # A tibble: 154 × 8 ## layer level class id metric value plot_id percentage_inside ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 class 11 NA pland 7.25 1 98.0 ## 2 1 class 31 NA pland 0.0918 1 98.0 ## 3 1 class 42 NA pland 77.5 1 98.0 ## 4 1 class 52 NA pland 1.93 1 98.0 ## 5 1 class 71 NA pland 13.2 1 98.0 ## 6 1 class 11 NA pland 3.49 2 98.0 ## 7 1 class 12 NA pland 1.01 2 98.0 ## 8 1 class 31 NA pland 0.735 2 98.0 ## 9 1 class 42 NA pland 39.6 2 98.0 ## 10 1 class 52 NA pland 5.33 2 98.0 ## # ℹ 144 more rows The percent cover of all cover types should add up to ~ 100% (i.e., 1) for each site. We can check this with the function dplyr::summarize(). First, we need to group the data using the plot_id, then sum all percentages. ## g.oup by plot_id and sum all percentages pland_sum_a &lt;- dplyr::summarize(dplyr::group_by(percentage_forest_500_df, by = plot_id), sum_pland = sum(value)) pland_sum_a ## # A tibble: 31 × 2 ## by sum_pland ## &lt;int&gt; &lt;dbl&gt; ## 1 1 100 ## 2 2 100 ## 3 3 100 ## 4 4 100 ## 5 5 100 ## 6 6 100 ## 7 7 100 ## 8 8 100 ## 9 9 100 ## 10 10 100 ## # ℹ 21 more rows Same workflow, but using a pipe: pland_sum_b &lt;- percentage_forest_500_df %&gt;% dplyr::group_by(plot_id) %&gt;% dplyr::summarize(sum_pland = sum(value)) pland_sum_b ## # A tibble: 31 × 2 ## plot_id sum_pland ## &lt;int&gt; &lt;dbl&gt; ## 1 1 100 ## 2 2 100 ## 3 3 100 ## 4 4 100 ## 5 5 100 ## 6 6 100 ## 7 7 100 ## 8 8 100 ## 9 9 100 ## 10 10 100 ## # ℹ 21 more rows d. Extract all landscape metrics for a single cover type (as data frame) Filterdplyr::filter() for class == 42 and add the sites names as coloumn to the resulting tibble. ## f.lter for class == 42 (forest) forest_500_df &lt;- dplyr::filter(nlcd_sampled, class == 42) ## d.ta.frame with id and name of site SiteName_df &lt;- data.frame(id = 1:length(Sites.sf$SiteName), site_name = Sites.sf$SiteName) ## a.d site_name to metrics using plot_id and id of sampling sites forest_500_df &lt;- dplyr::left_join(forest_500_df, SiteName_df, by = c(&quot;plot_id&quot; = &quot;id&quot;)) forest_500_df ## # A tibble: 93 × 9 ## layer level class id metric value plot_id percentage_inside site_name ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 class 42 NA ai 93.3 1 98.0 AirplaneLake ## 2 1 class 42 NA np 2 1 98.0 AirplaneLake ## 3 1 class 42 NA pland 77.5 1 98.0 AirplaneLake ## 4 1 class 42 NA ai 89.4 2 98.0 BachelorMeadow ## 5 1 class 42 NA np 7 2 98.0 BachelorMeadow ## 6 1 class 42 NA pland 39.6 2 98.0 BachelorMeadow ## 7 1 class 42 NA ai 79.4 3 98.0 BarkingFoxLake ## 8 1 class 42 NA np 10 3 98.0 BarkingFoxLake ## 9 1 class 42 NA pland 38.0 3 98.0 BarkingFoxLake ## 10 1 class 42 NA ai 81.2 4 104. BirdbillLake ## # ℹ 83 more rows Done! Check out this week’s bonus material to see: More details on how to create static and interactive maps of an sf object with tmap. How to import and export ESRI shapefiles. How to plot a categorical raster map with a predefined color scheme, using tmap and terra. package). "],["r-exercise-week-2.html", "5.4 R Exercise Week 2", " 5.4 R Exercise Week 2 Task: Create a bubble plot of the number of genotyped individuals in the dataset pulsatilla_genotypes.csv, using Latitude/Longitude coordinates. Hints: Load libraries: Load libraries gstudio, dplyr, tibble and sf. Import data: Re-use your code from Week 1 exercise to import the dataset pulsatilla_genotypes.csv into gstudio. Recall that the resulting object is a data.frame. Check the variables with function str. Which variables contain the sites and the spatial coordinates? Summarize by site: Use the function group_by from library dplyr to group individuals (rows) by site (using pipe notation: %&gt;%), and add the function summarize to count the number of genotyped individuals per population (i.e., sampling site). Recall that this can be done with nesting the function n within summarize: summarize(nIndiv = n()). Write the result into a new object Pulsatilla. Add mean coordinates: You can nest multiple functions within summarize and separate them with a comma. E.g., to calculate both sample size and the mean of a variable myVar, you could write: summarize(nIndiv = n(), myMean = n(myVar)) Modify your code to calculate the number of genotyped individuals for each site and their mean X and Y coordinates. Your object ‘Pulsatilla’ should now have three columns, one with the number of individuals and two with the mean coordinates. Display the dataset with as_tibble to check. Convert to sf object: Modify code from section 2.a to convert your data frame Pulsatilla to an sf object. Make sure to adjust the variable names for the coordinates (i.e., use the variable names that you assigned in the previous step for the mean X and Y coordinates). Specify known projection: The correct EPSG number for this dataset is: 31468. You can specify the CRS with: st_crs(Pulsatilla) &lt;- 31468. Transform projection: Adapt code from section 2.c to transform the projection to the “longlat” coordinate system, and write it into an object Pulsatilla.longlat. Create bubble plot: Adapt code from section 4.d to create a bubble plot of the number of individuals per population. Note: you may drop the argument key.entries as it has a default. Save data as R object: Save the object Pulsatilla.longlat as an R object using the following code: saveRDS(Pulsatilla.longlat, file = here::here(\"output/Pulsatilla.longlat.rds\")). We will need it for a later R exercise. Question: Where on earth are the sites in the Pulsatilla dataset located? "],["bonus_2a.html", "5.5 Bonus: ‘sf’ and ‘terra’", " 5.5 Bonus: ‘sf’ and ‘terra’ Helene Wagner 1. Overview This bonus vignette uses the packages sf, terra, and tmap, yet it can only scratch the surface. For a thorough introduction to geocomputation with R, see this excellent Gitbook: https://geocompr.robinlovelace.net/index.html. a. Goals This bonus material expands the Worked Example to show: How to import and export ESRI shapefiles. Compatibility with packages sp and raster How to create static and interactive maps with tmap. How to plot a categorical raster with a predefined color scheme. Try modifying the code to import your own data! b. Required R packages library(LandGenCourse) library(sf) library(GeNetIt) library(terra) library(tmap) library(dplyr) library(tibble) library(here) 2. Import and export ESRI shapefiles ESRI shapefiles are a widely used data format for sharing geospatial vector data. With the package sf, they are easy to import and export. Here’ we will export Sites.sf to a shapefile and then import it again. 5.5.0.1 a. Export ‘sf’ object to shapefile The following code may produce a warning that column names were abbreviated. It writes the component files for the ESRI shapefile into the pre-existing folder output (the first line will create it if does not exist yet). Remember to remove the hashtags ‘#’ to un-comment the code before running it. The argument delete_dsn specifies whether any existing file with the same name should be deleted first (i.e., overwritten). data(ralu.site) #if(!dir.exists(here(&quot;output&quot;))) dir.create(here(&quot;output&quot;)) #dir.create(here(&quot;output/Sites&quot;)) #st_write(ralu.site, here(&quot;output/Sites/Sites.shp&quot;), delete_dsn = TRUE) Navigate to the Sites folder in the output folder (Files tab). You should now see four component files of the shapefile. These four files are required parts of the shapefile, always keep (or share) them together! Some shapefiles will have additional, optional component files. Sites.dbf Sites.prj Sites.shp Sites.shx 5.5.0.2 b. Import shapefile to ‘sf’ object Importing a shapefile is very easy with the function st_read from the sf package. While you only need to provide the path to the .shp file, the other files listed above must be in the same folder. #Sites.sf_a &lt;- st_read(here(&quot;output/Sites/Sites.shp&quot;)) #Sites.sf_a 3. Compatibility with sp and raster objects The packages sf and terra recently replaced the older packages sp and raster. You may still encounter code that requires objects from these packages. Here we show how to convert between sf and sp, and between terra and raster. The conversion is easy, though please note that sp objects are S4 objects. 5.5.0.3 a. Converting between sf and sp It is easy to convert an sf object into a Spatial object of package sp, using the function as_Spatial of the sf package. For point data, the resulting class will be a SpatialPointsDataFrame. data(ralu.site) Sites.sp &lt;- sf::as_Spatial(ralu.site) Sites.sp ## class : SpatialPointsDataFrame ## features : 31 ## extent : 686908.5, 690890.1, 4994089, 5004435 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## variables : 17 ## names : SiteName, Drainage, Basin, Substrate, NWI, AREA_m2, PERI_m, Depth_m, TDS, FISH, ACB, AUC, AUCV, AUCC, AUF, ... ## min values : AirplaneLake, ClearCreek, Birdbill, Cobble, Lacustrine, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ## max values : WelcomeLake, WilsonCreek, TipTop, Silt, Riverine_UpperPerennial_UnconsolidatedBottom, 353898.1, 4312.9, 24.3, 20, 1, 0.11, 0.817, 0.463, 0.817, 1, ... We can convert the Spatial object back to an sf object with the function st_as_sf of the sf package: Sites.sf_b &lt;- sf::st_as_sf(Sites.sp) Sites.sf_b ## Simple feature collection with 31 features and 17 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 686908.5 ymin: 4994089 xmax: 690890.1 ymax: 5004435 ## Projected CRS: +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## First 10 features: ## SiteName Drainage Basin Substrate ## 1 AirplaneLake ShipIslandCreek Sheepeater Silt ## 2 BachelorMeadow WilsonCreek Skyhigh Silt ## 3 BarkingFoxLake WaterfallCreek Terrace Silt ## 4 BirdbillLake ClearCreek Birdbill Sand ## 5 BobLake WilsonCreek Harbor Silt ## 6 CacheLake WilsonCreek Skyhigh Silt ## 7 DoeLake WilsonCreek Skyhigh Silt ## 8 EggWhiteLake WilsonCreek Skyhigh Silt ## 9 ElenasLake ShipIslandCreek Sheepeater Sand ## 10 FawnLake WilsonCreek Skyhigh Silt ## NWI AREA_m2 PERI_m Depth_m TDS FISH ACB AUC ## 1 Lacustrine 62582.2 1142.8 21.64 2.5 1 0 0.411 ## 2 Riverine_Intermittent_Streambed 225.0 60.0 0.40 0.0 0 0 0.000 ## 3 Lacustrine 12000.0 435.0 5.00 13.8 1 0 0.300 ## 4 Lacustrine 12358.6 572.3 3.93 6.4 1 0 0.283 ## 5 Palustrine 4600.0 321.4 2.00 14.3 0 0 0.000 ## 6 Palustrine 2268.8 192.0 1.86 10.9 0 0 0.000 ## 7 Lacustrine 13034.9 463.2 6.03 10.0 1 0 0.415 ## 8 Palustrine 4544.5 291.9 3.30 2.4 0 0 0.000 ## 9 Palustrine 0.0 0.0 0.00 0.0 0 0 0.000 ## 10 Palustrine 3865.9 237.7 1.98 3.6 0 0 0.000 ## AUCV AUCC AUF AWOOD AUFV geometry ## 1 0.000 0.411 0.063 0.063 0.464 POINT (688816.6 5003207) ## 2 0.000 0.000 1.000 0.000 0.000 POINT (688494.4 4999093) ## 3 0.000 0.300 0.700 0.000 0.000 POINT (687938.4 5000223) ## 4 0.000 0.283 0.717 0.000 0.000 POINT (689732.8 5002522) ## 5 0.000 0.000 0.500 0.000 0.500 POINT (690104 4999355) ## 6 0.000 0.000 0.556 0.093 0.352 POINT (688742.5 4997481) ## 7 0.171 0.585 0.341 0.000 0.073 POINT (688962.4 4996675) ## 8 0.047 0.047 0.686 0.209 0.058 POINT (688539.3 4998146) ## 9 0.000 0.000 0.000 0.000 0.000 POINT (688878.7 5004435) ## 10 0.000 0.000 1.000 0.000 0.000 POINT (688901.5 4996837) 5.5.0.4 b. Converting between terra and raster To convert a SpatRaster (terra) with multiple layers into RasterStack of the package raster, we use the function stack of the raster package: RasterMaps &lt;- rast(system.file(&quot;extdata/covariates.tif&quot;, package=&quot;GeNetIt&quot;)) RasterMaps.r &lt;- raster::stack(RasterMaps) RasterMaps.r ## class : RasterStack ## dimensions : 426, 358, 152508, 6 (nrow, ncol, ncell, nlayers) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## names : cti, err27, ffp, gsp, hli, nlcd ## min values : 8.429851e-01, 3.906551e-02, 0.000000e+00, 2.270000e+02, 1.014000e+03, 1.100000e+01 ## max values : 23.7147598, 0.7637643, 51.0000000, 338.0696716, 9263.0000000, 95.0000000 To convert a single layer into a RasterLayer of the package raster, we would use the function raster: nlcd.r &lt;- raster::raster(RasterMaps$nlcd) nlcd.r ## class : RasterLayer ## band : 6 (of 6 bands) ## dimensions : 426, 358, 152508 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : covariates.tif ## names : nlcd ## values : 11, 95 (min, max) To convert from a RasterLayer (package raster) to a SpatRaster (package terra), we use the function rast of the terra package: nlcd &lt;- terra::rast(nlcd.r) nlcd ## class : SpatRaster ## size : 426, 358, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : covariates.tif ## name : nlcd ## min value : 11 ## max value : 95 We can use the same function to convert from a RasterStack (package raster) to a SpatRaster with multiple layers (package terra): RasterMaps_b &lt;- terra::rast(RasterMaps.r) RasterMaps_b ## class : SpatRaster ## size : 426, 358, 6 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs ## source : covariates.tif ## names : cti, err27, ffp, gsp, hli, nlcd ## min values : 0.8429851, 0.03906551, 0, 227.0000, 1014, 11 ## max values : 23.7147598, 0.76376426, 51, 338.0697, 9263, 95 4. Plotting spatial data with tmap a. Plot geometry The package sf makes a clear distinction between the geometry information (spatial coordinates: where in space) and attribute information (what’s at these locations). Hence, when using the function plot with sf objects, we need to decide what we want to plot: geometry or attributes? To plot the geometry, we use function st_geometry to extract the geometry information from the sf object: data(ralu.site) Sites.sf_c &lt;- ralu.site plot(st_geometry(Sites.sf_c)) b. Plot attributes in space If we don’t extract the geometry, then R will assume that we want to plot attribute data. The default is to plot the first ten attributes. Here we set the point character pch to a filled circle, which is symbol #16. With cex=2, we define the symbol size. For an overview of ‘pch’ symbol numbers, and colors, check: http://vis.supstat.com/2013/04/plotting-symbols-and-color-palettes/ par(mar=c(2,2,2,2)) plot(Sites.sf_c, pch=16, cex=2) ## Warning: plotting the first 10 out of 17 attributes; use max.plot = 17 to plot ## all This is pretty cool! Let’s have a closer look at two of the variables. Basin (left): this is a factor, and each factor level is assigned a different color. Depth_m (right): this is a quantitative variable, and R automatically uses a color ramp (from blue to pink to orange) to indicate variation in the values. Note: To learn about options for the plot function for sf objects, access the help file by typing ?plot and select ‘Plot sf object’. plot(Sites.sf_c[,c(&quot;Basin&quot;, &quot;Depth_m&quot;)], pch=16) c. Create a static bubble plot with ‘tmap’ The tmap package (for plotting thematic maps) is a great tool for plotting maps. It is based on the grammar of graphics concepts, which take a bit of getting used to. Most importantly, we need the following parts: tmap_mode(\"plot\"): to plot a static map (default) tm_shape: this function defines the data to be used. tm_sf: this function defines what information should be plotted and how. Note: for the second part, there are many other functions for various types of data. If we use tm_sf without arguments (i.e., with the default settings), we get a plot of the geometry: tmap_mode(&quot;plot&quot;) tm_shape(Sites.sf_c) + tm_sf() We can indicate an attribute to plot it. Also, there is a special function tm_bubbles for bubble plots. Here we define the bubble size by wetland depth and bubble color by basin. In addition, we specify that the legend should be placed outside of the plot, on the right. tmap_mode(&quot;plot&quot;) tm_shape(Sites.sf_c) + tm_bubbles(size=&quot;Depth_m&quot;, col=&quot;Basin&quot;) + tm_layout(legend.outside=TRUE, legend.outside.position=&quot;right&quot;) Let’s make the boundary box (map extent) a little larger so that the symbols are not cut off. First we extract the boundary box of Sites.sf_c and save it as Bbox. Bbox = st_bbox(Sites.sf_c) Bbox ## xmin ymin xmax ymax ## 686908.5 4994089.3 690890.1 5004435.0 Then we define the range along x and y coordinates (delta.x, delta.y), set a zoom factor (Zoom) and add that fraction of the range on each side. Unfortunately, there is no dedicated function for this so we do this manually: delta.x &lt;- Bbox[3] - Bbox[1] delta.y &lt;- Bbox[4] - Bbox[2] Zoom &lt;- 0.1 Bbox2 &lt;- Bbox + c(-delta.x, -delta.y, delta.x, delta.y) * Zoom Bbox2 ## xmin ymin xmax ymax ## 686510.4 4993054.7 691288.2 5005469.6 Now we add the boundary box information as an argument bbox in function tm_shape. Note that we write the figure into an object, Map1, then plot the Map1. This will help e.g. with exporting the map, or we can later add more layers to this map object with +. tmap_mode(&quot;plot&quot;) Map1 &lt;- tm_shape(Sites.sf_c, bbox=Bbox2) + tm_bubbles(size=&quot;Depth_m&quot;, col=&quot;Basin&quot;) + tm_layout(legend.outside=TRUE, legend.outside.position=&quot;right&quot;) Map1 d. Create an interactive bubble plot with ‘tmap’ Creating an interactive map with a basemap from the internet is not difficult. Un-comment the code below by removing the hashtag (#) and run it. Go ahead and play with the interactive map! #tmap_mode(&quot;view&quot;) #tm_shape(Sites.sf_c) + tm_bubbles(size=&quot;Depth_m&quot;, col=&quot;Basin&quot;) By default, R will include an interactive menu to toggle between “Esri.WorldGrayCanvas”, “OpenStreetMap”, and “Esri.WorldTopoMap”. We can add more base maps to this selection by providing a list of servers with the function tm_basemap. The first one listed will be shown by default. #tm_shape(Sites.sf_c) + tm_bubbles(size=&quot;Depth_m&quot;, col=&quot;Basin&quot;) + #tm_basemap(server = c(&quot;Esri.WorldTopoMap&quot;, &quot;Esri.WorldGrayCanvas&quot;, # &quot;OpenStreetMap&quot;, &quot;OpenTopoMap&quot;, &quot;Esri.WorldImagery&quot;)) Let’s make a few more changes: We change the symbolization to fixed-size, filled circles with a black border, and plot them on top of filled circles that indicate the basin. Note that although both attributes are in the same dataset Sites.sf_c, we need to include a data statement for each attribute we add to the map (i.e., map layer), using tm_shape(Sites.sf). We include the argument bbox=Bbox2 for at least one of the map layers. This is not needed for the interactive map but will look better when exporting it as a static map. Un-comment the code below by removing the hashtag (#) and run it. Go ahead and toggle between the base maps in the map below! Different base maps are suitable for different situations (data type, symbol type, size of study area) and purposes. #tmap_mode(&quot;view&quot;) #Map2 &lt;- tm_shape(Sites.sf_c, bbox=Bbox2) + tm_sf(&quot;Basin&quot;, size=2, border.col=&quot;black&quot;) + # tm_shape(Sites.sf_c) + tm_sf(size=0.8, col=&quot;Depth_m&quot;, # palette = &quot;Blues&quot;, border.col=&quot;black&quot;) + # tm_basemap(server = c(&quot;Esri.WorldTopoMap&quot;, &quot;Esri.WorldGrayCanvas&quot;, # &quot;OpenStreetMap&quot;, &quot;OpenTopoMap&quot;, &quot;Esri.WorldImagery&quot;)) #Map2 e. Export maps Save the map to folder output: Remember to un-comment the code by removing the hashtags (#). Interactive map saved to html file. This includes all the features of the interactive map Map2. Static map saved to png file, with height = 7 inches. This drops the base map and retains only the symbolized data. Go ahead and check out the two files! #if(!dir.exists(here(&quot;output&quot;))) dir.create(here(&quot;output&quot;)) #tmap_save(Map1, here::here(&quot;output/StaticMap.png&quot;), height=7) #tmap_save(Map2, here::here(&quot;output/InteractiveMap.html&quot;)) Navigate to the folder output (Files tab) and check out the saved maps! For the dynamic map (Map2), select “open in a web browser”. You can share this file with others, who can open it in their browser and interact with the map without access to R or your data! 5. Plot a categorical map with predefined color scheme a. Define the raster attribute table Now to a more tricky topic. Recall that the last raster layer in the Worked Example, nlcd, contains categorical land cover data that are coded numerically. The terra package actually misinterpreted them as numeric data. Let’s extract the categorical raster layer into a new object ‘NLCD’. We can use as.factor to tell R that this is a categorical raster layer. Here, we save the (numerical) raster layer nlcd as categorical raster (factor) NLCD. RasterMaps &lt;- rast(system.file(&quot;extdata/covariates.tif&quot;, package=&quot;GeNetIt&quot;)) NLCD &lt;- terra::as.factor(RasterMaps$nlcd) What values occur in the raster? These are codes for cover types. Here is a description of the cover types: https://www.mrlc.gov/data/legends/national-land-cover-database-2019-nlcd2019-legend levels(NLCD)[[1]] ## ID nlcd ## 1 11 11 ## 2 12 12 ## 3 31 31 ## 4 42 42 ## 5 52 52 ## 6 71 71 ## 7 90 90 ## 8 95 95 We will import a table with predefined colors (using hex color code) from the file Colortable_LULC.csv that is included with LandGenCourse. This list has more entries (e.g., 21-24) than we need, because not all US land cover classes occur in the study area. Check in the table below that the colors and cover types are stored as ‘character’. (Luckily, since R 4.0, this is the new default for function read.csv). If they were coded as factors that could lead to errors later on. ColTab &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;Colortable_LULC.csv&quot;, package = &quot;LandGenCourse&quot;), header=TRUE) ColTab ## value color attribute ## 1 11 #456DA8 Open Water ## 2 12 #E6EEF9 Perennial Ice/Snow ## 3 21 #E1CBCD Developed, Open Space ## 4 22 #DC9786 Developed, Low Intensity ## 5 23 #F40100 Developed, Medium Intensity ## 6 24 #B00206 Developed, High Intensity ## 7 31 #B2AEA3 Barren Land (Rock/Sand/Clay) ## 8 41 #6BA95C Deciduous Forest ## 9 42 #16692E Evergreen Forest ## 10 43 #B9CA8F Mixed Forest ## 11 51 #AD9439 Dwarf Scrub ## 12 52 #D5B883 Shrub/Scrub ## 13 71 #EDEFCA Grassland/Herbaceous ## 14 72 #D3D27C Sedge/Herbaceous ## 15 73 #A5CD53 Lichens ## 16 74 #88B8A1 Moss ## 17 81 #DED73E Pasture/Hay ## 18 82 #AD722C Cultivated Crops ## 19 90 #BED8F6 Woody Wetlands ## 20 95 #6EA5C4 Emergent Herbaceous Wetlands We join this list with the list of factor levels of NLCD to create a raster attribute table (RAT). An RAT is a table that contains attributes for each distinct value in a raster. Use function left_join to extract the corresponding row from ColTab for each row in Levels(NLCD) Indicate which columns should be used to match the rows: by=c(\"ID\"=\"value\"). Note that ID is of type numeric whereas nlcd is character. As value is numeric, we match it with ID. Copy the values from variable attribute to variable nlcd Select the variables that we will need. RAT &lt;- dplyr::left_join(levels(NLCD)[[1]], ColTab, by=c(&quot;ID&quot;=&quot;value&quot;)) %&gt;% mutate(nlcd=attribute) %&gt;% dplyr::select(ID, nlcd, color) We replace the list of levels by the RAT: levels(NLCD) &lt;- RAT NLCD ## class : SpatRaster ## size : 426, 358, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 683282.5, 694022.5, 4992833, 5005613 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 11N (EPSG:26911) ## source : covariates.tif ## categories : nlcd, color ## name : nlcd ## min value : Open Water ## max value : Emergent Herbaceous Wetlands 5.5.0.5 b. Plot the map with terra::plot Now we can plot the map with the predefined color palette, using the plot function for SpatRaster objects: plot(NLCD, col=RAT$color) points(Sites.sf_c, pch=21, col=&quot;black&quot;, bg=&quot;white&quot;, cex=1) 5.5.0.6 c. Plot the map with tmap We first define the dataset with tm_shape With tm_raster, we tell tmap that we want to plot raster values style=\"cat\" tells tmap to interpret values as categories palette defines the colors labels defines the labels to be used for the categories title defines the legend caption for the categories With tm_layout, we tell tmap where to place the legend (outside to the right of map) With tm_grid(lines=FALSE) we tell tmap to show the coordinates along x and y axes but to suppress grid lines that would be drawn on top of the map (change it to TRUE to see the effect) Map3 &lt;- tm_shape(NLCD) + tm_raster(style=&quot;cat&quot;, palette=RAT$color, labels=RAT$nlcd, title=&quot;Land cover&quot;) + tm_layout(legend.outside=TRUE, legend.outside.position=&quot;right&quot;) ## ## ── tmap v3 code detected ─────────────────────────────────────────────────────── ## [v3-&gt;v4] `tm_raster()`: instead of `style = &quot;cat&quot;`, use col.scale = ## `tm_scale_categorical()`. ## ℹ Migrate the argument(s) &#39;palette&#39; (rename to &#39;values&#39;), &#39;labels&#39; to ## &#39;tm_scale_categorical(&lt;HERE&gt;)&#39; ## [v3-&gt;v4] `tm_raster()`: migrate the argument(s) related to the legend of the ## visual variable `col` namely &#39;title&#39; to &#39;col.legend = tm_legend(&lt;HERE&gt;)&#39; Map3 Let’s beef it up a bit. We can do so by adding layers to Map3: To add an additional layer with the sampling sites to the map, we define the data for the layer with tm_shape. We define how the points should be symbolized with tm_symbols. We add a North arrow with tm_compass , using default settings. We add a scale bar with tm_scale_bar and set the background to a semitransparent (bg.alpha=0.5) light gray (bg.color=\"lightgray\"). Map4 &lt;- Map3 + tm_shape(Sites.sf_c) + tm_symbols(size=0.4, col=&quot;yellow&quot;, border.col=&quot;red&quot;) + tm_compass() + tm_scale_bar(bg.color=&quot;lightgray&quot;, bg.alpha=0.5) ## ## ── tmap v3 code detected ─────────────────────────────────────────────────────── ## [v3-&gt;v4] `symbols()`: use &#39;fill&#39; for the fill color of polygons/symbols ## (instead of &#39;col&#39;), and &#39;col&#39; for the outlines (instead of &#39;border.col&#39;). ## ! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead. ## This message is displayed once every 8 hours. Map4 This time, we’ll export the map to a PDF file. #if(!dir.exists(here(&quot;output&quot;))) dir.create(here(&quot;output&quot;)) #tmap_save(Map4, here::here(&quot;output/RasterMap.pdf&quot;), height=6, width=8) "],["Week3.html", "6 Lab 3: Genetic Diversity", " 6 Lab 3: Genetic Diversity In this week’s computer lab, we will learn about how to perform basic population genetic analyses and quantify genetic diversity. Along the way, we will review hypothesis testing and learn about data manipulation in R. View Course Video Interactive Tutorial 3 Worked Example R Exercise Week 3 Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_3.html", "6.1 View Course Video", " 6.1 View Course Video 1. Embedded Video External link: Week 3 video (Part 1); Week 3 video (Part 2) Transcript: Download transcript Video, Part 1 iframe not supported Video, Part 2 iframe not supported Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_3.html", "6.2 Interactive Tutorial 3", " 6.2 Interactive Tutorial 3 1. List of R commands covered this week Function Package table base range base split base length base nrow, ncol base sapply base matrix base dimnames base Reduce base rbind, cbind base list base t base [ , ] base is.na base != base seppop adegenet propTyped adegenet apply base mean base 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_3.html", "6.3 Worked Example", " 6.3 Worked Example Helene Wagner 1. Overview of Worked Example a. Goals This worked example shows how to: Check markers and populations (polymorphism, HWE, linkage, null alleles). Assess genetic diversity. Aggregate genetic data at the population level. b. Data set This is the same data set as used in Weeks 1 &amp; 2. Microsatellite data for 181 individuals of Colombia spotted frogs (Rana luteiventris) from 12 populations. Site-level spatial coordinates and attributes. The data are a subsample of the full data set analyzed in Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set. ralu.loci: Data frame with populations and genetic data (181 rows x 9 columns). Included in package ‘LandGenCourse’. To load it, type: data(ralu.loci) ralu.site: Spatial points data frame with spatial coordinates and site variables Included in package GeNetIt’. To load it, type: data(ralu.site) c. Required R packages All required packages should have been installed already when you installed ‘LandGenCourse’. #require(adegenet) require(LandGenCourse) #require(pegas) #require(PopGenReport) require(dplyr) require(poppr) 2. Basic checking of markers and populations Before we do landscape genetic analysis, we need to perform a basic population genetic analysis of the genetic data, in order to better understand the nature and quality of the data and to check for underlying assumptions of population genetic models and corresponding methods. a. Re-create genind object Adapted from Week 1 tutorial: Note: we use the double colon notation ‘package::function(argument)’ to indicate, for each function, which package it belongs to (see Week 2 video). data(ralu.loci, package=&quot;LandGenCourse&quot;) Frogs &lt;- data.frame(FrogID = paste(substr(ralu.loci$Pop, 1, 3), row.names(ralu.loci), sep=&quot;.&quot;), ralu.loci) Frogs.genind &lt;- adegenet::df2genind(X=Frogs[,c(4:11)], sep=&quot;:&quot;, ncode=NULL, ind.names= Frogs$FrogID, loc.names=NULL, pop=Frogs$Pop, NA.char=&quot;NA&quot;, ploidy=2, type=&quot;codom&quot;, strata=NULL, hierarchy=NULL) Frogs.genind ## /// GENIND OBJECT ///////// ## ## // 181 individuals; 8 loci; 39 alleles; size: 55.5 Kb ## ## // Basic content ## @tab: 181 x 39 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 39 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::df2genind(X = Frogs[, c(4:11)], sep = &quot;:&quot;, ncode = NULL, ## ind.names = Frogs$FrogID, loc.names = NULL, pop = Frogs$Pop, ## NA.char = &quot;NA&quot;, ploidy = 2, type = &quot;codom&quot;, strata = NULL, ## hierarchy = NULL) ## ## // Optional content ## @pop: population of each individual (group size range: 7-23) b. Check that markers are polymorphic The genetic resolution depends on the number of markers and their polymorphism. The table above and the summary function for genind objects together provide this information. Now we run the summary function: summary(Frogs.genind) ## ## // Number of individuals: 181 ## // Group sizes: 21 8 14 13 7 17 9 20 19 13 17 23 ## // Number of alleles per locus: 3 4 4 4 9 3 4 8 ## // Number of alleles per group: 21 21 20 22 20 19 19 25 18 14 18 26 ## // Percentage of missing data: 10.64 % ## // Observed heterozygosity: 0.1 0.4 0.09 0.36 0.68 0.02 0.38 0.68 ## // Expected heterozygosity: 0.17 0.47 0.14 0.59 0.78 0.02 0.48 0.74 The output of the summary function shows us the following: 8 loci with 3 - 9 alleles (39 in total) Expected heterozygosity varies between 0.14 (locus C) and 0.78 (locus E) There’s a reasonable level of missing values (10.6%) c. Check for deviations from Hardy-Weinberg equilibrium (HWE) See also: http://dyerlab.github.io/applied_population_genetics/hardy-weinberg-equilibrium.html For a very large population (no drift) with random mating and non-overlapping generations (plus a few more assumptions about the mating system), and in the absence of mutation, migration (gene flow) and selection, we can predict offspring genotype frequencies from allele frequencies of the parent generation (Hardy-Weinberg equilibrium). In general, we don’t expect all of these assumptions to be met (e.g., if we want to study gene flow or selection, we kind of expect that these processes are present). Note: plants often show higher levels of departure from HWE than animals. Here are p-values for two alternative tests of deviation from HWE for each locus. Columns: chi^2: value of the classical chi-squared test statistic df: degrees of freedom of the chi-squared test Pr(chi^2 &gt;): p-value of the chi-squared test (‘&gt;’ indicates that the alternative is ‘greater’, which is always the case for a chi-squared test) Pr.exact: p-value from an exact test based on Monte Carlo permutation of alleles (for diploids only). The default is B = 1000 permutations (set B = 0 to skip this test). Here we use the function ‘round’ with argument ‘digits = 3’ to round all values to 3 decimals. round(pegas::hw.test(Frogs.genind, B = 1000), digits = 3) ## chi^2 df Pr(chi^2 &gt;) Pr.exact ## A 40.462 3 0.000 0.000 ## B 17.135 6 0.009 0.034 ## C 136.522 6 0.000 0.000 ## D 83.338 6 0.000 0.000 ## E 226.803 36 0.000 0.000 ## F 0.024 3 0.999 1.000 ## G 12.349 6 0.055 0.008 ## H 76.813 28 0.000 0.000 Both tests suggest that all loci except for locus “F” are out of HWE globally (across all 181 individuals). Next, we check for HWE of each locus in each population. Notes on the code: The curly brackets ‘{ }’ below are used to keep the output from multiple lines together in the html file. Function ‘seppop’ splits the genind object by population. We use ‘sapply’ to apply the function ‘hw.test’ from package ‘pegas’ to each population (see this week’s video and tutorial). We set ‘B=0’ to specify that we don’t need any permutations right now. The function ‘t’ takes the transpose of the resulting matrix, which means it flips rows and columns. This works on a matrix, not a data frame, hence we use ‘data.matrix’ to temporarily interpret the data frame as a matrix. # Chi-squared test: p-value HWE.test &lt;- data.frame(sapply(seppop(Frogs.genind), function(ls) pegas::hw.test(ls, B=0)[,3])) HWE.test.chisq &lt;- t(data.matrix(HWE.test)) {cat(&quot;Chi-squared test (p-values):&quot;, &quot;\\n&quot;) round(HWE.test.chisq,3)} ## Chi-squared test (p-values): ## A B C D E F G H ## Airplane 0.092 0.359 1.000 0.427 0.680 1.000 0.178 0.051 ## Bachelor 1.000 0.557 0.576 0.686 0.716 1.000 0.414 0.609 ## BarkingFox 0.890 0.136 0.005 0.533 0.739 0.890 0.708 0.157 ## Bob 0.764 0.864 0.362 0.764 0.033 1.000 0.860 0.287 ## Cache 1.000 0.325 0.046 0.659 0.753 1.000 0.709 0.402 ## Egg 1.000 0.812 1.000 1.000 0.156 1.000 0.477 0.470 ## Frog 1.000 0.719 0.070 0.722 0.587 1.000 0.564 0.172 ## GentianL 0.809 0.059 1.000 0.028 0.560 0.717 0.474 0.108 ## ParagonL 1.000 0.054 0.885 0.709 0.868 1.000 0.291 0.000 ## Pothole 1.000 1.000 1.000 0.488 0.248 1.000 0.296 0.850 ## ShipIsland 0.807 0.497 1.000 0.521 0.006 1.000 0.498 0.403 ## Skyhigh 0.915 0.493 0.063 0.001 0.155 1.000 0.126 0.078 Let’s repeat this with a Monte Carlo permutation test with B = 1000 replicates: # Monte Carlo: p-value HWE.test &lt;- data.frame(sapply(seppop(Frogs.genind), function(ls) pegas::hw.test(ls, B=1000)[,4])) HWE.test.MC &lt;- t(data.matrix(HWE.test)) {cat(&quot;MC permuation test (p-values):&quot;, &quot;\\n&quot;) round(HWE.test.MC,3)} ## MC permuation test (p-values): ## A B C D E F G H ## Airplane 0.017 1.000 1.000 0.375 0.627 1 0.247 0.005 ## Bachelor 1.000 0.467 1.000 1.000 0.853 1 0.478 0.600 ## BarkingFox 1.000 0.224 0.072 1.000 0.765 1 1.000 0.164 ## Bob 1.000 1.000 1.000 1.000 0.022 1 1.000 0.249 ## Cache 1.000 0.382 0.128 1.000 1.000 1 1.000 0.580 ## Egg 1.000 1.000 1.000 1.000 0.077 1 0.522 0.519 ## Frog 1.000 1.000 0.083 1.000 0.439 1 1.000 0.174 ## GentianL 1.000 0.063 1.000 0.058 0.697 1 0.620 0.130 ## ParagonL 1.000 0.145 1.000 1.000 1.000 1 0.321 0.084 ## Pothole 1.000 1.000 1.000 1.000 0.544 1 0.518 1.000 ## ShipIsland 1.000 0.618 1.000 0.706 0.128 1 0.555 0.462 ## Skyhigh 1.000 0.355 0.183 0.096 0.110 1 0.074 0.029 To summarize, let’s calculate, for each locus, the proportion of populations where it was out of HWE. Here we’ll use the conservative cut-off of alpha = 0.05 for each test. There are various ways of modifying this, including a simple Bonferroni correction, where we divide alpha by the number of tests, which you can activate here by removing the ### i. front of the line. We write the results into a data frame ‘Prop.loci.out.of.HWE’ and use ‘=’ to specify the name for each column. alpha=0.05 # /96 Prop.loci.out.of.HWE &lt;- data.frame(Chisq=apply(HWE.test.chisq&lt;alpha, 2, mean), MC=apply(HWE.test.MC&lt;alpha, 2, mean)) Prop.loci.out.of.HWE # Type this line again to see results table ## Chisq MC ## A 0.00000000 0.08333333 ## B 0.00000000 0.00000000 ## C 0.16666667 0.00000000 ## D 0.16666667 0.00000000 ## E 0.16666667 0.08333333 ## F 0.00000000 0.00000000 ## G 0.00000000 0.00000000 ## H 0.08333333 0.16666667 And similarly, for each population, the proportion of loci that were out of HWE: Prop.pops.out.of.HWE &lt;- data.frame(Chisq=apply(HWE.test.chisq&lt;alpha, 1, mean), MC=apply(HWE.test.MC&lt;alpha, 1, mean)) Prop.pops.out.of.HWE ## Chisq MC ## Airplane 0.000 0.250 ## Bachelor 0.000 0.000 ## BarkingFox 0.125 0.000 ## Bob 0.125 0.125 ## Cache 0.125 0.000 ## Egg 0.000 0.000 ## Frog 0.000 0.000 ## GentianL 0.125 0.000 ## ParagonL 0.125 0.000 ## Pothole 0.000 0.000 ## ShipIsland 0.125 0.000 ## Skyhigh 0.125 0.125 The results suggest that: While most loci are out of HWE globally, this is largely explained by subdivision (variation in allele frequencies among local populations indicating limited gene flow). No locus is consistently out of HWE across populations (loci probably not affected by selection). No population is consistently out of HWE across loci (probably no recent major bottlenecks/ founder effects). Let’s repeat this with ‘false discovery rate’ correction for the number of tests. Here we use the function ‘p.adjust’ with the argument ‘method=“fdr”’ to adjust the p-values from the previous tests. This returns a vector of length 96 (the number of p-values used), which we convert back into a matrix of 12 rows (pops) by 8 columns (loci). Then we procede as above. Chisq.fdr &lt;- matrix(p.adjust(HWE.test.chisq,method=&quot;fdr&quot;), nrow=nrow(HWE.test.chisq)) MC.fdr &lt;- matrix(p.adjust(HWE.test.MC, method=&quot;fdr&quot;), nrow=nrow(HWE.test.MC)) Prop.pops.out.of.HWE &lt;- data.frame(Chisq=apply(HWE.test.chisq&lt;alpha, 1, mean), MC=apply(HWE.test.MC&lt;alpha, 1, mean), Chisq.fdr=apply(Chisq.fdr&lt;alpha, 1, mean), MC.fdr=apply(MC.fdr&lt;alpha, 1, mean)) Prop.pops.out.of.HWE ## Chisq MC Chisq.fdr MC.fdr ## Airplane 0.000 0.250 0.000 0 ## Bachelor 0.000 0.000 0.000 0 ## BarkingFox 0.125 0.000 0.000 0 ## Bob 0.125 0.125 0.000 0 ## Cache 0.125 0.000 0.000 0 ## Egg 0.000 0.000 0.000 0 ## Frog 0.000 0.000 0.000 0 ## GentianL 0.125 0.000 0.000 0 ## ParagonL 0.125 0.000 0.125 0 ## Pothole 0.000 0.000 0.000 0 ## ShipIsland 0.125 0.000 0.000 0 ## Skyhigh 0.125 0.125 0.125 0 After using false discovery rate correction for the 8 * 12 = 96 tests performed, very few combinations of locus and population were out of HWE based on the chi-squared test, and none with the MC test. Note: exact results are likely to differ somewhat between runs due to the permutation tests. d. Check for linkage disequilibrium (LD) See also: https://grunwaldlab.github.io/Population_Genetics_in_R/Linkage_disequilibrium.html For microsatellite markers, we typically don’t know where on the genome they are located. The closer together two markers are on a chromosome, the more likely they are inherited together, which means that they don’t really provide independent information. Testing for linkage disequilibrium assesses this, for each pair of loci, by checking whether alleles of two loci are statistically associated. This step is especially important when developing a new set of markers. You may want to drop (the less informative) one marker of any pair of linked loci. Here, we start with performing an overall test of linkage disequilibrium (the null hypothesis is that there is no linkage among the set of markers). Two indices are calculated and tested: an index of association (Ia; Brown et al. 1980) and a measure of correlation (rbarD; Agapow and Burt 2001), which is less biased (see URL above). The number of permutations is specified by ‘sample = 199’. Overall, there is statistically significant association among the markers (p-value: prD = 0.005; also left figure). Recall that the power of a statistical test increases with sample size, and here we have n = 181, hence even a small effect may be statistically significant. Hence we look at effect size, i.e., the actual strength of the pairwise associations (right figure). poppr::ia(Frogs.genind, sample=199) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## ℹ The deprecated feature was likely used in the poppr package. ## Please report the issue at &lt;https://github.com/grunwaldlab/poppr/issues/&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information. ## ℹ The deprecated feature was likely used in the poppr package. ## Please report the issue at &lt;https://github.com/grunwaldlab/poppr/issues/&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Ia p.Ia rbarD p.rD ## 0.33744318 0.00500000 0.05366542 0.00500000 LD.pair &lt;- poppr::pair.ia(Frogs.genind) ## Warning: myTheme is not a valid theme. ## Please use `theme()` to construct themes. LD.pair ## Ia rbarD ## A:B 0.0485 0.0492 ## A:C -0.0314 -0.0335 ## A:D 0.1886 0.1966 ## A:E 0.0560 0.0569 ## A:F -0.0272 -0.0452 ## A:G 0.0931 0.0935 ## A:H 0.0294 0.0304 ## B:C -0.0329 -0.0375 ## B:D 0.0903 0.0911 ## B:E 0.0910 0.0910 ## B:F -0.0013 -0.0025 ## B:G 0.0451 0.0452 ## B:H 0.0621 0.0623 ## C:D -0.0859 -0.1049 ## C:E 0.0247 0.0284 ## C:F -0.0311 -0.0397 ## C:G -0.0107 -0.0118 ## C:H 0.0012 0.0015 ## D:E 0.0455 0.0458 ## D:F 0.0094 0.0199 ## D:G 0.0069 0.0070 ## D:H 0.0461 0.0462 ## E:F 0.0013 0.0025 ## E:G 0.0453 0.0454 ## E:H 0.2153 0.2159 ## F:G 0.0167 0.0299 ## F:H 0.0296 0.0606 ## G:H 0.0942 0.0953 The strongest correlation is around 0.2, for markers E and H. Effect size: If rbarD can be interpreted similarly to a linear correlation coefficient r, that would mean that less than 5% of the variation in one marker is shared with the other marker (recall from stats: the amount of variance explained in regression, Rsquared, is the square of the linear correlation coefficient). This is probably not large enough to worry about. e. Check for null alleles See also Dakin and Avise (2004): http://www.nature.com/articles/6800545 One potential drawback for microsatellites as molecular markers is the presence of null alleles that fail to amplify, thus they couldn’t be detected in the PCR assays. The function ‘null.all’ takes a genind object and returns a list with two components (‘homozygotes’ and ‘null.allele.freq’), and each of these is again a list. See ‘?null.all’ for details and choice of method. List ‘homozygotes’: homozygotes$observed: observed number of homozygotes for each allele at each locus homozygotes$bootstrap: distribution of the expected number of homozygotes homozygotes$probability.obs: probability of observing the number of homozygotes Note: we are turning off warnings here (currently the code throws a warning for each sample, though results don’t seem to be affected). # Null alleles: depends on method! See help file. Null.alleles &lt;- PopGenReport::null.all(Frogs.genind) ## Registered S3 method overwritten by &#39;genetics&#39;: ## method from ## [.haplotype pegas Null.alleles$homozygotes$probability.obs ## Allele-1 Allele-2 Allele-3 Allele-4 Allele-5 Allele-6 Allele-7 Allele-8 ## A 0.112 0.000 0.038 NA NA NA NA NA ## B 0.149 0.045 0.054 0.001 NA NA NA NA ## C 0.162 0.002 0.001 0.004 NA NA NA NA ## D 0.000 0.001 0.229 0.006 NA NA NA NA ## E 0.061 0.009 0.034 0.302 0.071 0.041 0.732 0.524 ## F 0.459 0.001 0.012 NA NA NA NA NA ## G 0.077 0.071 0.011 0.000 NA NA NA NA ## H 0.439 0.083 0.011 0.024 0.286 0.319 0.006 0.004 ## Allele-9 ## A NA ## B NA ## C NA ## D NA ## E 0 ## F NA ## G NA ## H NA List ‘null.allele.freq’: null.allele.freq$summary1: null allele frequency estimates based upon the forumulas of Chakraborty et al. (1994) null.allele.freq$summary2: null allele frequency estimates based upon the forumulas of Brookfield (1996) From the help file: “Brookfield (1996) provides a brief discussion on which estimator should be used. In summary, it was recommended that Chakraborty et al. (1994)’s method (e.g. summary1) be used if there are individuals with no bands at a locus seen, but they are discounted as possible artefacts. If all individuals have one or more bands at a locus then Brookfield (1996)’s method (e.g. summary2) should be used.” In this case, we have many individuals with missing values for both alleles, hence better use summary1. Each summary table contains a summary with observed, median, 2.5th percentile and 97.5the percentile. The percentiles form a 95% confidence interval. From the help file: “If the 95% confidence interval includes zero, it indicates that the frequency of null alleles at a locus does not significantly differ from zero.” {cat(&quot; summary1 (Chakraborty et al. 1994):&quot;, &quot;\\n&quot;) round(Null.alleles$null.allele.freq$summary1,2)} ## summary1 (Chakraborty et al. 1994): ## A B C D E F G H ## Observed frequency 0.24 0.08 0.23 0.25 0.06 0.00 0.11 0.04 ## Median frequency 0.23 0.08 0.23 0.24 0.06 0.00 0.11 0.04 ## 2.5th percentile 0.07 0.01 0.03 0.17 0.02 -0.01 0.02 0.00 ## 97.5th percentile 0.42 0.15 0.47 0.33 0.11 0.00 0.21 0.09 {cat(&quot;summary2 (Brookfield et al. 1996):&quot;, &quot;\\n&quot;) round(Null.alleles$null.allele.freq$summary2,2)} ## summary2 (Brookfield et al. 1996): ## A B C D E F G H ## Observed frequency 0.06 0.05 0.05 0.17 0.05 0 0.07 0.04 ## Median frequency 0.06 0.05 0.05 0.17 0.05 0 0.07 0.03 ## 2.5th percentile 0.01 0.01 0.01 0.12 0.02 0 0.01 0.00 ## 97.5th percentile 0.11 0.10 0.10 0.23 0.09 0 0.13 0.08 For this example, both methods suggest that there may be null alleles in most loci. However, the estimates of the frequency of null alleles differ a lot between the two methods. A different approach for estimating null alleles at microsatellite loci, based on the Estimation-Maximization algorithm, is implemented in FreeNA (outside of the R environment). FreeNA will directly provide Fst values and some other measurements using the corrected allele frequencies: https://www1.montpellier.inra.fr/CBGP/software/FreeNA/ Relevant papers for the Estimation-Maximization algorithm: Kalinowski et al. (2006), Conservation Genetics 7:991–995, doi: 10.1007/s10592-006-9134-9. Chapuis and Estoup (2007), Mol. Biol. Evol. 24:621–631, doi: 10.1093/molbev/msl191. f. Overall interpretation Spatial genetic structure: The Columbia spotted frog data used in this lab come from a study area with a great deal of genetic structure. If we use a population assignment test (see Week 9), each basin is a separate unit with significant substructure within basins. Testing for significant genetic distance, most pairs of ponds have a genetic distance that is significantly different from zero. HWE: Therefore, we expect global estimates (i.e., the whole dataset) of He to be out of HWE due to population substructure (HWE assumes panmictic populations). We would also expect data to be out of HWE when analyzing data by basin due to population substructure. We could, then, test HWE and linkage disequilibrium at the pond level (as shown here). However, some ponds have low sample sizes (which refleect a low number of individuals: based on repeated surveys of sites, most if not nearly all animals were captured). Linkage: These low samples sizes can result in deviations from HWE and in linkage disequilibrium as an artifact of sample size and/or breeding structure (if one male is responsible for all breeding, his genes would appear “linked” in offspring genotypes, even though they are not physically linked in the genome). In addition, each pond may not represent a “population”, but only a portion of a population. So, what do we do? We can look for patterns. Are there loci that are consistently out of HWE across samples sites while other loci are not out of HWE suggesting that there are null alleles or other data quality control issues? With the full data set, this was not the case. Are there loci that are consistently linked across different ponds (while others are not), suggesting that they are linked? With the full dataset, this was not the case. Null alleles: Missing data can imply null (non-amplifying) alleles. In this case, while there are loci that have higher “drop-out” rates than others, this is more likely due to some loci being more difficult to call (the authors were very strict in removing any questionable data; equivocal calls resulting in no data). In addition, some of the samples used in this study were toe clips which with low yields of DNA, resulting in incomplete genotypes. While presence of null alleles is a possibility, genetic structure, breeding patterns at low population size and aggressive quality control of genotypes can all explain the results. Finally, with all of the structure in these data, there are examples (at the basin level and pond level) of unique alleles that are fixed or nearly fixed. When the data are assessed globally, this will result in a similar pattern to the presence of null alleles and will result in positive null allele tests. 3. Assess genetic diversity These measures are typically quantified per population. a. Rarefied allelic richness Both nominal sample size (number of frogs sampled) and valid sample size (e.g., for each locus, the number of frogs with non-missing genetic data) vary between sites. We would expect the number of alleles found in a population to increase with the number of individuals genotyped. We can check this by plotting the number of alleles against sample size. Here we create an object ‘Sum’ that contains the summary of the genind object, then we can access its elements by ‘$’ to plot what we need. The function ‘names’ lists the names of the elements, which reduced the guesswork. Sum &lt;- adegenet::summary(Frogs.genind) names(Sum) ## [1] &quot;n&quot; &quot;n.by.pop&quot; &quot;loc.n.all&quot; &quot;pop.n.all&quot; &quot;NA.perc&quot; &quot;Hobs&quot; ## [7] &quot;Hexp&quot; The site names are quite long, hence we print the labels vertically by setting ‘las=3’, and we modify the margins (‘mar’). The four numbers give the size of each margin in the following order: bottom, left, top, right. We add a regression line to the scatterplot with the function ‘abline’, where we specify the linear regression model with the function ‘lm’. In this case, we model the response ‘pop.n.all’ as a function of predictor ‘n.by.pop’. The barchart (left) shows that there is considerable variation among ponds in the number of alleles observed across all loci. The scatterplot (right) with the red regression line shows that the number of alleles increases with sample size. par(mar=c(5.5, 4.5,1,1)) barplot(Sum$pop.n.all, las=3, xlab = &quot;&quot;, ylab = &quot;Number of alleles&quot;) plot(Sum$n.by.pop, Sum$pop.n.all, xlab = &quot;Sample size&quot;, ylab = &quot;Number of alleles&quot;) abline(lm(Sum$pop.n.all ~ Sum$n.by.pop), col = &quot;red&quot;) Hence we should not compare the number of alleles directly. Instead, we’ll use rarefied allelic richness (Ar). By default, the function ‘allel.rich’ finds the lowest valid sample size across all populations and loci, and multiplies it by the ploidy level. The number is stored as ‘Richness$alleles.sampled’ (here: 3 individuals * 2 alleles = 6 alleles). Alternatively, this number can be set with the ‘min.alleles’ argument. Richness &lt;- PopGenReport::allel.rich(Frogs.genind, min.alleles = NULL) Richness$alleles.sampled ## [1] 6 Populations with more alleles are resampled to determine the average allelic richness among the minimum number of alleles. Here, this means that 6 alleles are sampled from each population, allelic richness is calculated, and the process is repeated many times to determine the average). Let’s plot the results again. The barchart shows that there is considerable variation in genetic diversity among ponds. The scatterplot against sample size (here: for each population, the average number of valid alleles across loci) suggests that the variation is not related to sample size. The regression line (red) is almost horizontal. Here we plot the average Ar across loci, so that the result does not depend on the number of loci used. par(mar=c(5.5, 4.5,1,1)) barplot(Richness$mean.richness, las=3, ylab=&quot;Rarefied allelic richness (Ar)&quot;) plot(colMeans(Richness$pop.sizes), Richness$mean.richness, xlab=&quot;Valid sample size&quot;, ylab=&quot;Rarefied allelic richness (Ar)&quot;) abline(lm(Richness$mean.richness ~ colMeans(Richness$pop.sizes)), col=&quot;red&quot;) b. Observed and expected heterozygosity Note: Writing the ‘genind’ summary into an object ‘Sum’ allows accessing its attributes by name. Sum &lt;- summary(Frogs.genind) names(Sum) ## [1] &quot;n&quot; &quot;n.by.pop&quot; &quot;loc.n.all&quot; &quot;pop.n.all&quot; &quot;NA.perc&quot; &quot;Hobs&quot; ## [7] &quot;Hexp&quot; Expected heterozygosity (here: Hexp) is the heterozygosity expected in a population under HWE, and observed heterozygosity (here: Hobs) is the observed number of heterozygotes at a locus divided by the total number of genotyped individuals. Here are the global values (pooled across all populations): par(mar=c(3, 4.5,1,1)) barplot(Sum$Hexp, ylim=c(0,1), ylab=&quot;Expected heterozygosity&quot;) barplot(Sum$Hobs, ylim=c(0,1), ylab=&quot;Observed heterozygosity&quot;) By locus and population: Here we use ‘seppop’ to split the genind object by population, then ‘sapply’ to apply function ‘summary’ to each population. Hobs &lt;- t(sapply(seppop(Frogs.genind), function(ls) summary(ls)$Hobs)) Hexp &lt;- t(sapply(seppop(Frogs.genind), function(ls) summary(ls)$Hexp)) {cat(&quot;Expected heterozygosity (Hexp):&quot;, &quot;\\n&quot;) round(Hexp, 2)} ## Expected heterozygosity (Hexp): ## A B C D E F G H ## Airplane 0.49 0.28 0.00 0.36 0.67 0.00 0.23 0.72 ## Bachelor 0.00 0.54 0.32 0.22 0.70 0.00 0.50 0.65 ## BarkingFox 0.07 0.25 0.22 0.49 0.63 0.07 0.20 0.33 ## Bob 0.14 0.33 0.33 0.14 0.79 0.00 0.10 0.70 ## Cache 0.00 0.57 0.38 0.24 0.77 0.00 0.48 0.70 ## Egg 0.00 0.39 0.00 0.00 0.79 0.00 0.46 0.72 ## Frog 0.00 0.43 0.36 0.44 0.62 0.00 0.50 0.58 ## GentianL 0.10 0.66 0.00 0.72 0.80 0.14 0.48 0.73 ## ParagonL 0.00 0.19 0.07 0.15 0.29 0.00 0.50 0.57 ## Pothole 0.00 0.00 0.00 0.28 0.38 0.00 0.58 0.63 ## ShipIsland 0.44 0.49 0.00 0.51 0.60 0.00 0.44 0.66 ## Skyhigh 0.04 0.49 0.20 0.34 0.63 0.00 0.52 0.62 {cat(&quot;\\n&quot;, &quot;Observed heterozygosity (Hobs):&quot;, &quot;\\n&quot;) round(Hobs, 2)} ## ## Observed heterozygosity (Hobs): ## A B C D E F G H ## Airplane 0.25 0.33 0.00 0.33 0.62 0.00 0.16 0.74 ## Bachelor 0.00 0.38 0.40 0.25 0.88 0.00 0.33 0.75 ## BarkingFox 0.07 0.14 0.00 0.57 0.79 0.07 0.22 0.23 ## Bob 0.15 0.38 0.42 0.15 0.92 0.00 0.11 0.67 ## Cache 0.00 0.83 0.00 0.29 1.00 0.00 0.40 0.86 ## Egg 0.00 0.41 0.00 0.00 0.87 0.00 0.36 0.87 ## Frog 0.00 0.38 0.14 0.56 0.86 0.00 0.33 1.00 ## GentianL 0.11 0.95 0.00 0.74 0.90 0.15 0.56 0.84 ## ParagonL 0.00 0.11 0.08 0.16 0.33 0.00 0.36 0.47 ## Pothole 0.00 0.00 0.00 0.33 0.50 0.00 0.57 0.73 ## ShipIsland 0.41 0.41 0.00 0.59 0.53 0.00 0.36 0.71 ## Skyhigh 0.04 0.57 0.11 0.30 0.52 0.00 0.77 0.59 Locus F shows variation only in two populations (i.e., Hexp = 0 in 10 populations). Let’s plot the average across all loci for each population: Here we use ‘apply’ to apply the function ‘mean’ to the rows (MARGIN = 1). For columns, use ‘MARGIN = 2’. par(mar=c(5.5, 4.5, 1, 1)) Hobs.pop &lt;- apply(Hobs, MARGIN = 1, FUN = mean) Hexp.pop &lt;- apply(Hexp, 1, mean) barplot(Hexp.pop, ylim=c(0,1), las=3, ylab=&quot;Expected heterozygosity&quot;) barplot(Hobs.pop, ylim=c(0,1), las=3, ylab=&quot;Observed heterozygosity&quot;) c. Create table with sitel-level genetic diversity measures Frogs.diversity &lt;- data.frame(Pop = names(Hobs.pop), n = Sum$n.by.pop, Hobs = Hobs.pop, Hexp = Hexp.pop, Ar = Richness$mean.richness) Frogs.diversity ## Pop n Hobs Hexp Ar ## Airplane Airplane 21 0.3038064 0.3433019 1.939629 ## Bachelor Bachelor 8 0.3729167 0.3651953 2.003673 ## BarkingFox BarkingFox 14 0.2619811 0.2818609 1.741904 ## Bob Bob 13 0.3504274 0.3171011 1.961791 ## Cache Cache 7 0.4220238 0.3923413 2.085115 ## Egg Egg 17 0.3135918 0.2951215 1.841382 ## Frog Frog 9 0.4079861 0.3659439 1.925469 ## GentianL GentianL 20 0.5296418 0.4529000 2.331599 ## ParagonL ParagonL 19 0.1880302 0.2200536 1.539923 ## Pothole Pothole 13 0.2665043 0.2328137 1.597687 ## ShipIsland ShipIsland 17 0.3755252 0.3918983 1.949255 ## Skyhigh Skyhigh 23 0.3632542 0.3552167 1.953610 You can save the R object ‘Frogs.diversity’ with the code below (need to uncomment by removing the hashtags ‘#’): #require(here) #if(!dir.exists(paste0(here(),&quot;/output&quot;))) dir.create(paste0(here(),&quot;/output&quot;)) #save(Frogs.diversity, file = paste0(here(),&quot;/output/Frogs.diversity.RData&quot;)) #load(paste0(here(),&quot;/output/Frogs.diversity.RData&quot;)) 4. Aggregate genetic data at population level (allele frequencies) For some analyses, we will need to aggregate data from the individual to the population level, e.g. as a table of allele frequencies per population. Here we convert the ‘genind’ object to a ‘genpop’ object (NOT the same as a ‘genepop’ object!). This is defined in the package ‘adegenet’ to hold population-level genetic data. The function ‘genind2genpop’ obviously converts from ‘genind’ to ‘genpop’. Frogs.genpop &lt;- adegenet::genind2genpop(Frogs.genind) ## ## Converting data from a genind to a genpop object... ## ## ...done. The function ‘makefreq’ extracts the table with allele frequencies from the ‘genpop’ object. We’ll plot just a few lines and alleles. Freq &lt;- adegenet::makefreq(Frogs.genpop) ## ## Finding allelic frequencies from a genpop object... ## ## ...done. round(Freq[1:6,1:10], 2) ## A.1 A.2 A.3 B.1 B.3 B.2 B.4 C.1 C.2 C.4 ## Airplane 0.62 0.35 0.03 0.83 0.17 0.00 0.00 1.00 0.00 0 ## Bachelor 1.00 0.00 0.00 0.56 0.06 0.38 0.00 0.80 0.20 0 ## BarkingFox 0.96 0.00 0.04 0.86 0.04 0.11 0.00 0.88 0.12 0 ## Bob 0.92 0.00 0.08 0.81 0.00 0.12 0.08 0.79 0.21 0 ## Cache 1.00 0.00 0.00 0.42 0.00 0.50 0.08 0.75 0.25 0 ## Egg 1.00 0.00 0.00 0.74 0.00 0.26 0.00 1.00 0.00 0 The allele frequencies of all alleles from the same locus (e.g., A.1, A.2 and A.3) should sum to 1 for each population. With eight loci, the row sums should thus add to 8. apply(Freq, MARGIN = 1, FUN = sum) # Just checking ## Airplane Bachelor BarkingFox Bob Cache Egg Frog ## 8 8 8 8 8 8 8 ## GentianL ParagonL Pothole ShipIsland Skyhigh ## 8 8 8 8 8 "],["r-exercise-week-3.html", "6.4 R Exercise Week 3", " 6.4 R Exercise Week 3 Task: Drop offspring (seeds) from dataset pulsatilla_genotypes.csv, check for HWE by site and locus and calculate Hexp for each site. Hints: Load packages: Make sure the packages gstudio, dplyr and adegenet are loaded. Import data: Re-use your code from Week 1 exercise to import the dataset pulsatilla_genotypes.csv into gstudio. Count genotyped individuals. Determine the number of rows (and thus genotyped individuals). The dataset contains adults (OffID == 0) and genotyped seeds (OffID != 0). Determine the number of adults in the dataset. You can achieve this either by subsetting with square brackets [ ], or as a pipe using the function filter from the dplyr package, followed by nrow(). Drop offspring from dataset: Subset the data to retain only the adults, and call it Pulsatilla.adults. Again, you can achieve this either by indexing with square brackets, or by using the function filter from the dplyr package. Check the number of rows (adults). Split dataset by site. Use function split to split the data by site (population) and create an object Adults.by.site. Determine the length of the resulting list, i.e., the number of sub-datasets, one for each site. Count adults per site with sapply: Use sapply to calculate the number of rows (and thus genotyped individuals) per site (population). What is the range of sample sizes for adults? Convert to genind object: adapt your code from Week 1 exercise to convert the dataset with all adults, Pulsatilla.adults, to a genind object. Print the object to check that the data have been correctly imported. Is the number of rows equal to the number of adults that you found above? Check polymorphism: Use function summary (section 2.b) to check whether markers are polymorphic: what is the range of expected heterozygosity among the loci? Test for HWE by site and locus: adapt the code from section 2.c to test for HWE deviations across by site and locus (using chi-square or Monte-Carlo test). How many tests were significant (p-value &lt; 0.05)? Is there a systematic pattern of deviations for a specific locus, or for a specific site? Calculate Hexp and Hobs by site: adapt code from section 3.b to calculate Hexp and Hobs by site and locus, then take the mean across all loci (Hexp.pop, Hobs.pop) and combine them into a dataframe H.pop. Include the population name as a variable. Save result as R object: Save the object H.pop as an R object using the following code: saveRDS(H.pop, file = paste0(here::here(), \"/output/H.pop.rds\")). We will need it for a later R exercise. Question: Which site had the lowest expected heterozygosity? "],["Week4.html", "7 Lab 4: Metapopulations", " 7 Lab 4: Metapopulations In this week’s computer lab, we will test for genetic effects of metapopulation dynamics. Along the way, we will review regression analysis and learn how to create plots with the ggplot2 package. View Course Video Interactive Tutorial 4 Worked Example R Exercise Week 4 Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_4.html", "7.1 View Course Video", " 7.1 View Course Video 1. Embedded Video External link: Week 4 video (Part 1); Week 4 video (Part 2) Transcript: Download transcript Video, Part 1 iframe not supported Video, Part 2 iframe not supported Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_4.html", "7.2 Interactive Tutorial 4", " 7.2 Interactive Tutorial 4 1. List of R commands covered this week Function Package ecoslot.SlotName EcoGenetics sort base order base row.names base rank base sample base is.numeric base cor stats plot graphics with base lm stats summary base abline graphics points graphics qplot ggplot2 ggplot ggplot2 aes ggplot2 geom_point ggplot2 geom_smooth ggplot2 xlab, ylab ggplot2 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_4.html", "7.3 Worked Example", " 7.3 Worked Example Nusha Keyghobadi, Thomas Lamy and Helene Wagner 1. Overview of Worked Example a. Goals This worked example shows how to: Assess the spatial distribution of genetic structure in a metapopulation using hierarchical AMOVA Relate site-specific Fst to patch connectivity and population size Relate site-specific genetic diversity to explanatory variables (node-level analysis) Assess temporal changes (between years for same site) and evidence for extinction events Perform power analysis and sample size calculation for the temporal study b. Data set Lamy et al. (2012) sampled the freshwater snail Drepanotrema depressissimum in a fragmented landscape of tropical ponds on the island of Guadeloupe in the French West Indies. They used a spatially and temporally stratified sampling design with a total of 25 sites, where 12 sites formed four well-separated clusters of three neighbouring sites each, to study spatial variability, and 12 sites spread across the island were sampled in multiple years to study temporal variability. For each site and year, 22 - 34 individuals were genotyped at ten microsatellite loci. The species is diploid, hermaphroditic, and outcrossed. A key characteristic of this system is the presence of a dry and a rainy season. In the dry season, many ponds can dry out, possibly causing extinction of the local snail populations. During the rainy season, ponds refill and can even overflow, thereby becoming connected through the hydrological network. During this rainy season, dispersal between ponds may occur. dd.genind: The dataset ‘dd.genind’ with genetic data for 1270 snails from 42 populations is included in package ‘LandGenCourse’. To load it, type: data(dd.genind). dd.site: Population-level data from Tables 2 - 5 of Lamy et al. (2012) are available in dataset dd.site (with 25 variables) in package LandGenCourse. To load it, type: data(dd.site). Reference Lamy, T., Pointier, J. P., Jarne, P. and David, P. (2012), Testing metapopulation dynamics using genetic, demographic and ecological data. Molecular Ecology, 21: 1394–1410. doi:10.1111/j.1365-294X.2012.05478.x c. Required R libraries library(LandGenCourse) library(methods) library(dplyr) #require(tibble) #require(poppr) #require(ade4) #require(pwr) #require(effsize) require(sf) #require(car) library(ggplot2) library(tmap) d. Import data Let’s import the genetic data (spatial and temporal data sets combined, 42 combinations of site and year). The ‘genind’ object ‘dd.genind’ contains individual-level data in the following slots: tab: a matrix of allele frequencies (one column per allele) loc.fac: a factor that identifies the locus for each allele loc.n.all: a vector with the number of alleles per locus all.names: a list with allele names ploidy: a vector with the ploidy of each individual (row) type: marker type (here: “codom” as microsatellites are codominant) other: a list with additional variables, here: spatial coordinates xy: a data frame with spatial coordinates (lat-long format) call: call that created the object. Here it is empty. **pop*: a factor with the population of each individual. Here it is empty as the information is in (strata?). strata: a data frame with structure variables, here: SiteID, SITE, YEAR, Cluster hierarchy: a hierarchy. Here it is empty as the information is in (strata?). data(dd.genind, package = &quot;LandGenCourse&quot;) dd.genind ## Loading required namespace: adegenet ## /// GENIND OBJECT ///////// ## ## // 1,270 individuals; 10 loci; 372 alleles; size: 2.2 Mb ## ## // Basic content ## @tab: 1270 x 372 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 18-50) ## @loc.fac: locus factor for the 372 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: NULL ## ## // Optional content ## @strata: a data frame with 4 columns ( SiteID, SITE, YEAR, Cluster ) ## @other: a list containing: xy We also import site-level data from Tables 2 - 5 in Lamy et al. (2012). This is a (spatial) sf object. Use ‘?dd.site’ to check the helpfile with the data set description of the variables (atribute data). data(dd.site, package = &quot;LandGenCourse&quot;) tibble::as_tibble(dd.site) ## # A tibble: 42 × 23 ## SiteID SITE YEAR Spatial MultiYear APE Cluster n RA He f ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PICO2006 PICO 2006 FALSE TRUE FALSE &lt;NA&gt; 34 11.2 0.897 0.038 ## 2 PICO2007 PICO 2007 FALSE TRUE FALSE &lt;NA&gt; 24 11.4 0.911 0.022 ## 3 PICO2009 PICO 2009 TRUE TRUE FALSE &lt;NA&gt; 32 11.4 0.896 0.024 ## 4 ROC2006 ROC 2006 TRUE TRUE FALSE &lt;NA&gt; 32 11.2 0.831 0.005 ## 5 ROC2007 ROC 2007 FALSE TRUE FALSE &lt;NA&gt; 32 11.0 0.82 0.018 ## 6 SEN2006 SEN 2006 FALSE TRUE FALSE &lt;NA&gt; 32 11.2 0.876 -0.008 ## 7 SEN2007 SEN 2007 TRUE TRUE FALSE &lt;NA&gt; 30 11.6 0.88 0.046 ## 8 VEE2006 VEE 2006 FALSE TRUE FALSE &lt;NA&gt; 31 10.5 0.864 0.034 ## 9 VEE2007 VEE 2007 FALSE TRUE FALSE &lt;NA&gt; 32 11.3 0.854 0.044 ## 10 VEE2008 VEE 2008 TRUE TRUE FALSE &lt;NA&gt; 31 11.2 0.871 0.014 ## # ℹ 32 more rows ## # ℹ 12 more variables: s &lt;dbl&gt;, Type &lt;fct&gt;, FST.GESTE &lt;dbl&gt;, Size &lt;dbl&gt;, ## # V &lt;dbl&gt;, C &lt;dbl&gt;, Stab &lt;dbl&gt;, D &lt;int&gt;, APA &lt;int&gt;, NLT &lt;dbl&gt;, ## # Fst.temp &lt;dbl&gt;, geometry &lt;POINT [°]&gt; ?dd.site Questions: with the help file for ‘dd.site’, check the meaning of the following explanatory variables: What does ‘APE’ refer to, and how is it different from ‘APA’? What does ‘NLT’ represent, and is it calculated independently from ‘Size’? What does ‘Type’ mean, and what about ‘V’ and ‘D’? To understand how connectivity ‘C’ and stability ‘Stab’ were calculated, you’ll need to consult Lamy et al. (2012). Your hypothesis: which explanatory variables would you expect to affect: Genetic diversity within local populations? Genetic differentiation among local populations? Both? In the following, we’ll perform three types of analyses: Compare 25 populations in space, across the island of Guadeloupe. Compare 12 populations in 4 clusters: differentiation within vs. among clusters? Compare 12 sites over time, some of which experienced a local extinction event. 2. Spatial distribution of genetic structure How similar are populations from nearby habitat patches compared to populations across the island? To answer this question, we perform a hiearchical AMOVA (analysis of molecular variance) with individuals from 12 populations that form 4 clusters with 3 populations each. a. Subsetting the ‘genind’ object First, we need to extract the samples that belong to the hierarchical data set. There are four clusters: “North”, “East”, “Center” and “South”. We are looking for the observations where the variable “Cluster” has one of these four values, all other observations will have a missing value for “Cluster”. We can use ‘!is.na’ to identify the rows with non-missing values. Because a genind object is at its core a data frame @tab with rows (individuals) and columns(alleles), we can subset rows or columns. Here we subset rows. dd.genind.Cluster &lt;- dd.genind[!is.na(dd.genind@strata$Cluster),] b. Hierarchical AMOVA There are several implementations of AMOVA in R, e.g. in pacakges ‘ade4’, ‘pegas’ and ‘vegan’. The ‘ade4’ implementation is closest to the original implementation in Arlequin. Package ‘poppr’ has a wrapper function ‘poppr.amova’ that makes it easy to perform AMOVA with the ‘ade4’ or with the ‘pegas’ implementation (see ‘?poppr.amova’ for a discussion of their pros and cons). Here we’ll use ‘ade4’. The first argument is the ‘genind’ object. The argument ‘hier’ defines the hierarchy, with the top level first (i.e., here SITE is nested within Cluster). The variables are expected to be found in the @strata slot of the ‘genind’ object. The argument ‘within=FALSE’ specifies that within-individual variance (i.e., observed heterozygosity) should not be tested. Setting this to ‘TRUE’ can lead to problems with missing values. First we run the AMOVA and estimate the percent of molecular variance at each hierarchical level. amova.result &lt;- poppr::poppr.amova(dd.genind.Cluster, hier = ~ Cluster/SITE, within=FALSE, method = &quot;ade4&quot;) ## Registered S3 method overwritten by &#39;pegas&#39;: ## method from ## print.amova ade4 ## ## No missing values detected. amova.result ## $call ## ade4::amova(samples = xtab, distances = xdist, structures = xstruct) ## ## $results ## Df Sum Sq Mean Sq ## Between Cluster 3 67.27429 22.424764 ## Between samples Within Cluster 8 158.37917 19.797396 ## Within samples 353 1563.08216 4.427995 ## Total 364 1788.73562 4.914109 ## ## $componentsofcovariance ## Sigma % ## Variations Between Cluster 0.02770227 0.5582719 ## Variations Between samples Within Cluster 0.50644882 10.2062461 ## Variations Within samples 4.42799478 89.2354820 ## Total variations 4.96214587 100.0000000 ## ## $statphi ## Phi ## Phi-samples-total 0.107645180 ## Phi-samples-Cluster 0.102635446 ## Phi-Cluster-total 0.005582719 Then we test whether each variance component is statistically significant (i.e., significantly larger than zero). Note: if this takes very long, you can set nrepet = 199 for this exercise (not for your research). amova.test &lt;- ade4::randtest(amova.result, nrepet = 999) amova.test ## class: krandtest lightkrandtest ## Monte-Carlo tests ## Call: randtest.amova(xtest = amova.result, nrepet = 999) ## ## Number of tests: 3 ## ## Adjustment method for multiple comparisons: none ## Permutation number: 999 ## Test Obs Std.Obs Alter Pvalue ## 1 Variations within samples 4.42799478 -83.294544 less 0.001 ## 2 Variations between samples 0.50644882 56.488569 greater 0.001 ## 3 Variations between Cluster 0.02770227 1.786349 greater 0.039 Questions: At what level is there stronger differentiation, within or among clusters? What does this mean biologically? Are both levels statistically significant? 3. What determines genetic differentiation among sites? What factors explain site-specific Fst? Let’s consider the key micro-evolutionary processes: Genetic drift: the smaller the population, the higher the rate of drift, hence we expect higher differention for small populations. Predictor: long-term population size ‘NLT’. Gene flow: gene flow homogenizes allele frequencies, hence we expect less differentiation for well connected patches. Predictors: connectivity ‘C’, density of favorable habitat ‘D’ (within 2 km radius). First, we create a new sf object with the subset of data for the spatial analysis (25 ponds, one year each). dd.spatial &lt;- dd.site[dd.site$Spatial==TRUE,] a. Correlation matrix Let’s start with a correlation matrix. First, we need to drop the geometry from the sf object with st_drop_geometry. This will return a data frame dd.df with the attribute data. We select the variables that we want to correlate with the function cor. The attribute use of the function cor defines how to handle missing values. use=\"pairwise.complete\" means that for each pair of variables, those individuals with no missing values for these variable will be used. See help file ?cor for alternatives. dd.df &lt;- st_drop_geometry(dd.spatial) cor(dd.df[ , c(&quot;FST.GESTE&quot;, &quot;NLT&quot;, &quot;C&quot;, &quot;D&quot;)], use=&quot;pairwise.complete&quot;) ## FST.GESTE NLT C D ## FST.GESTE 1.00000000 -0.48565247 -0.43104600 0.02768978 ## NLT -0.48565247 1.00000000 -0.08369541 0.10242973 ## C -0.43104600 -0.08369541 1.00000000 -0.32078070 ## D 0.02768978 0.10242973 -0.32078070 1.00000000 Questions: Is there genetic evidence for higher drift in small populations? Is there genetic evidence for higher gene flow among well connected patches? Are the two factors confounded for this data set? Would you prefer ‘C’ or ‘D’ to quantify patch connectivity? Does it matter? What does this mean biologically? b. Scatterplots Let’s plot the response variable FST.GESTE against each of the two predictors NLT and C. Here, we use functions from the package ggplot2 (already loaded) to define two ggplot objects NLT.plot and C.plot, then we plot them side-by-side with the function cowplot::plot_grid. For NLT.plot, we define the dataset as dd.spatial@data, the x-axis as variable NLT, the y-axis as variable FST.GESTE, and the labels as variable SITE. We add points with geom_point. We add a regression line (geom_smooth), make it linear (method = lm) and add a shaded area for plus/minus 1 SE of the mean for a given value of x (se = TRUE). We add backfilled labels (geom_label), define their size (size), and move them up a little along the y-axis (nudge_y) NLT.plot &lt;- ggplot(dd.df, aes(x=NLT, y=FST.GESTE)) + geom_point() + geom_smooth(formula = &#39;y ~ x&#39;, method = lm, se = TRUE) + geom_text(aes(x=NLT, y=FST.GESTE, label=SITE), size=2, nudge_x=0, nudge_y=0.01, check_overlap=TRUE) C.plot &lt;- ggplot(dd.df, aes(x=C, y=FST.GESTE)) + geom_point() + geom_smooth(formula = &#39;y ~ x&#39;, method = lm, se = TRUE) + geom_text(aes(x=C, y=FST.GESTE, label=SITE), size=2.5, nudge_x=0, nudge_y=0.01, check_overlap=TRUE) cowplot::plot_grid(NLT.plot, C.plot) c. Regression model The two predictors ‘NLT’ and ‘C’ are not strongly correlated. We’ll fit a regression model with both predictors. Here we use function ‘scale’ to standardize each variable, so that we can interpret the regression slope coefficients as partial correlation coefficients (beta coefficients). mod.diff &lt;- lm(scale(FST.GESTE) ~ scale(NLT) + scale(C), data=dd.spatial) summary(mod.diff) ## ## Call: ## lm(formula = scale(FST.GESTE) ~ scale(NLT) + scale(C), data = dd.spatial) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0576 -0.4142 -0.1361 0.2653 2.2394 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.181e-16 1.535e-01 0.000 1.00000 ## scale(NLT) -5.254e-01 1.572e-01 -3.342 0.00296 ** ## scale(C) -4.750e-01 1.572e-01 -3.021 0.00628 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7676 on 22 degrees of freedom ## Multiple R-squared: 0.4599, Adjusted R-squared: 0.4108 ## F-statistic: 9.367 on 2 and 22 DF, p-value: 0.00114 Is the model valid? Let’s check the residual plots. Here’s a link to a great resource about the interpretation of these plots generated by R: http://strata.uga.edu/6370/rtips/regressionPlots.html par(mfrow=c(2,2)) plot(mod.diff, labels.id = dd.spatial$SITE) par(mfrow=c(1,1)) If we had more than two predictors, it would be a good idea to calculate variance inflation factors. The package ‘car’ has a function ‘vif’ that takes as argument a fitted model. Here, both predictors have VIF = 1.007, which indicates no collinearity. car::vif(mod.diff) ## scale(NLT) scale(C) ## 1.007054 1.007054 d. Which populations don’t fit the general pattern? Let’s plot the residuals in space. The function tm_bubbles from the package ‘sf’ evaluates the projection information of the sf object ‘dd.spatial’. First, we need to create some new variables: Add the residuals as variable to dd.spatial. Add the absolute valuel of the residuals as another variable. Create an index variable a that identifies potential outliers (here: absolute value &gt; 1.5) dd.spatial$Residuals &lt;- mod.diff$residuals dd.spatial$Absolute &lt;- abs(mod.diff$residuals) a &lt;- which(dd.spatial$Absolute &gt; 1.5) Now we can create a bubble plot with the size of the bubble proportional to the absolute value of the residual, and the color according to the sign (positive or negative). For the latter, we use the argument breaks. Here, any value between -Inf and 0 will be plotted in red, and any value between 0 and Inf will be plotted in blue. In addition, we label the two largest outliers (with absolute values &gt;1.5) and position the labels with the argument just. Here, just=c(0.7,2.5) means that the labels are placed at 0.7 along the horizontal axes and at 2.5 along the vertical axis, compared to the point location. This requires some playing around with values. tmap_mode(&quot;plot&quot;) ## ℹ tmap modes &quot;plot&quot; - &quot;view&quot; ## ℹ toggle with `tmap::ttm()` Map1 &lt;- tm_shape(dd.spatial) + tm_bubbles(size=&quot;Absolute&quot;,col=&quot;Residuals&quot;, breaks=c(-Inf, 0, Inf), palette=c(&quot;red&quot;, &quot;blue&quot;)) + tm_shape(dd.spatial[a,]) + tm_text(text=&quot;SITE&quot;, size=0.8, just=c(0.7,2.5)) ## ## ── tmap v3 code detected ─────────────────────────────────────────────────────── ## [v3-&gt;v4] `tm_tm_bubbles()`: migrate the argument(s) related to the scale of the ## visual variable `fill` namely &#39;breaks&#39;, &#39;palette&#39; (rename to &#39;values&#39;) to ## fill.scale = tm_scale(&lt;HERE&gt;).[v3-&gt;v4] `tm_bubbles()`: use &#39;fill&#39; for the fill color of polygons/symbols ## (instead of &#39;col&#39;), and &#39;col&#39; for the outlines (instead of &#39;border.col&#39;).[v3-&gt;v4] `tm_text()`: migrate the layer options &#39;just&#39; to &#39;options = ## opt_tm_text(&lt;HERE&gt;)&#39; Map1 Export this map as a pdf file. (Un-comment the lines below to run the code below, i.e., remove the hashtag symsbols ‘#’). #if(!dir.exists(here::here(&quot;output&quot;))) dir.create(here::here(&quot;output&quot;)) #tmap_save(Map1, file=here::here(&quot;output/ResidualMap.pdf&quot;), width = 7, height = 5.5, units = &quot;in&quot;, dpi = 300) By changing the mode to view, we can create convert the map into an interactive plot with a background map from the internet (see Week 2). Un-comment the lines below to run the code. #tmap_mode(&quot;view&quot;) #Map1 What might explain the large residuals for the two sites ‘PTC’ and ‘DESB’? Site ‘PTC’ lies on the tip of a peninsula and thus is very isolated geographically. Site ‘DESB’ is a very instable site that can frequently dry out during the dry season, as it is shallow and lies in the comparatively dry northern part of the island. In addition, although ‘DESB’ is surrounded by many ponds, these ponds never get connected to ‘DESB’ hydrologically during the rainy season. Therefore, immigration can only occur via cattle or birds, which are much less important drivers of gene flow than immigration by hydrological connectivity during the rainy season. e. Regression model without outliers We can use the same index a to exclude the potential outliers from the regression model: mod.diff.minus2 &lt;- lm(scale(FST.GESTE) ~ scale(NLT) + scale(C), data=dd.spatial[-a,]) summary(mod.diff.minus2) ## ## Call: ## lm(formula = scale(FST.GESTE) ~ scale(NLT) + scale(C), data = dd.spatial[-a, ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2815 -0.5002 -0.1009 0.4732 1.7142 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.498e-16 1.562e-01 0.000 1.00000 ## scale(NLT) -5.096e-01 1.622e-01 -3.142 0.00514 ** ## scale(C) -5.761e-01 1.622e-01 -3.552 0.00200 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7493 on 20 degrees of freedom ## Multiple R-squared: 0.4896, Adjusted R-squared: 0.4386 ## F-statistic: 9.593 on 2 and 20 DF, p-value: 0.001199 Did omitting the two sites improve model fit? Did it change the nature of the results? Does this affect the biological interpretation? par(mfrow=c(2,2)) plot(mod.diff.minus2, labels.id = dd.spatial$SITE[-a]) par(mfrow=c(1,1)) 4. What determines genetic diversity? Can the same predictors (population size and connectivity) explain genetic diversity? Is patch size (‘Size’) a good proxy for population size (as often used in ecological studies)? Which measure of genetic diversity shows the stronger response, allelic richness (rarefied) or expected heterozygosity? a. Correlation matrix cor(dd.df[, c(&quot;RA&quot;, &quot;He&quot;, &quot;Size&quot;, &quot;NLT&quot;, &quot;C&quot;, &quot;D&quot;)], use=&quot;pairwise.complete&quot;) ## RA He Size NLT C D ## RA 1.00000000 0.95151016 0.3482453 0.51343861 0.46576979 -0.09856668 ## He 0.95151016 1.00000000 0.3057086 0.49851503 0.45206733 -0.09593604 ## Size 0.34824532 0.30570855 1.0000000 0.76128271 -0.17765815 0.30002700 ## NLT 0.51343861 0.49851503 0.7612827 1.00000000 -0.08369541 0.10242973 ## C 0.46576979 0.45206733 -0.1776581 -0.08369541 1.00000000 -0.32078070 ## D -0.09856668 -0.09593604 0.3000270 0.10242973 -0.32078070 1.00000000 Questions: How strongly are the two diversity measures ‘RA’ and ‘He’ correlated? Are ‘NLT’ and ‘C’ strongly correlated with the diversity measures ‘RA’ and ‘He’? Is the correlation with ‘Size’ similarly strong as the correlation with ‘NLT’? How strongly are ‘Size’ and ‘NLT’ correlated with each other? Does ‘D’ show a stronger correlation with diversity than with differentiation? b. Regression models For allelic richness: mod.RA &lt;- lm(scale(RA) ~ scale(NLT) + scale(C), data = dd.spatial) summary(mod.RA) ## ## Call: ## lm(formula = scale(RA) ~ scale(NLT) + scale(C), data = dd.spatial) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.5115 -0.5140 0.2356 0.4995 1.2462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.963e-16 1.441e-01 0.000 1.00000 ## scale(NLT) 5.563e-01 1.476e-01 3.770 0.00106 ** ## scale(C) 5.123e-01 1.476e-01 3.472 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7204 on 22 degrees of freedom ## Multiple R-squared: 0.5243, Adjusted R-squared: 0.481 ## F-statistic: 12.12 on 2 and 22 DF, p-value: 0.0002825 par(mfrow=c(2,2)) plot(mod.RA, labels.id = dd.spatial$SITE) par(mfrow=c(1,1)) For gene diversity (expected heterozygosity): mod.He &lt;- lm(scale(He) ~ scale(NLT) + scale(C), data = dd.spatial) summary(mod.He) ## ## Call: ## lm(formula = scale(He) ~ scale(NLT) + scale(C), data = dd.spatial) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7545 -0.3619 0.1211 0.4458 1.2550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.456e-16 1.486e-01 0.000 1.00000 ## scale(NLT) 5.401e-01 1.522e-01 3.549 0.00180 ** ## scale(C) 4.973e-01 1.522e-01 3.268 0.00352 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7429 on 22 degrees of freedom ## Multiple R-squared: 0.4941, Adjusted R-squared: 0.4481 ## F-statistic: 10.74 on 2 and 22 DF, p-value: 0.0005559 par(mfrow=c(2,2)) plot(mod.He, labels.id = dd.spatial$SITE) par(mfrow=c(1,1)) 5. Are genetic differentiation and diversity related? Would you expect a relationship between genetic diversity and genetic differentiation of individual patches? Lets examine the correlation between gene diversity (He) and site-specific Fst: cor(dd.site$He, dd.site$FST.GESTE, use = &quot;pairwise.complete&quot;) ## [1] -0.9567184 There are a number of possible reasons for such a correlation. Can you put forward some hypotheses to explain this relationship? See Lamy et al. (2012) for their interpretation. 6. Effect of recent extinction events Several patches fell dry between observation years, which is assumed to signify extinction of the local population. Does genetic evidence support this interpretation, i.e., is there genetic evidence of bottlenecks or founder effects in D. depressissimum? a. Effect of patch extinction event (temporal data set) dd.temporal &lt;- dd.site[dd.site$MultiYear==TRUE,] dd.temporal.df &lt;- sf::st_drop_geometry(dd.temporal) cor(dd.temporal.df[, c(&quot;Fst.temp&quot;, &quot;APE&quot;, &quot;NLT&quot;, &quot;C&quot;)], use=&quot;pairwise.complete&quot;) ## Fst.temp APE NLT C ## Fst.temp 1.0000000 0.29379662 -0.10494809 -0.3356817 ## APE 0.2937966 1.00000000 -0.07420821 -0.4927664 ## NLT -0.1049481 -0.07420821 1.00000000 0.3103541 ## C -0.3356817 -0.49276638 0.31035409 1.0000000 We can compare a number of competing models using the Akaike Information Criterion (AIC). Models with lower AIC are better (see Week 12). mod.Fst.temp &lt;- lm(scale(Fst.temp) ~ scale(APE), data=dd.temporal.df) summary(mod.Fst.temp) ## ## Call: ## lm(formula = scale(Fst.temp) ~ scale(APE), data = dd.temporal.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0290 -0.5094 -0.3566 0.2140 2.0886 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.04099 0.29246 -0.140 0.891 ## scale(APE) 0.27599 0.28396 0.972 0.354 ## ## Residual standard error: 1.003 on 10 degrees of freedom ## (17 observations deleted due to missingness) ## Multiple R-squared: 0.08632, Adjusted R-squared: -0.005052 ## F-statistic: 0.9447 on 1 and 10 DF, p-value: 0.354 mod.Fst.temp.C &lt;- lm(scale(Fst.temp) ~ scale(APE) + scale(C), data=dd.temporal.df) mod.Fst.temp.NLT &lt;- lm(scale(Fst.temp) ~ scale(APE) + scale(NLT), data=dd.temporal.df) mod.Fst.temp.both &lt;- lm(scale(Fst.temp) ~ scale(APE) + scale(NLT) + scale(C), data=dd.temporal.df) AIC(mod.Fst.temp, mod.Fst.temp.C, mod.Fst.temp.NLT, mod.Fst.temp.both) ## df AIC ## mod.Fst.temp 3 37.92714 ## mod.Fst.temp.C 4 39.27770 ## mod.Fst.temp.NLT 4 39.83549 ## mod.Fst.temp.both 5 41.27462 The best model includes neither ‘C’ nor ‘NLT’. Note that ‘APE’ is a binary variable, so in essence we’re performing a t-test here. res.Fst.temp &lt;- t.test(Fst.temp ~ APE, data=dd.temporal, alternative = &quot;less&quot;) res.Fst.temp ## ## Welch Two Sample t-test ## ## data: Fst.temp by APE ## t = -0.94604, df = 7.893, p-value = 0.1861 ## alternative hypothesis: true difference in means between group FALSE and group TRUE is less than 0 ## 95 percent confidence interval: ## -Inf 0.01085372 ## sample estimates: ## mean in group FALSE mean in group TRUE ## 0.0080 0.0192 b. Power analysis The effect is not statistically significant. Does that mean that we found no effect of apparent population extinctions on temporal Fst? Let’s check effect size. For means, Cohen’s effect size is measured by d (which is measured in units of standard deviations): small effect: d &gt; 0.2 (means at least 0.2 standard deviations apart) medium effect: d &gt; 0.5 large effect: d &gt; 0.8 We can let R calculate effect size for us: effsize::cohen.d(Fst.temp ~ factor(APE), data=dd.temporal.df) ## ## Cohen&#39;s d ## ## d estimate: -0.5691221 (medium) ## 95 percent confidence interval: ## lower upper ## -1.8992165 0.7609722 So, we actually found a ‘medium’ effect (more than 0.5 standard deviations difference between group means). Maybe sample size was too small to have sufficient power? Let’s check sample size: table(dd.temporal$APE[!is.na(dd.temporal.df$Fst.temp)]) ## ## FALSE TRUE ## 7 5 Ah, that explains a lot. There were only 5 sites with apparent extinction, and 7 without. Given that sample size, what was the statistical power of our test to detect at least a large effect (d = - 0.8), i.e., be able to reject the null hypothesis if such an effect is present in the population from which we sampled? pwr::pwr.t2n.test(n1=7, n2=5, d=-0.8, alternative = &quot;less&quot;) ## ## t test power calculation ## ## n1 = 7 ## n2 = 5 ## d = -0.8 ## sig.level = 0.05 ## power = 0.3552962 ## alternative = less So the power to detect at least a large effect, if it exists in the population, was only 0.355, way below the 0.8 (or even 0.95) that we would want to see. For a medium effect, the power is even smaller. c. Sample size calculation How large a sample would we have needed in each group to achieve a power of 0.8 to detect a large effect? And for a medium effect? pwr::pwr.t.test(power = 0.8, d = -0.8, alternative = &quot;less&quot;) ## ## Two-sample t test power calculation ## ## n = 20.03277 ## d = -0.8 ## sig.level = 0.05 ## power = 0.8 ## alternative = less ## ## NOTE: n is number in *each* group pwr::pwr.t.test(power = 0.8, d = -0.5, alternative = &quot;less&quot;) ## ## Two-sample t test power calculation ## ## n = 50.1508 ## d = -0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = less ## ## NOTE: n is number in *each* group More than 20 sites in each group would have been needed to detect a large effect, or more than 50 per group to detect a medium effect, with a power of 80%. Hence, these particular results are inconclusive. There was a trend showing a large effect size but power was very low. This aspect of the study should ideally be repeated with a larger sample size before reaching any conclusions. Note however that using additional evidence (e.g., population assignment tests), Lamy et al. (2012) concluded that extinctions were in fact less common in this system than previously assumed – in many cases of apparent extinction, individuals may still be present but just not detected. "],["r-exercise-week-4.html", "7.4 R Exercise Week 4", " 7.4 R Exercise Week 4 Task: Build on your previous exercises and plot the sites on a map downloaded from the internet. Explore the relationships between Hexp, census population size and percent forest cover within 500 m of the site (forest may act as a barrier for grassland plants). Hints: Load packages: You may want to load the packages dplyr and tmap. Alternatively, you can use :: to call functions from packages. Import your datasets from Weeks 2 &amp; 3 R Exercises. Here’s an example for your code, adapt it as needed to import the R objects “Pulsatilla.longlat.rds” (sf object, Week 2) and “H.pop.rds” (Week 3) that you saved previously: Pulsatilla.longlat &lt;- readRDS(here::here(\"output/Pulsatilla.longlat.rds\")) Plot sites on map from internet: adapt the code from section 3.d to plot the sampling locations on a background map from the internet. Next, modify code from section 3.d to add labels for all sites. Combine data: Use the function dplyr::left_join to add the variables from the dataset H.pop to Pulsatilla.longlat. Notes: This is important, as the order of populations may not be the same in the two datasets. Remember to check the structure of the datasets (variable names and types) first so that you know which are the ID variables that you can use to match sites. If the two ID variables are not of the same type (e.g., one if a factor, the other is character), it is best to change the format of one (e.g., with as.character) before doing the left-join. Scatterplot with regression line: Create a scatterplot of Hexp (y axis) plotted against nIndiv (x axis). Add a regression line and, if possible, label points. You may modify code from section 3.b or use base R functions. Regression analysis: Adapt code from section 3.c to perform a regression of Hexp (response variable) on the predictor nIndiv. Create residual plots and inspect them. What is the main issue here? Questions: There is one influential point in the regression analysis: Which site is it? Where is it located (north / east / south / west)? What makes it an influential point (large residual, leverage, or both)? What would happen to the regression line and the R-squared if this point was omitted? "],["Week5.html", "8 Lab 5: Spatial Statistics", " 8 Lab 5: Spatial Statistics In this week’s computer lab, we will test for spatial autocorrelation in genetic and landscape data. Along the way, we will learn how to calculate pairwise distances in R, how to define spatial weights, and how to work with two basic R object types: S3 vs. S4 objects. View Course Video Interactive Tutorial 5 Worked Example R Exercise Week 5 Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_5.html", "8.1 View Course Video", " 8.1 View Course Video 1. Embedded Video External link: Week 5 video; Week 5 video (Part 2) Transcript: Download transcript iframe not supported Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_5.html", "8.2 Interactive Tutorial 5", " 8.2 Interactive Tutorial 5 1. List of R commands covered this week Function Package plot(asp = 1) base dist stats as.matrix base lower.tri base diag base dnearneigh spdep data.matrix base nb2listw spdep listw2mat spdep nbdists spdep 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_5.html", "8.3 Worked Example", " 8.3 Worked Example Helene Wagner 1. Overview of Worked Example a. Goals This worked example shows how to: Calculate genetic distance at population level. Perform a Mantel test to test for IBD. Create Mantel correlograms for the genetic data. Calculate and test Moran’s I for population-level genetic diversity data. b. Data set This is a larger version of the frog data set used in Weeks 1 - 3. Here, we will analyze microsatellite data from Funk et al. (2005) and Murphy et al. (2010) for 418 individuals of Colombia spotted frogs (Rana luteiventris) from 30 populations, together with site-level spatial coordinates. Please see the introductory video on the DGS course website, R lab page. ralu_loci_allpops.csv: Data frame with populations and genetic data (406 rows x 9 columns). Included as external file in package LandGenCourse. Frogs_diversity_allpops.csv: Table with genetic diversity variables similar to Week 3. ralu_coords_allpops.csv: Spatial coordinates (both in UTM and LongLat format), and basin. c. Required R packages Install some packages (not on CRAN) that are needed for this worked example. if(!requireNamespace(&quot;EcoGenetics&quot;, quietly = TRUE)) remotes::install_github(&quot;leandroroser/EcoGenetics-devel&quot;) if(!requireNamespace(&quot;GeNetIt&quot;, quietly = TRUE)) remotes::install_github(&quot;jeffreyevans/GeNetIt&quot;) if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) { install.packages(c(&quot;RgoogleMaps&quot;, &quot;geosphere&quot;, &quot;proto&quot;, &quot;sampling&quot;, &quot;seqinr&quot;, &quot;spacetime&quot;, &quot;spdep&quot;), dependencies=TRUE) remotes::install_github(&quot;dyerlab/popgraph&quot;) } if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) library(LandGenCourse) library(GeNetIt) library(dplyr) library(EcoGenetics) #require(adegenet) #require(tibble) #require(gstudio) #require(hierfstat) #require(PopGenReport) #require(mmod) #require(spdep) 2. Data import and manipulation a. Create an ecogen object Import the site data that we used before in Week 2. Frogs.coords &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;ralu_coords_allpops.csv&quot;, package = &quot;LandGenCourse&quot;)) In Week 3, we calculated population-level genetic data as a table Frogs.diversity. Here we import a table with the same diversity measures for all 29 sites from a system file in LandGenCourse. Frogs.diversity &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;Frogs_diversity_allpops.csv&quot;, package = &quot;LandGenCourse&quot;)) Import the genetic data for 29 sites: Frogs.loci &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;ralu_loci_allpops.csv&quot;, package = &quot;LandGenCourse&quot;)) We start building an ecogen object by assigning the genetic data to the G slot, and the structure (i.e., hierarchical sampling information, here site names) to the S slot. For the genetic data, we need to specify the type of data and coding. Note the tweak using data.frame when specifying data.frame(ralu.loci[,1:2]). This is necessary to import the multiple columns correctly and with their original names. Frogs.ecogen &lt;- ecogen(G = Frogs.loci[,-c(1:2)], ploidy = 2, type = &quot;codominant&quot;, sep = &quot;:&quot;, S = data.frame(Frogs.loci[,1:2])) Frogs.ecogen ## ## || ECOGEN CLASS OBJECT || ## ---------------------------------------------------------------------------- ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## ---------------------------------------------------------------------------- ## ## | slot XY: | --&gt; 0 x 0 coordinates ## | slot P: | --&gt; 0 x 0 phenotypic variables ## | slot G: | --&gt; 413 x 8 loci &gt;&gt; ploidy: 2 || codominant ## | slot A: | --&gt; 413 x 43 alleles ## | slot E: | --&gt; 0 x 0 environmental variables ## | slot S: | --&gt; 413 x 2 structures &gt;&gt; 2 structures found ## | slot C: | --&gt; 0 x 0 variables ## | slot OUT: | --&gt; 0 results ## ---------------------------------------------------------------------------- The summary confirms that we now have data in the S and G slots. In addition, EcoGenetics created a table of absolute frequencies (i.e. counts) of alleles for each individual in slot A. b. Aggregate to ‘ecopop’ Most of our analysis for this lab will be at the population level. The function ecogen2ecopop aggregates from individual to population-level data. Frogs.ecopop &lt;- ecogen2ecopop(Frogs.ecogen, hier = &quot;SiteName&quot;) Frogs.ecopop ## ## &lt;&lt; ECOPOP CLASS OBJECT &gt;&gt; ## **************************************************************************** ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## **************************************************************************** ## ## # slot XY: # =&gt; 0 x 0 coordinates ## # slot P: # =&gt; 0 x 0 phenotypic variables ## # slot AF: # =&gt; 30 x 43 loci &gt;&gt; ploidy: 2 || codominant ## # slot E: # =&gt; 0 x 0 environmental variables ## # slot S: # =&gt; 30 x 1 population found ## # slot C: # =&gt; 0 x 0 variables ## **************************************************************************** Instead of 413 individuals, we now have data for 30 populations. Slot AF contains the population-level absolute frequencies (counts) of alleles. b. Add site-level data Before importing the genetic diversity, spatial coordinates and site data into the ecopop object, we need to match the rows and extract the data for the sampled populations. Unfortunately, the datasets use different versions of the site name, both were include in the file ralu_loci_allpops.csv and thus in the S slot of Frogs.ecogen. Here, we use group_by and summarize to get the unique set of site names in both versions. We save it as a data frame Subset. Then we join the genetic diversity data. Because the two data frames share a common variable, we dont’ need to specify the argument by. R will confirm what ID variable it used to join. Subset &lt;- ecoslot.S(Frogs.ecogen) %&gt;% group_by(SiteName, Pop) %&gt;% summarize() ## `summarise()` has grouped output by &#39;SiteName&#39;. You can override using the ## `.groups` argument. Subset &lt;- left_join(Subset, Frogs.diversity) ## Joining with `by = join_by(Pop)` R tells us that it used the shared ID variable Pop to join the data. Now we can join the site data. Note: with as.data.frame, we combine the @data and @coords slots of the SpatialPointsDataFrame ralu.site to a single data frame. Subset &lt;- left_join(Subset, as.data.frame(Frogs.coords)) ## Joining with `by = join_by(SiteName)` This time, R used the shared ID variable SiteName to join the data. Now we have all site-level data in the data frame Subset that has the same row names as Frogs.ecopop. We check the names of the variables to decide which ones to put where. Then we assign them to their respective slots (@C for the custom data, here genetic diversity, @E for the environmental data, and @XY for the coordinates). The argument pop specifies the matching ID variable in the @S slot. When we aggregated from Frogs.ecogen to Frogs.ecopop, unfortunately EcoGenetics renamed the variable SiteName to pop. The argument pop_levels identifies the corresponding ID variable in Subset. names(Subset) ## [1] &quot;SiteName&quot; &quot;Pop&quot; &quot;n&quot; &quot;Hobs&quot; &quot;Hexp&quot; &quot;Ar&quot; ## [7] &quot;UTM83_E&quot; &quot;UTM83_N&quot; &quot;Longitude&quot; &quot;Latitude&quot; &quot;Basin&quot; Frogs.ecopop &lt;- EcoGenetics::eco.fill_ecogen_with_df(Frogs.ecopop, pop =&quot;pop&quot;, pop_levels = Subset$SiteName, C = Subset[,3:6], XY = Subset[,7:10]) Frogs.ecopop ## ## &lt;&lt; ECOPOP CLASS OBJECT &gt;&gt; ## **************************************************************************** ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) ## **************************************************************************** ## ## # slot XY: # =&gt; 30 x 4 coordinates ## # slot P: # =&gt; 0 x 0 phenotypic variables ## # slot AF: # =&gt; 30 x 43 loci &gt;&gt; ploidy: 2 || codominant ## # slot E: # =&gt; 0 x 0 environmental variables ## # slot S: # =&gt; 30 x 1 population found ## # slot C: # =&gt; 30 x 4 variables ## **************************************************************************** c. Export to ‘adegenet’ and ‘gstudio’ ‘EcoGenetics’ provides convenient functions for converting genetic data to and from other packages. Import into ‘genind’ object (package ‘adegenet’): there is a dedicated function, but we need to separately declare the variable that represents the populations and write it into the @pop slot of the ‘genind’ object. Frogs.genind &lt;- EcoGenetics::ecogen2genind(Frogs.ecogen) ## Loading required package: adegenet ## Loading required package: ade4 ## ## /// adegenet 2.1.11 is loaded //////////// ## ## &gt; overview: &#39;?adegenet&#39; ## &gt; tutorials/doc/questions: &#39;adegenetWeb()&#39; ## &gt; bug reports/feature requests: adegenetIssues() Frogs.genind@pop &lt;- ecoslot.S(Frogs.ecogen)$SiteName For calculating population-level genetic distances, we aggregate the individual-level data to a genpop object with population-level allele frequencies. Frogs.genpop &lt;- adegenet::genind2genpop(Frogs.genind) ## ## Converting data from a genind to a genpop object... ## ## ...done. Frogs.genpop ## /// GENPOP OBJECT ///////// ## ## // 30 populations; 8 loci; 43 alleles; size: 18.2 Kb ## ## // Basic content ## @tab: 30 x 43 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 3-9) ## @loc.fac: locus factor for the 43 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: adegenet::genind2genpop(x = Frogs.genind) ## ## // Optional content ## - empty - Note: Alternatively, we could directly import the ecopop object into a genpop object (adegenet) with EcoGenetics::ecopop2genpop(Frogs.ecopop). However, this can create warnings later on when calculating genetic distances. The object ‘Frogs.genpop’ has 30 rows, each representing a population. We will also use some functions from the package gstudio, hence we import the individual-level genetic data into gstudio. This should be easy with the function EcoGenetics::ecogen2gstudio. However, there seems to be an issue. The following chunk contains code adapted from within the ecogen2gstudio function, tweaked to work for our data. (You can view the original function with: findMethods(ecogen2gstudio).) #Frogs.gstudio &lt;- EcoGenetics::ecogen2gstudio(Frogs.ecogen, type=&quot;codominant&quot;) dat &lt;- eco.convert(Frogs.ecogen@G, &quot;matrix&quot;, sep.in = &quot;:&quot;, sep.out = &quot;:&quot;) dat &lt;- as.data.frame(dat, stringsAsFactors = FALSE) for (i in 1:ncol(dat)) { class(dat[, i]) &lt;- &quot;locus&quot; } dat[is.na(dat)] &lt;- gstudio::locus(NA) colnames(dat) &lt;- colnames(Frogs.ecogen@G) Frogs.gstudio &lt;- data.frame(ecoslot.S(Frogs.ecogen), dat) tibble::as_tibble(Frogs.gstudio) ## # A tibble: 413 × 10 ## SiteName Pop A B C D E F G H ## &lt;fct&gt; &lt;fct&gt; &lt;locus&gt; &lt;locus&gt; &lt;locus&gt; &lt;locus&gt; &lt;loc&gt; &lt;loc&gt; &lt;loc&gt; &lt;loc&gt; ## 1 AirplaneLake Airplane 1:1 1:1 1:1 1:1 1:2 1:1 1:1 4:5 ## 2 AirplaneLake Airplane 2:2 1:1 1:1 1:1 2:2 ## 3 AirplaneLake Airplane 1:1 1:1 1:1 1:1 3:3 1:1 1:1 3:3 ## 4 AirplaneLake Airplane 1:1 1:1 2:2 1:2 ## 5 AirplaneLake Airplane 1:2 1:3 1:1 1:1 1:2 1:1 1:2 4:5 ## 6 AirplaneLake Airplane 1:2 1:1 1:1 1:3 1:1 1:1 1:2 4:5 ## 7 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 8 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:3 1:1 1:1 2:3 ## 9 AirplaneLake Airplane 1:3 1:1 1:1 1:1 1:7 1:1 1:1 3:5 ## 10 AirplaneLake Airplane 2:2 1:3 1:1 1:1 3:7 1:1 1:1 3:3 ## # ℹ 403 more rows 3. Calculate genetic distances Here, we’ll calculate a number of different measures of genetic distance, using functions from several packages. Adding the package name to each distance matrix name helps keeping track of methods used. Normally you would not calculate all of them for your own data, and some are redundant, as we will see. Note: Some functions provide an option linearized = TRUE to linearize distances d by calculating d/(1-d). This should result in more linear relationships when plotted or correlated against geographic distance.Here we don’t linearize, we can do so later manually. a. Genetic distances calculated from genind object Pairwise Fst with package hierfstat: GD.pop.PairwiseFst.hierfstat &lt;- as.dist(hierfstat::pairwise.neifst(hierfstat::genind2hierfstat(Frogs.genind))) Proportion of shared alleles with package ‘PopGenReport’: GD.pop.propShared &lt;- PopGenReport::pairwise.propShared(Frogs.genind) ## Registered S3 method overwritten by &#39;pegas&#39;: ## method from ## print.amova ade4 ## Registered S3 methods overwritten by &#39;genetics&#39;: ## method from ## print.locus gstudio ## [.haplotype pegas Several distance matrices with package ‘adegenet’: GD.pop.Nei &lt;- adegenet::dist.genpop(Frogs.genpop, method=1) GD.pop.Edwards &lt;- adegenet::dist.genpop(Frogs.genpop, method=2) GD.pop.Reynolds &lt;- adegenet::dist.genpop(Frogs.genpop, method=3) GD.pop.Rogers &lt;- adegenet::dist.genpop(Frogs.genpop, method=4) GD.pop.Provesti &lt;- adegenet::dist.genpop(Frogs.genpop, method=5) Additional distance matrices with package ‘mmod’: GD.pop.Joost &lt;- mmod::pairwise_D(Frogs.genind, linearized = FALSE) GD.pop.Hedrick &lt;- mmod::pairwise_Gst_Hedrick(Frogs.genind, linearized = FALSE) GD.pop.NeiGst &lt;- mmod::pairwise_Gst_Nei(Frogs.genind, linearized = FALSE) b. More distance matrices with ‘gstudio’ GD.pop.Euclidean.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Euclidean&quot;, stratum=&quot;SiteName&quot;) ## Multilous estimates of Euclidean distance are assumed to be additive. GD.pop.cGD.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;cGD&quot;, stratum=&quot;SiteName&quot;) GD.pop.Nei.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Nei&quot;, stratum=&quot;SiteName&quot;) GD.pop.Dps.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Dps&quot;, stratum=&quot;SiteName&quot;) ## Bray distance will be assumed to be entirely additive across loci. GD.pop.Jaccard.gstudio &lt;-gstudio::genetic_distance(Frogs.gstudio, mode = &quot;Jaccard&quot;, stratum=&quot;SiteName&quot;) ## Jaccard distance will be assumed to be entirely additive across loci. c. Assemble distance matrices We’ll store the population-level genetic distances in a list ‘GD.pop’. Note: a few measures return similarities (scaled between 0 and 1) instead of distances. For instance, ‘proporition of shared alleles’ is 1 if the alleles are identical, and zero of no alleles are shared. We convert these values to distances by subtracting them from 1. GD.pop &lt;- list(pairwiseFst.hierfstat = GD.pop.PairwiseFst.hierfstat, propShared.PopGenReport = 1 - GD.pop.propShared, Nei.adegenet = GD.pop.Nei, Edwards.adegenet = GD.pop.Edwards, Reynolds.adegenet = GD.pop.Reynolds, Rogers.adegenet = GD.pop.Rogers, Provesti.adegenet = GD.pop.Provesti, Joost.mmod = GD.pop.Joost, Hedrick.mmod = GD.pop.Hedrick, Nei.mmod = GD.pop.NeiGst, Euclidean.gstudio = as.dist(GD.pop.Euclidean.gstudio), cGD.gstudio = as.dist(GD.pop.cGD.gstudio), Nei.gstudio = as.dist(GD.pop.Nei.gstudio), Dps.gstudio = as.dist(1 - GD.pop.Dps.gstudio), Jaccard.gstudio = as.dist(1 - GD.pop.Jaccard.gstudio)) round(cor(sapply(GD.pop, function(ls) as.vector(ls))),2)[,1:2] ## pairwiseFst.hierfstat propShared.PopGenReport ## pairwiseFst.hierfstat 1.00 0.79 ## propShared.PopGenReport 0.79 1.00 ## Nei.adegenet 0.85 0.96 ## Edwards.adegenet 0.83 0.93 ## Reynolds.adegenet 0.97 0.82 ## Rogers.adegenet 0.83 0.99 ## Provesti.adegenet 0.79 1.00 ## Joost.mmod 0.90 0.95 ## Hedrick.mmod 0.98 0.88 ## Nei.mmod 1.00 0.77 ## Euclidean.gstudio 0.93 0.95 ## cGD.gstudio 0.50 0.62 ## Nei.gstudio 0.85 0.96 ## Dps.gstudio 0.71 0.90 ## Jaccard.gstudio 0.71 0.90 Consider the correlations printed above (only the first two columns of the correlation matrix are shown). Correlations are high in general, except for conditional genetic distance (cGD) (see Week 13). There are some duplicate measures (with correlation = 1). Note: the following functions calculate distance matrices at the individual level: PopGenReport::gd.smouse() adegenet::propShared() gstudio::genetic_distance(mode = “AMOVA”) d. Export genetic distance matrices Optional: You can use save to save an R object to your file system, and load to read it in again. Note: the default setting is that save will overwrite existing files with the same name. The code is commented out with #. To run it, remove the #. The first part creates a folder output in your project folder if it does not yet exist. The function save writes the list GD.pop into a file “GD.pop.RData”, and the function load imports it again. #require(here) #if(!dir.exists(paste0(here::here(),&quot;/output&quot;))) dir.create(paste0(here::here(),&quot;/output&quot;)) #save(GD.pop, file = paste0(here::here(),&quot;/output/GD.pop.RData&quot;)) #load(paste0(here::here(),&quot;/output/GD.pop.RData&quot;)) 4. Perform a Mantel test to test for IBD First, we calculate geographic (Euclidean) distances Dgeo with the dist function, using the spatial coordinates that we imported from ralu.site. These are UTM coordinates and thus metric. Dgeo &lt;- as.vector(dist(ecoslot.XY(Frogs.ecopop)[,1:2])) a. Visually check linearity Before we quantify the linear relationship between genetic and geographic distances, let’s check visually whether the relationship is indeed linear. To start, we will define genetic distance Dgen based on proportion of shared alleles. par(mar=c(4,4,0,0)) Dgen &lt;- as.vector(GD.pop$propShared.PopGenReport) dens &lt;- MASS::kde2d(Dgeo, Dgen, n=300) myPal &lt;- colorRampPalette(c(&quot;white&quot;,&quot;blue&quot;,&quot;gold&quot;,&quot;orange&quot;,&quot;red&quot;)) plot(Dgeo, Dgen, pch=20, cex=0.5, xlab=&quot;Geographic Distance&quot;, ylab=&quot;Genetic Distance&quot;) image(dens, col=transp(myPal(300), 0.7), add=TRUE) abline(lm(Dgen ~ Dgeo)) lines(loess.smooth(Dgeo, Dgen), col=&quot;red&quot;) There is clearly an increase of genetic distance with geographic distance. However, the red line, which is a smoothed local mean, indicates that the relationship is not linear. Let’s take the natural logarithm of geographic distance: par(mar=c(4,4,0,0)) dens &lt;- MASS::kde2d(log(Dgeo), Dgen, n=300) plot(log(Dgeo), Dgen, pch=20, cex=0.5, xlab=&quot;Geographic Distance&quot;, ylab=&quot;Genetic Distance&quot;) image(dens, col=transp(myPal(300), 0.7), add=TRUE) abline(lm(Dgen ~ log(Dgeo))) lines(loess.smooth(log(Dgeo), Dgen), col=&quot;red&quot;) Questions: Is the relationship between Dgen and Dgeo more linear than before the transformation? What happened to the spread of points along the x axis (geographic distance)? What do the units of the x-axis represent, after the transformation? b. Perform Mantel test Next, we perform a Mantel test with the function mantel from the vegan package. We define Dgen and Dgeo anew, as we need them in ‘dist’ format this time, not as vectors. Dgen &lt;- GD.pop$propShared.PopGenReport Dgeo &lt;- dist(ecoslot.XY(Frogs.ecopop)[,1:2]) IBD &lt;- vegan::mantel(Dgen,Dgeo, method=&quot;pearson&quot;) IBD ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## vegan::mantel(xdis = Dgen, ydis = Dgeo, method = &quot;pearson&quot;) ## ## Mantel statistic r: 0.6211 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0779 0.1085 0.1263 0.1611 ## Permutation: free ## Number of permutations: 999 What happens if we take the log of geographic distance, which we saw above helps linearize the relationship for these data? IBD &lt;- vegan::mantel(Dgen,log(Dgeo), method=&quot;pearson&quot;) IBD ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## vegan::mantel(xdis = Dgen, ydis = log(Dgeo), method = &quot;pearson&quot;) ## ## Mantel statistic r: 0.6839 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0629 0.0902 0.1124 0.1306 ## Permutation: free ## Number of permutations: 999 The statistical significance (p-value) didn’t really change, but the strength of the Mantel correlation increased from r = 0.62 to r = 0.68. Instead of transforming variables, we could use Spearman rank correlation to quantify the strength of the association. Rank correlations can be used to quantify and test the strength of curvilinear relationships. IBD &lt;- vegan::mantel(Dgen,Dgeo, method=&quot;spearman&quot;) IBD ## ## Mantel statistic based on Spearman&#39;s rank correlation rho ## ## Call: ## vegan::mantel(xdis = Dgen, ydis = Dgeo, method = &quot;spearman&quot;) ## ## Mantel statistic r: 0.6512 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0703 0.0979 0.1172 0.1454 ## Permutation: free ## Number of permutations: 999 For this measure of genetic diversity, the Mantel correlation was significant, quite strong, and the relationship not linear, hence the transformed data or the rank correlation performed better. What about the other measures? Here we use lapply to apply the function mantel to each genetic distance matrix in GD.pop. Then we use sapply to extract two values for each distance matrix: statistic is the Mantel r statistic (here: Pearson linear correlation), and signif is the p-value. We can find these names with the function attributes (see above). attributes(IBD) ## $names ## [1] &quot;call&quot; &quot;method&quot; &quot;statistic&quot; &quot;signif&quot; &quot;perm&quot; ## [6] &quot;permutations&quot; &quot;control&quot; ## ## $class ## [1] &quot;mantel&quot; Mantel.test &lt;- lapply(GD.pop, function(x) vegan::mantel(x,Dgeo, method=&quot;pearson&quot;)) data.frame(Mantel.r = sapply(Mantel.test, function(x) x$statistic), p.value = sapply(Mantel.test, function(x) x$signif)) ## Mantel.r p.value ## pairwiseFst.hierfstat 0.6238025 0.001 ## propShared.PopGenReport 0.6211335 0.001 ## Nei.adegenet 0.6316224 0.001 ## Edwards.adegenet 0.6639963 0.001 ## Reynolds.adegenet 0.6162305 0.001 ## Rogers.adegenet 0.6188092 0.001 ## Provesti.adegenet 0.6211335 0.001 ## Joost.mmod 0.6556870 0.001 ## Hedrick.mmod 0.6520614 0.001 ## Nei.mmod 0.6147866 0.001 ## Euclidean.gstudio 0.6591863 0.001 ## cGD.gstudio 0.5667087 0.001 ## Nei.gstudio 0.6317948 0.001 ## Dps.gstudio 0.5371591 0.001 ## Jaccard.gstudio 0.5357096 0.001 The nature of the result did not depend on the measure of genetic diversity used. Let’s repeat the analysis with log(Dgeo). Mantel.test &lt;- lapply(GD.pop, function(x) ade4::mantel.randtest(x,log(Dgeo))) data.frame(Mantel.r = sapply(Mantel.test, function(x) x$obs), p.value = sapply(Mantel.test, function(x) x$pvalue)) ## Mantel.r p.value ## pairwiseFst.hierfstat 0.6218519 0.001 ## propShared.PopGenReport 0.6839092 0.001 ## Nei.adegenet 0.6547320 0.001 ## Edwards.adegenet 0.7122499 0.001 ## Reynolds.adegenet 0.6437788 0.001 ## Rogers.adegenet 0.6718605 0.001 ## Provesti.adegenet 0.6839092 0.001 ## Joost.mmod 0.6748485 0.001 ## Hedrick.mmod 0.6673508 0.001 ## Nei.mmod 0.6034555 0.001 ## Euclidean.gstudio 0.6967457 0.001 ## cGD.gstudio 0.6008983 0.001 ## Nei.gstudio 0.6548413 0.001 ## Dps.gstudio 0.5911350 0.001 ## Jaccard.gstudio 0.5880305 0.001 Questions: - Did the transformation lead to higher Mantel correlations for all measures of genetic distance used here? - Why was it higher? 5. Create Mantel correlogram for genetic data Let’s look at the relationship between genetic distance and geographic distance in a different way, with a Mantel correlogram. Note that this method does not make the assumption of linearity. a. Create a first Mantel correlogram Here, we’ll create a population-level Mantel correlogram with the proportion of shared alleles. Note: The function eco.cormantel has an option latlon=TRUE that takes care of the distance calculation from lat-lon coordinates. To use this option, the coordinates must be in a matrix or data frame with the longitude in the first column and the latitude in the second. Here, we can set we can set latlon=FALSE because the spatial coordinates are in UTM projection. The biological hypothesis of isolation-by-distance postulates that genetic distance increases with geographic distance. Hence it makes sense to use a one-sided alternative. Somewhat counter-intutitively, we use ‘alternative=“less”’ to test for positive spatial autocorrelation. corm &lt;- EcoGenetics::eco.cormantel(M = GD.pop$propShared.PopGenReport, XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, alternative=&quot;less&quot;, method = &quot;pearson&quot;) ## interval 0 / 10 completed interval 1 / 10 completed interval 2 / 10 completed interval 3 / 10 completed interval 4 / 10 completed interval 5 / 10 completed interval 6 / 10 completed interval 7 / 10 completed interval 8 / 10 completed interval 9 / 10 completed interval 10 / 10 completed corm ## ## ############################ ## Mantel statistic ## ############################ ## ## &gt; Correlation coefficient used --&gt; Pearson ## &gt; Number of simulations --&gt; 199 ## &gt; Random test --&gt; permutation ## &gt; P-adjust method --&gt; holm -sequential: TRUE ## ## &gt; ecoslot.OUT(x): results --&gt; ## ## [[1]] ## d.mean obs exp p.val cardinal ## d= 0 - 1055.243 697.776 -0.4815 0.0020 0.005 45 ## d= 1055.243 - 2110.485 1583.490 -0.2708 -0.0026 0.010 67 ## d= 2110.485 - 3165.728 2622.371 -0.2043 -0.0047 0.015 64 ## d= 3165.728 - 4220.97 3710.591 0.1302 -0.0032 0.990 64 ## d= 4220.97 - 5276.213 4716.029 0.1719 0.0008 1.000 54 ## d= 5276.213 - 6331.455 5824.450 0.2021 -0.0071 1.000 46 ## d= 6331.455 - 7386.698 6856.457 0.1496 -0.0038 1.000 27 ## d= 7386.698 - 8441.941 7883.679 0.2297 -0.0014 1.000 33 ## d= 8441.941 - 9497.183 8882.536 0.1909 -0.0031 1.000 25 ## d= 9497.183 - 10552.426 9847.000 0.0807 0.0025 1.000 9 ## ## ## Results table(s) in ecoslot.OUT(x) ## -------------------------------------------------------------------------- ## Access to slots: &lt;ecoslot.&gt; + &lt;name of the slot&gt; + &lt;(name of the object)&gt; ## See: help(&quot;EcoGenetics accessors&quot;) The table shows: Breaks of distance lag ‘d’: here in meters (default for lag definition: Sturges rule) Mean distance ‘d.mean’: mean distance of pairs in each lag. obs: observed value of the statistic Expected value ‘exp’: expected if there is no autocorrelation. P-value ‘p.val’: default uses a two-sided permutation test with sequential Holm-Bonferroni adjustement of p-values. Number of pairs ‘cardinal’: number of unique pairs per lag. The result corm is an object of class eco.correlog (package: EcoGenetics). A safe way to access thet table is ecoslot.OUT(corm). Let’s plot the correlogram: EcoGenetics::eco.plotCorrelog(corm) You can hover over individual points of the correlogram to see the test statistic and mean distance. b. Vary distance class definition Under IBD, at least the first distance lag should show positive spatial autocorrelation. Here, it is the first two classes, as indicated by the red symbols. To what degree does this result depend on the following: The distance lag definition? The measure of genetic distance? Non-linear relationship between genetic and geographic distances? There are several options of the eco.cormantel function to modify the definition of distance classes: int: distance interval in the units of XY smin: minimum class distance in the units of XY smax: maximum class distance in the units of XY nclass: number of classes seqvec: vector with breaks in the unites of XY size: number of individuals per class bin: rule for constructing intervals if no other parameters provided (default: Sturge’s rule) The easiest ones to modify are either nclass or size. Here we use size to specify that there should be at least 50 or 100 pairs in each distance class. (Note: for a reliable analysis, this should be 100 or more). corm.50 &lt;- EcoGenetics::eco.cormantel(M = GD.pop$propShared.PopGenReport, XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, alternative=&quot;less&quot;, size=50) ## interval 0 / 8 completed interval 1 / 8 completed interval 2 / 8 completed interval 3 / 8 completed interval 4 / 8 completed interval 5 / 8 completed interval 6 / 8 completed interval 7 / 8 completed interval 8 / 8 completed EcoGenetics::ecoslot.OUT(corm.50) ## [[1]] ## d.mean obs exp p.val cardinal ## d= 0 - 1152.337 739.966 -0.4872 0.0012 0.005 50 ## d= 1152.337 - 1821.843 1536.052 -0.2256 -0.0036 0.010 50 ## d= 1821.843 - 2769.341 2286.134 -0.2307 0.0043 0.015 50 ## d= 2769.341 - 3526.318 3169.188 -0.0411 0.0060 0.195 50 ## d= 3526.318 - 4408.262 3992.072 0.1714 -0.0036 1.000 50 ## d= 4408.262 - 5339.102 4863.176 0.1579 0.0075 1.000 50 ## d= 5339.102 - 6668.986 6011.873 0.2404 -0.0036 1.000 50 ## d= 6668.986 - 8434.883 7615.126 0.2429 0.0028 1.000 50 corm.100 &lt;- EcoGenetics::eco.cormantel(M = GD.pop$propShared.PopGenReport, XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, alternative=&quot;less&quot;, size=100) ## interval 0 / 4 completed interval 1 / 4 completed interval 2 / 4 completed interval 3 / 4 completed interval 4 / 4 completed EcoGenetics::ecoslot.OUT(corm.100) ## [[1]] ## d.mean obs exp p.val cardinal ## d= 0 - 1821.843 1138.009 -0.5403 0.0096 0.005 100 ## d= 1821.843 - 3526.318 2727.661 -0.2060 0.0015 0.010 100 ## d= 3526.318 - 5339.102 4427.624 0.2496 -0.0100 1.000 100 ## d= 5339.102 - 8434.883 6813.499 0.3664 -0.0038 1.000 100 EcoGenetics::eco.plotCorrelog(corm.50) EcoGenetics::eco.plotCorrelog(corm.100) Let’s compare the observed Mantel r statistic, p-value, number of pairs in the first distance class and their mean distance, as well as the definition of the first lag interval. We can get all of this by extracting the first line from each object. The lag intervals are stored only in the row names, and we need to extract those separately and add them as a colum. Lag1.def &lt;- data.frame(rbind(Sturge = EcoGenetics::ecoslot.OUT(corm)[[1]][1,], size.50 = EcoGenetics::ecoslot.OUT(corm.50)[[1]][1,], size.100 = EcoGenetics::ecoslot.OUT(corm.100)[[1]][1,])) Lag1.def$bin &lt;- c(row.names(EcoGenetics::ecoslot.OUT(corm)[[1]])[1], row.names(EcoGenetics::ecoslot.OUT(corm.50)[[1]])[1], row.names(EcoGenetics::ecoslot.OUT(corm.100)[[1]])[1]) Lag1.def ## d.mean obs exp p.val cardinal bin ## Sturge 697.776 -0.4815 0.0020 0.005 45 d= 0 - 1055.243 ## size.50 739.966 -0.4872 0.0012 0.005 50 d= 0 - 1152.337 ## size.100 1138.009 -0.5403 0.0096 0.005 100 d= 0 - 1821.843 All three distance class definitions (Sturge’s rule, size = 50, size = 100) resulted in statistically significant p-values. The Mantel correlation in the first distance lag was strongest for size.100. For the first distance class, Sturge’s rule resulted in 45 pairs and a mean distance similar to size.50 (for this specific dataset). Overall, Sturge’s rule to define distance classes seems to be a good compromise. What is the trade-off, i.e., what happens if distance lags are defined too narrowly or too widely? It can be really helpful to plot the distribution of distances among the pairs and compare it to the distance intervals: par(mfrow=c(3,1)) hist(Dgeo, nclass=50, main=&quot;Sturge&#39;s rule&quot;, axes=F, xlab=&quot;&quot;, ylab=&quot;&quot;) for(i in 1:length(EcoGenetics::ecoslot.BREAKS(corm))){ lines(rep(EcoGenetics::ecoslot.BREAKS(corm)[i], 2), c(0,50), col=&quot;blue&quot;)} hist(Dgeo, nclass=50, main = &quot;50 pairs per lag&quot;, axes=F) for(i in 1:length(EcoGenetics::ecoslot.BREAKS(corm.50))){ lines(rep(EcoGenetics::ecoslot.BREAKS(corm.50)[i], 2), c(0,50), col=&quot;blue&quot;)} hist(Dgeo, nclass=50, main = &quot;100 pairs per lag&quot;, axes=F) for(i in 1:length(EcoGenetics::ecoslot.BREAKS(corm.100))){ lines(rep(EcoGenetics::ecoslot.BREAKS(corm.100)[i], 2), c(0,50), col=&quot;blue&quot;)} Question: Compare what happens at larger distances. Do you think Sturge’s rule does a good job for these as well? Unlike a Mantel test, where all pairs are considered, in geostatistics we typically interpret only values for distances up to a certain threshold, e.g. half the maximum distance, for two reasons: There are few pairs in of these bins, making estimates highly variable. Not all pairs contribute (equally) to the largest distance classes (those in the center of the study area are not involved in very large distances). c. Alternative measures of genetic distances Which measure of genetic distance would provide the strongest Mantel correlation in the first distance class for this data set? Here we will cycle through all genetic distance matrices in GD.pop and calculate a Mantel correlogram with Sturge’s rule (not linearized, method=“pearson”). This may take a while. Note: the code that calculates corm.GD.pop is included here twice, first commented out, then with the option include=FALSE. This avoids printing out a lot of unnecessary output while still showing the (commented out) code in the .html version of the file. #corm.GD.pop &lt;- lapply(GD.pop, function(x) EcoGenetics::eco.cormantel(M = x, # XY = ecoslot.XY(Frogs.ecopop)[,1:2], nsim = 199, latlon=FALSE, # alternative=&quot;less&quot;)) Next, we extract for each genetic distance matrix the observed value of the Mantel correlation for the first distance class and its p-value. t(sapply(corm.GD.pop, function(x) EcoGenetics::ecoslot.OUT(x)[[1]][1,c(2,4)])) ## obs p.val ## pairwiseFst.hierfstat -0.3908 0.005 ## propShared.PopGenReport -0.4815 0.005 ## Nei.adegenet -0.4175 0.005 ## Edwards.adegenet -0.4782 0.005 ## Reynolds.adegenet -0.4337 0.005 ## Rogers.adegenet -0.4704 0.005 ## Provesti.adegenet -0.4815 0.005 ## Joost.mmod -0.4331 0.005 ## Hedrick.mmod -0.4331 0.005 ## Nei.mmod -0.3698 0.005 ## Euclidean.gstudio -0.4721 0.005 ## cGD.gstudio -0.3647 0.005 ## Nei.gstudio -0.4175 0.005 ## Dps.gstudio -0.4074 0.005 ## Jaccard.gstudio -0.4031 0.005 Compare the p-values: for this dataset, all genetic distance measures resulted in significant spatial autocorrelation (indicating IBD). Let’s plot the Mantel correlogram for Nei.adegenet. Statistically significant lags are shown in a different color than non-significant ones. EcoGenetics::eco.plotCorrelog(corm.GD.pop$Nei.adegenet) Questions: Starting with the first distance class, how many consecutive distance classes showed significant spatial autocorrelation? At what distance (range), roughly, did the positive autocorrelation disappear? 6. Specify spatial weights and calculate Moran’s I In this part, we’ll quantify and test Moran’s I for the genetic diversity data as calculated in Week 3 lab. Note: Above, we used a distance lag approach from geostatistics, here we use spatial neighbours and weights (neighbor topology). Either approach could be used with either type of data. For a detailed tutorial on defining spatial neighbors and weights, see: https://cran.r-project.org/web/packages/adespatial/vignettes/tutorial.html#irregular-samplings a. Defining spatial neighbors The function chooseCN (package: adegenet) provides an interface for choosing a connection network, i.e., for defining spatial neighbors. The underlying functions are defined in package spdep (for defining spatial dependences). It can return the following graph types: Delaunay triangulation (type 1) Gabriel graph (type 2) Relative neighbours (type 3) Minimum spanning tree (type 4) Neighbourhood by distance (type 5) K nearests neighbours (type 6) Inverse distances (type 7) Here we use types 1 - 6 to define neighbors in different ways. Then we plot each graph in geographic space. Lines indicate pairs of sites classified as neighbors. Note: this function expects metric spatial coordinates (e.g., UTM). nb.del &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 1) ## Registered S3 method overwritten by &#39;spdep&#39;: ## method from ## plot.mst ape nb.gab &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 2) nb.rel &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 3) nb.mst &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 4) nb.nbd &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 5, d1=100, d2=15000) nb.4nn &lt;- adegenet::chooseCN(xy = ecoslot.XY(Frogs.ecopop)[,1:2], result.type = &quot;nb&quot;, plot.nb = FALSE, type = 6, k = 4) par(mfrow=c(2,3), mai=c(0.1,0.1,0.1, 0.1)) plot(nb.del, coords=ecoslot.XY(Frogs.ecopop)); title(main=&quot;Delaunay&quot;) plot(nb.gab, coords=ecoslot.XY(Frogs.ecopop)); title(main=&quot;Gabriel&quot;) plot(nb.rel, coords=ecoslot.XY(Frogs.ecopop)); title(main= &quot;Rel. neighbors&quot;) plot(nb.mst, coords=ecoslot.XY(Frogs.ecopop)); title(main= &quot;Min spanning tree&quot;) plot(nb.nbd, coords=ecoslot.XY(Frogs.ecopop)); title(main = &quot;Neighbor distance&quot;) plot(nb.4nn, coords=ecoslot.XY(Frogs.ecopop)); title(main = &quot;4 nearest neighbors&quot;) par(mfrow=c(1,1)) For spatial statistics, spatial neighbors are used to calculate a local mean. We want each site to have multiple neighbors, but they should be nearby only. Gabriel graph (type = 2) is often a good option, and we’ll use it for the rest of this worked example. b. Defining spatial weights By default, chooseCN returns row-standardized weights, so that for each site, the weights of its neighbors sum to 1. This means that a local mean can be calculated as a weighted mean of the other sites (non-neighboring sites have a weight of 0). With the function nb2mat we can convert the neighbor object to a matrix of spatial weights. Let’s look at the first five lines and columns: spdep::nb2mat(nb.gab)[1:5,1:5] ## 1 2 3 4 5 ## 1 0 0.0000000 0.0000000 0.0000000 0 ## 2 0 0.0000000 0.1666667 0.1666667 0 ## 3 0 0.2500000 0.0000000 0.0000000 0 ## 4 0 0.3333333 0.0000000 0.0000000 0 ## 5 0 0.0000000 0.0000000 0.0000000 0 Each row contains the weights for the neighbors of one site. We see that the third site is a neighbor of the second site and vice versa. However, the weights are not the same. It seems that site 2 has six neighbors, so each has a weight of 0.167, whereas site 3 has four neighbors, each with a weight of 0.25. c. Calculating and testing Moran’s I Let’s see whether rarefied allelic richness, Ar, shows spatial autocorrelation among these sites. spdep::moran.test(ecoslot.C(Frogs.ecopop)$Ar, spdep::nb2listw(nb.gab), alternative=&quot;greater&quot;) ## ## Moran I test under randomisation ## ## data: ecoslot.C(Frogs.ecopop)$Ar ## weights: spdep::nb2listw(nb.gab) ## ## Moran I statistic standard deviate = 4.648, p-value = 1.676e-06 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.60990344 -0.03448276 0.01921995 The test statistic is 0.61, and the p-value for a one-sided alternative “greater” (i.e., positive spatial autocorrelation) is p &lt; 0.0001. Thus, Ar showed strong and statistially significant spatial autocorrelation. Let’s do this for all genetic diversity variables and extract the value of the Moran I statistics (for first neighbors) and its p-value. Frogs.moran &lt;- lapply(ecoslot.C(Frogs.ecopop), function(x) spdep::moran.test(x, spdep::nb2listw(nb.gab), alternative=&quot;two.sided&quot;)) round(data.frame(obs = sapply(Frogs.moran, function(x) as.vector(x$estimate[1])), p.value = sapply(Frogs.moran, function(x) x$p.value)),3) ## obs p.value ## n -0.109 0.606 ## Hobs 0.507 0.000 ## Hexp 0.551 0.000 ## Ar 0.610 0.000 Questions: Which measure of genetic diversity showed the strongest autocorrelation? Sample size n had a negative value of Moran’s I (obs). What does this mean? Did all four variables show statistically significant spatial autocorrelation? "],["r-exercise-week-5.html", "8.4 R Exercise Week 5", " 8.4 R Exercise Week 5 Task: Assess fine-scale spatial genetic structure in Pulsatilla vulgaris within a single patch, A25: Test for IBD with Mantel rank correlation, and use a Mantel correlogram to assess the range of spatial autocorrelation. Hints: Load packages: You may want to load the packages dplyr, EcoGenetics and adegenet. Alternatively, you can use :: to call functions from packages. Import data, extract adults from A25. Use the code below to import the data into gstudio. Inspect a few rows of the data. Filter the dataset to retain adults (OffID == 0) from site A25 (Population == “A25”). Pulsatilla.gstudio &lt;- gstudio::read_population(path=system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;), type=&quot;column&quot;, locus.columns=c(6:19), phased=FALSE, sep=&quot;,&quot;, header=TRUE) Plot locations of individuals: use plot with the argument asp = 1 to plot the locations of the sampled individuals from site A25 to scale. Convert to ecogen and genind: use EcoGenetics::gstudio2ecogen to convert to an ecogen object. From there, use EcoGenetics::ecogen2genind to convert to a genind object. Calculate genetic and Euclidean distance: Use adegenet::propShared to calculate the proportion of shared alleles at the individual level. (Do not convert to genpop). Convert to a distance measure by calculating 1 - propShared. Check object class. If it is not dist, use as.dist to convert to a dist object. Use dist to calculate Euclidean distance between individuals. Mantel test: Adapt code from section 4.a to plot pairwise genetic distance against Euclidean distance. Do you notice something unusial in the plot? Why are there so few different values of genetic distance? Do you think there is spatial autocorrelation? If so, up to what distance? Adapt code from section 4.b to test for IBD with a Mantel test, using Spearman rank correlation. Mantel correlogram: adapt code from section 5.a to create and plot a Mantel correlogram. Interpret the results. Questions: What is the range of spatial autocorrelation in P. vulgaris in site A25? Based on a plot of genetic distance against Euclidean distance? Based on where the Mantel correlogram reaches 0? Based on statistical significance tests for the Mantel correlogram (with default settings: one-sided alternative “less”, Holm’s adjustment)? "],["Week6.html", "9 Lab 6: Quantitative Genetics", " 9 Lab 6: Quantitative Genetics In this week’s computer lab, we will compare phenotypic and genotypic variation from a common garden experiment and test for adaptation. Along the way, we will learn how to specify, run and interpret linear mixed models. View Course Video Interactive Tutorial 6 Worked Example R Exercise Week 6 Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_6.html", "9.1 View Course Video", " 9.1 View Course Video 1. Embedded Video External link: Week 6 video (Part 1); Week 6 video (Part 2) Transcript: Download transcript Video, Part 1 iframe not supported Video, Part 2 iframe not supported Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_6.html", "9.2 Interactive Tutorial 6", " 9.2 Interactive Tutorial 6 1. List of R commands covered this week Function Package str utils lmer lme4 geom_boxplot ggplot2 facet_wrap ggplot2 geom_line ggplot2 stat_summary ggplot2 geom_errorbar ggplot2 theme_bw ggplot2 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_6.html", "9.3 Worked Example", " 9.3 Worked Example Andrew Eckert, Katalin Csillery and Helene Wagner 1. Overview of Worked Example a. Goals Justification: Natural selection acts on the heritable component of the total phenotypic variation. Traditionally, the heritable proportion of trait variation is estimated using information on the degree of relatedness between individuals. This approach (degree of relatedness) - extensively employed in breeding and evolutionary quantitative genetics – ignores the molecular underpinnings of traits. An alternative approach is scanning genomes using molecular markers to search for so-called outlier loci (see week 11), where selection invoked a change in allele frequencies that is detectable as unusual differentiation between populations in different environments. This approach (outlier loci) – advocated in population genetics - neglects information about the trait variation, but assumes that the markers are in or close to genes that code for adaptive traits. Genetic outliers often hard to interpret because generally we lack information what phenotype they affect and if this phenotype results in fitness differences between populations. Learning Objectives: This lab was constructed to give you experience in working with basic quantitative and population genetic analyses to test hypotheses about the presence of local adaptation. Similar to the genomic revolution, phenotyping is also undergoing a revolution thanks to developments e.g. in imagery that allow for a highly automated recording of traits; a task that has been extremely time consuming in the past. These developments will certainly advance the quantitative genetic approach in the future in evolutionary biology let alone breeding. By the end of the laboratory, you should be able to do the following: Construct, fit, and assess linear mixed effects models (LMMs) to estimate genetic values and variance components of a phenotypic trait measured in families, e.g. in a common garden experiments. Use LMMs to estimate the heritability of a trait. Test for the presence of unusual trait differentiation among populations relative to differentiation expected in the absence of adaptation Assess if trait differentiation is correlated with differentiation in environmental variables to try identifying the selective component of the environment. b. Data set The data come from a study of western white pine (Pinus monticola Dougl. ex D. Don) sampled around the Lake Tahoe Basin of California and Nevada. These data consist of 157 trees sampled from 10 populations (n = 9 to 22 trees/population). Within each population, trees were sampled within three plots. For each plot, GPS coordinates were collected (i.e. each plot in each population has its own GPS coordinates) and used to generate a set of 7 environmental variables. From these trees, needle tissue was collected from which total DNA was extracted and genotyped for 164 single nucleotide polymorphisms (SNPs). Seeds were also collected and planted in a common garden. The seedlings (n = 5 to 35/tree) were measured for several phenotypic traits. The phenotypic trait we will be working with today is known as the carbon isotope ratio (\\(δ^{13}C\\)). It is the ratio of two isotopes of carbon (\\(^{13}C\\) and \\(^{12}C\\)) relative to an experimental control, and it is strongly correlated with intrinsic water-use efficiency in plants. Plants need water to live, so it is not a stretch of the imagination to believe that this phenotypic trait has a link with plant fitness. We will thus have the following data: WWP_phenotype_data.txt: Phenotypic measurements for 5 seedlings per tree made in a common garden. WWP.ecogen: an ecogen object (package EcoGenetics) with SNP genotypes for all trees sampled in the field, and with environmental data collected from each plot within each population. c. Required R packages Install some packages needed for this worked example. if(!requireNamespace(&quot;EcoGenetics&quot;, quietly = TRUE)) remotes::install_github(&quot;leandroroser/EcoGenetics-devel&quot;) if(!requireNamespace(&quot;QstFstComp&quot;, quietly = TRUE)) remotes::install_github(&quot;kjgilbert/QstFstComp&quot;) library(LandGenCourse) library(lme4) #require(EcoGenetics) library(methods) library(tibble) library(ggplot2) #require(lattice) #require(MuMIn) #require(predictmeans) #require(nlme) #require(QstFstComp) #require(car) Package ggeffects not automatically installed with LandGenCourse: if(!require(ggeffects)) install.packages(&quot;ggeffects&quot;, repos=&#39;http://cran.us.r-project.org&#39;) Source two files with additional functions: source(system.file(&quot;extdata&quot;, &quot;supplemental_R_functions.R&quot;, package = &quot;LandGenCourse&quot;)) source(system.file(&quot;extdata&quot;, &quot;panel.cor.r&quot;, package = &quot;LandGenCourse&quot;)) 2. Estimate genetic and non-genetic variance components from a common garden experiment Motivation: A lot of genetics can be carried out without use of any molecular markers. Practitioners of empirical population genetics forget this quite often. A common garden allows us to create a standardized environment in which we minimize the influence of environment on the expression of a particular phenotypic trait. Phenotypic variation can be partitioned to genetic , environmental and residual components based on quantitative genetic theory. Further, we may also test for the presence of interaction between the genetic and environmental variation. The rationale of a common garden is to “standardize” the environment, thus phenotypic variation we observe is mainly due to genetic variation, even though environmental heterogeneity can never be completely eliminated and we still have to additionally control for it, e.g. using a block design (see below). Goals &amp; Background: The goal for this part of the laboratory is to construct, fit, and assess LMMs for \\(δ^{13}C\\). We will be using the data in the file named “WWP_phenotypic_data.txt”. These data are organized in a tab-delimited text file with seedlings grouped by maternal tree (i.e. its mother tree), plot, and population. Also included is an experimental treatment known as “block”. In a common garden, seedlings from the same maternal tree are randomized among blocks to avoid the influence of micro-environmental variation on expression of phenotypic traits. a. Import phenytypic data phen &lt;- read.delim(system.file(&quot;extdata&quot;, &quot;WWP_phenotype_data.txt&quot;, package = &quot;LandGenCourse&quot;), sep = &quot;\\t&quot;, header = T) tibble::as_tibble(phen) ## # A tibble: 779 × 5 ## population plot family block d13c ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 blk cyn BC1 59 5 -30.2 ## 2 blk cyn BC1 59 2 -29.7 ## 3 blk cyn BC1 59 4 -29.6 ## 4 blk cyn BC1 59 3 -29.2 ## 5 blk cyn BC1 59 1 -29.0 ## 6 blk cyn BC1 60 4 -31.2 ## 7 blk cyn BC1 60 3 -30.7 ## 8 blk cyn BC1 60 1 -30.5 ## 9 blk cyn BC1 60 5 -30.1 ## 10 blk cyn BC1 60 2 -29.3 ## # ℹ 769 more rows Check the variable types: family and block have been imported as type “integer”, and we need to convert them to “factor” first. phen$family &lt;- as.factor(phen$family) phen$block &lt;- as.factor(phen$block) sapply(phen, class) ## population plot family block d13c ## &quot;character&quot; &quot;character&quot; &quot;factor&quot; &quot;factor&quot; &quot;numeric&quot; b. Fit linear mixed effects models (LMM) to trait data observed on families Now,we are ready to fit a series of linear models. Here we will fit four models in total, though the last two are equivalent for our data: mod1: a model with only fixed effects (intercept and block), mod2: a LMM with the fixed effects and a random effect due to family, and mod3: a LMM where the random effect due to family is nested within population. We will thus be ignoring the variable plot. mod4: a LMM where the nesting of family within population is not explicitly specified, it is implied by the data. This model makes sense only if each family has a unique ID, so that there are no families from other populations that have the same ID. Disclaimer: mod1 is not a valid model because it ignores the pseudo-replication that arises from the hierarchical sampling design. The goal of using random effects (for family, or family and plot), in the other models is to account for this. Note: d13c ~ 1 + block is equivalent to d13c ~ block (see video, Part 1). mod1 &lt;- lm(d13c ~ block, data = phen) mod2 &lt;- lme4::lmer(d13c ~ 1 + (1|family) + block,data = phen, REML = TRUE) mod3 &lt;- lme4::lmer(d13c ~ 1 + (1|population/family) + block, data = phen, REML = TRUE) mod4 &lt;- lme4::lmer(d13c ~ 1 + (1|population) + (1|family) + block, data = phen, REML = TRUE) Models mod3 and mod4 produce identical results, as long as each family has its own unique ID. In the following, we will use mod4 as it simplifies interpretation (as commonly used in quantitative genetics). Notice that we are using REML=TRUE, which stands for restricted maximum likelihood. This method is advised for quantitative genetic analysis, because it provides correct (statistically unbiased) estimates of the variance components. The other approach would be to set REML=FALSE, in which case the method used is ML or maximum likelihood. However, ML generally underestimates the residual variance, thus leading to inflated estimates of the family variance and thus the heritability. ML would be needed e.g. to test fixed effects. c. Compare model fit Now, let’s explore which model best fits our data. To do this we will use the Akaike Information Criterion (AIC). This statistic scores model fit while giving a penalty for the number of parameters in a model. The model with the lowest score is the preferred model. We will learn more about model selection with AIC (and related measures like AICc, BIC) in Week 12. How do we ensure that the AIC values of the different models are comparable? For mod1 that was fitted with function lm, we use the function AIC. For the other models that were fitted with lmer, we use extractAIC. For model comparison purposes, we have to use the ML fit, as AIC is not valid for REML (see video). The function extractAIC refits the models with ‘REML=FALSE’ to obtain AIC values that are comparable between models with different fixed effects (though this does not apply here because the fixed effects were the same), or between models fitted with functions lm and lmer. It returns two values, the first is the “equivalent degrees of freedom” of the fitted model, and the second is the “AIC” value. Here we only extract the second value. Note: the function extractAIC does not allow to calculate AICc, which is the AIC with small sample correction (see Week 12). aic_vals &lt;- c(AIC(mod1), extractAIC(mod2)[2], extractAIC(mod3)[2], extractAIC(mod4)[2]) names(aic_vals) &lt;- c(&quot;mod1&quot;,&quot;mod2&quot;,&quot;mod3&quot;, &quot;mod4&quot;) aic_vals ## mod1 mod2 mod3 mod4 ## 2120.987 2080.266 2051.207 2051.207 Hence, model mod3 (and its equivalent mod4) appears to be the best model. This suggests that there are important differences among populations, and among trees within populations. We will learn more about model selection later in the course (Week 12). d. Check model validity Is the model mod4 valid? Let’s check the residuals (see video!). The function residplot from package predictmeans produces four diagnostic plots for a model fitted with lmer. The option level indicates which random factor should be plotted (here: 1 = family, 2 = population). This function uses the “conditional” residuals, which represent observed - fitted(fixed) - fitted(random). The option newwd=FALSE specifies that no new window should be created for the plot - this setting is necessary for the knitted R notebook to actually show the figure. predictmeans::residplot(mod4, group=&quot;population&quot;, level=1, newwd=FALSE) Hint: You may need to use the arrow symbols to scroll through the plots in the ‘Plot’ tab in RStudio. The plots are: A normal probability plot of the random effect family: points should follow line. A normal probability plot of (conditional) residuals: points should follow line. A scatterplot of the (conditional) residuals against fitted values: the plot should not “thicken” (which would indicate differences in variances between groups such as blocks = heteroscedasticity). Due to the option group = \"population\", residuals are colored by population. A plot of fitted values against observed: the scatterplot shows the overall model fit, where points would fall on a line if the model explained 100% of the variation in the response (you can ignore the dashed line). In addition, we may want to look for outliers and influential points. For this, we create two additional plots. Here we use marginal residuals, which are calculated without accounting for random effects: observed - fitted(fixed). A plot of marginal residuals against the fixed effect block: there should not be any large outliers. A plot of Cook’s distances, where D &gt; 1 indicates influential points (more relevant when using quantitative predictors (covariates). Note: this may take a while to calculate. First we calculate the marginal residuals by predicting values with fixed effects only (re.form=NA) and subtract these fitted values from the observed values. marginal.residuals &lt;- mod4@frame$d13c - predict(mod4, re.form=NA) plot(mod4@frame$block, marginal.residuals) #predictmeans::CookD(mod4) plot(cooks.distance(mod4)) The residual plots did not indicate any major problem. Hence we can proceed with interpreting the results. Note: As an alternative, the R package DHARMa uses a simulation-based approach to provide residual diagnostics for hierachical (multi-level / mixed) regression models: https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/ e. Estimate variance components How much of the variation in \\(d^{13}C\\) is explained by the fixed and random effects? In models fitted with the function lm, we use the \\(R^2\\) statistics to describe the total variance in the response explained by all the predictors in the model. In a mixed effects modeling context, there is no longer an obvious definition for \\(R^2\\). Two approximate \\(R^2\\) statistics have been implemented in the function r.squaredGLMM from the package MuMIn (which stands for multi-model inference): R2m: marginal \\(R^2\\). This is the variance explained by fixed factors. R2c: conditional \\(R^2\\). This is interpreted as the variance explained by both fixed and random factors, i.e., the entire model. In both statistics, the denominator is a sum of the fixed-effects variance, the random effect variance, and the residual variance. MuMIn::r.squaredGLMM(mod4) ## Registered S3 methods overwritten by &#39;MuMIn&#39;: ## method from ## nobs.multinom broom ## nobs.fitdistr broom ## R2m R2c ## [1,] 0.02466321 0.2297893 The fixed effect block had a small effect of about 2.5%. The total model explained about 23%, most of which was due to the random effects. How important are the random factors population and family? The summary for mod4 lists the variance components under “Random effects”. summary(mod4) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: d13c ~ 1 + (1 | population) + (1 | family) + block ## Data: phen ## ## REML criterion at convergence: 2050.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5436 -0.7485 0.0151 0.6028 3.7867 ## ## Random effects: ## Groups Name Variance Std.Dev. ## family (Intercept) 0.08164 0.2857 ## population (Intercept) 0.10859 0.3295 ## Residual 0.71429 0.8452 ## Number of obs: 779, groups: family, 157; population, 10 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -30.62635 0.12666 -241.792 ## block2 -0.13833 0.09667 -1.431 ## block3 -0.35071 0.09520 -3.684 ## block4 -0.10060 0.09538 -1.055 ## block5 -0.39443 0.09651 -4.087 ## ## Correlation of Fixed Effects: ## (Intr) block2 block3 block4 ## block2 -0.374 ## block3 -0.379 0.498 ## block4 -0.378 0.497 0.505 ## block5 -0.374 0.492 0.499 0.496 Here we extract these variance components from the summary and divide by their sum. fam.var &lt;- nlme::VarCorr(mod4)$&quot;family&quot;[1] pop.var &lt;- nlme::VarCorr(mod4)$&quot;population&quot;[1] res.var &lt;- summary(mod4)$sigma^2 Components &lt;- data.frame(fam.var, pop.var, res.var) Components / sum(Components) ## fam.var pop.var res.var ## 1 0.09026004 0.1200531 0.7896869 Compared to Cohen’s (1988) effect sizes, population and family each had a medium-size effect (&gt; 9% variance explained) on \\(d^{13}C\\) values. f. Significance testing In quantitative genetics, we are more interested in estimating variance components and the size of effects than in hypothesis testing. In other contexts, however, linear mixed models are often used to test fixed effects while accounting for the random effects. Let’s see how this works. The confusing part is that we need to fit the model differently to test fixed and random effects (see video). For testing the random effects, we can use model mod4 that was fitted with REML=TRUE. The simplest way to test the significance of a random effect is to calculate the model with and without it and use anova() to test whether the more complex model (listed first) explains significantly more than the simpler model. This implements a likelihood ratio test (LR). By default, when used for a model fitted with lmer, anova will refit the models with ML. Here we use the option refit=FALSE to prevent this. #mod.noPop &lt;- update(mod4, .~. -(1 | population)) mod.noPop &lt;- lmer(d13c ~ 1 + (1 | family) + block, data=phen, REML=TRUE) mod.noFam &lt;- lmer(d13c ~ 1 + (1 | population) + block, data=phen, REML=TRUE) anova(mod4, mod.noPop, refit=FALSE) ## Data: phen ## Models: ## mod.noPop: d13c ~ 1 + (1 | family) + block ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## mod.noPop 7 2097.2 2129.8 -1041.6 2083.2 ## mod4 8 2066.4 2103.7 -1025.2 2050.4 32.819 1 1.012e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod4, mod.noFam, refit=FALSE) ## Data: phen ## Models: ## mod.noFam: d13c ~ 1 + (1 | population) + block ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## mod.noFam 7 2077.4 2110.0 -1031.7 2063.4 ## mod4 8 2066.4 2103.7 -1025.2 2050.4 12.996 1 0.0003122 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can do a similar test for the fixed effects (see video). Here we have only one effect. Note that we tell the function anova to refit the model with ML (i.e., REML=FALSE). mod.noBlock &lt;- lmer(d13c ~ 1 + (1 | population) + (1 | family), data=phen, REML=TRUE) mod.noFam &lt;- lmer(d13c ~ 1 + (1 | population) + block, data=phen, REML=TRUE) anova(mod4, mod.noBlock, refit=TRUE, REML=FALSE) ## refitting model(s) with ML (instead of REML) ## Data: phen ## Models: ## mod.noBlock: d13c ~ 1 + (1 | population) + (1 | family) ## mod4: d13c ~ 1 + (1 | population) + (1 | family) + block ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## mod.noBlock 4 2067.7 2086.3 -1029.8 2059.7 ## mod4 8 2051.2 2088.5 -1017.6 2035.2 24.454 4 6.475e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output shows two lines of results, one for each model that was compared. The last column, with the p-value Pr(&gt;Chisq), tells us that model mod4, with the fixed effect block, is statistically significantly better than the simpler model mod.noBlock without the fixed effect block. This means that the block effect is statistically significant at a significance level of alpha = 0.05 (p-value &lt; alpha). We can also base our interpretation on the AIC: model mod4 has a lower AIC value and is thus considered better than model mod.noBlock. (See Week 12 for an introduction to model selection). If we have several fixed effects, it may be more convenient to use function Anova from the car package to perform a Wald chi-square test. However, the model must be fitted with ML. Choose between type II and type II sums of squares (see video). mod4.ML &lt;- lmer(d13c ~ 1 + (1 | population) + (1 | family) + block, data=phen, REML=FALSE) car::Anova(mod4.ML, type=&quot;II&quot;, test.statistic=&quot;Chisq&quot;) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: d13c ## Chisq Df Pr(&gt;Chisq) ## block 24.905 4 5.257e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This will return one line per fixed effect, each with a p-value (Pr(&gt;Chisq)). We see that the block effect is statistically significant at a significance level of alpha = 0.05 (p-value &lt; alpha). This result suggests that in the common garden experiment, there were differences in \\(d^{13}C\\) among the seeds that are associated with blocks, i.e., the blocks did not provide exactly the same growing conditions. That is why it is so important to repeat all treatments (here: families) within each block, rather than planting the seeds from one family in one block and the seeds from another family in a different block. Because the common garden experiment was properly designed, we can include the variable block in the model to test and account for this effect. We included it as a fixed effect, though if there were six or more blocks, we would include it as a random effect. Either way will work to account for differences between blocks. 3. Estimate trait heritability Motivation: Now that we have learned how to estimate genetic values for \\(δ^{13}C\\), let’s learn how to estimate what fraction of the total variation in trait values is due to genetic effects. More precisely, we shall estimate the heritable proportion of the trait variation. Not all genetic effects are transmitted from one generation to the next, but the so-called additive genetic effects (see Week 6 lecture). Here we have data on half-siblings because seeds come from the same mother tree, but their father is most likely different because our species is wind pollinated. In such family structure, the estimation of the additive genetic variance is straightforward because these analyses provide key information about whether or not local adaptation should even be considered. Remember that local adaptation is about genetically determined phenotypes that vary across environments in responses to differing selective pressures. This step allows us to assess how genetic variation for a phenotypic trait is distributed across the landscape. Goals &amp; Background: The goal for this part of the laboratory is to estimate heritability, trait differentiation, and correlation with environment for trait values determined in Part 1. To do this, we will be using the output from the previous part of the laboratory and the environmental data contained in the file named WWP_environmental_data.txt. As with the phenotype file this is a tab-delimited text file. a. Estimate heritability Let’s start with estimating the heritability of \\(δ^{13}C\\). If you remember from your undergraduate evolution course, heritability refers generally to the proportion of phenotypic variance due to genetic variance. It comes in at least two different versions. The version we are interested in is narrow-sense heritability (\\(h^2\\)), which is defined as the ratio of additive genetic variance to total phenotypic variance: \\[h^{2} = \\frac{\\sigma^{2}_{additive}}{\\sigma^{2}_{total}}\\] We need to extract the variance components from mod4 for all model terms. We do this visually by printing mod4 to screen or using a set of functions applied to mod4. Here, we will do both. mod4 ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: d13c ~ 1 + (1 | population) + (1 | family) + block ## Data: phen ## REML criterion at convergence: 2050.381 ## Random effects: ## Groups Name Std.Dev. ## family (Intercept) 0.2857 ## population (Intercept) 0.3295 ## Residual 0.8452 ## Number of obs: 779, groups: family, 157; population, 10 ## Fixed Effects: ## (Intercept) block2 block3 block4 block5 ## -30.6263 -0.1383 -0.3507 -0.1006 -0.3944 Using the results from above, let’s calculate \\(h^2\\). If we assume that the seedlings from each maternal tree are half-siblings (i.e. same mom, but each with a different father) then \\(σ^2_A = 4 σ^2_{family}\\) (so variance due to family). If the seedlings were all full-siblings, then the 4 would be replaced with 2. Let’s assume half-siblings. We can then do the following: Copying the values visually from the output above (note that we need to square the standard deviations to get the variances): add_var &lt;- 4*(0.2857^2) total_wp_var &lt;- (0.2857^2) + (0.3295^2) + (0.8452^2) h2 &lt;- add_var/total_wp_var h2 ## [1] 0.3609476 And the same using the variance components from above: one.over.relatedness &lt;- 1/0.25 h2 &lt;- (one.over.relatedness*Components$fam.var) / sum(Components) h2 ## [1] 0.3610402 Question: Inspect your value of \\(h^2\\). What does it mean? We have generated a point estimate for \\(h^2\\). It represents the average \\(h^2\\) across populations after removing the genetic effects due to population differences. To take this further and also get a SE (so that you can construct a confidence interval), you could check out this R package: https://bioconductor.org/packages/release/bioc/html/GeneticsPed.html b. Estimate trait differentiation Great, we have shown that within population genetic variation is statistically greater than zero. What about among population genetic variation? Let’s get to that right now. To measure among population genetic variation we will use a statistic known as \\(Q_{ST}\\). It is similar in concept to \\(F_{ST}\\) from population genetics. To estimate \\(Q_{ST}\\), we will use our LMM output again. If we assume that all seedlings are again half-siblings, then: \\[Q_{ST} = \\frac{\\sigma^{2}_{population}} {\\sigma^{2}_{population}+8\\sigma^{2}_{family}}\\] Again, we can do this by copying the values visually from the output: num_qst &lt;- 0.3295^2 dem_qst &lt;- 0.3295^2 + (8*(0.2857^2)) qst &lt;- num_qst/dem_qst qst ## [1] 0.1425618 Or using the variance components from above: num_qst &lt;- Components$pop.var dem_qst &lt;- Components$pop.var + (8*Components$fam.var) qst &lt;- num_qst/dem_qst qst ## [1] 0.1425583 Question: Inspect your qst value: What does it mean? Look at the quantities in the equation above, what is the denominator equal to? Is it the total phenotypic variance or the total genetic variance? 4. Compare \\(Q_{st}\\) to \\(F_{st}\\) measured from SNP data If you want to learn more about analyzing SNP data in R, here are some resources: adegenet SNP tutorial: http://adegenet.r-forge.r-project.org/files/tutorial-genomics.pdf popgen.nescent.org (click on ‘Users’ tab): http://popgen.nescent.org/ a. Import ‘ecogen’ object Load the data set ‘WWP.ecogen’. data(WWP.ecogen, package=&quot;LandGenCourse&quot;) This ecogen object has information in the following slots: XY: data frame with spatial coordinates (here: longitude, latitude) P: data frame with phenotypic traits (here: intercept, family effects and population effects from mod4) G: data frame with genotypic data (here: 164 SNPs) A: generated automatically from G: matrix of allele counts (codominant markers only) E: data frame with environmental data (here: 7 bioclimatic site variables) S: data frame with structure (hierarchical sampling levels) b. Compare \\(Q_{ST}\\) to \\(F_{ST}\\) See ‘Week 6 Bonus Material’ for calculating Fst from the SNP data. Here, we will use the QstFstComp package to formally test whether or not \\(Q_{ST} &gt; F_{ST}\\) for \\(δ^{13}C\\). The function QstFstComp from the package with the same name calculates both \\(Q_{ST}\\) and \\(F_{ST}\\) itself, then tests whether they are different from each other. Note: as this is a permutation test, if you run it several times, the results may change slightly from run to run. First, however, we need to re-format the data to the format that QstFstComp expects. This is easy with the function ecogen2hierfstat. WWP.hierfstat &lt;- EcoGenetics::ecogen2hierfstat(WWP.ecogen, pop=&#39;population&#39;, to_numeric=TRUE, nout=1) phen_mod &lt;- phen[,-c(2,4)] QstFst_out &lt;- QstFstComp::QstFstComp(fst.dat = WWP.hierfstat, qst.dat = phen_mod, numpops = nlevels(WWP@S$population), nsim = 10000, breeding.design = &quot;half.sib.dam&quot;, dam.offspring.relatedness = 0.25, output = &quot;concise_nowrite&quot;) ## [1] &quot;No output file of Q minus F values written.&quot; QstFst_out ## [[1]] ## Calculated Qst-Fst Lower Bound crit. value.2.5% ## 0.11917632 -0.02656872 ## Upper bound crit. value.97.5% ## 0.06437762 ## ## [[2]] ## [1] &quot;Qst-Fst values output to file: /Users/helene/Desktop/Github_Projects/LandGenCourse_book/vignettes/QminusFvalues_2026-01-21_16-04-43.734543.txt&quot; ## ## [[3]] ## Lower one-tailed p value Upper one-tailed p value Two-tailed p value ## 0.9946 0.0054 0.0108 ## ## [[4]] ## Estimated Fst Lower Bound CI.2.5% Upper bound CI.97.5% ## 0.01419871 0.01111181 0.01737035 ## ## [[5]] ## Estimated Qst Lower Bound CI.2.5% Upper bound CI.97.5% ## 0.13337503 0.02738439 0.37404063 ## ## [[6]] ## Va Lower bound CI.2.5% Upper bound CI.97.5% ## 0.3180848 0.1053936 0.5514179 The output contains the following elements: [[1]]: The calculated difference between Qst and Fst, with 95% critical values of the distribution of the distribution under the null hypothesis of no difference. [[2]]: an output file name (though we suppressed the writing of this file with the option output=\"consise_nowrite\") [[3]]: p-values for a hypothesis test with H0: \\(Q_{ST} = F_{ST}\\), with three different alternatives (less, greater, two.sided) [[4]]: the Fst value estimated from the data with 95% confidence intervals. [[5]]: the Qst value estimated from the data with 95% confidence intervals. [[6]]: the additive genetic variance for the trait with 95% confidence intervals. Note: the values are somewhat different from what we calculated, most likely because the function QstFstComp did not account for the block effect. Questions: Inspect the first and third elements of the list QstFst_out: For the observed values, is \\(Q_{ST} &gt; F_{ST}\\)? Which alternative is most appropriate here? Can the null hypothesis be rejected with alpha = 0.05? What does this mean biologically? 5. Assess correlation between trait and environment Motivation: Similar to the fixed effects, estimates of the random effects can be also extracted from a mixed effects model and used in further analysis. Such estimates are called Best Linear Unbiased Predictions or BLUPs. They are technically predictions because when you fit a mixed effects model, initially only the mean and variance of the random effects are estimated. Then, the random effects or BLUPs for a given experimental unit (e.g., population or family) can be “predicted” from the estimated mean and variance (and the data). Goals and background: Here we will extract block, population and family effects from the model. Then we will correlate the plot-level bioclimatic data with the population effects to see if population differences in seedling \\(δ^{13}C\\) trait can be explained by the environment at the site where the mother tree stands. The underlying idea is to look if selection has made the population different. a. Describe and extract effects Which blocks, populations or trees had higher mean \\(δ^{13}C\\) values than others? To answer this question, we extract and plot the fixed and random effects. We’ll start with the fixed effect for block. The effects are not simply the mean of all seedlings from the same block, but the marginal effect of block`, i.e., estimates of the mean per block after accounting for all other effects (i.e., keeping population and family constant). There are two ways to extract the block effects. The first is with function fixef. This treats the first level of block as a reference level and uses it to estimate the global intercept, then expresses the effect of the other blocks as deviations from the first block. lme4::fixef(mod4) ## (Intercept) block2 block3 block4 block5 ## -30.6263478 -0.1383311 -0.3507053 -0.1006031 -0.3944314 A more convenient way to extract the effects is with function ggeffect of the package ggeffects. ggeffects::ggeffect(mod4, terms=c(&quot;block&quot;)) ## # Predicted values of d13c ## ## block | Predicted | 95% CI ## ---------------------------------- ## 1 | -30.63 | -30.87, -30.38 ## 2 | -30.76 | -31.01, -30.51 ## 3 | -30.98 | -31.23, -30.73 ## 4 | -30.73 | -30.98, -30.48 ## 5 | -31.02 | -31.27, -30.77 The table lists the predicted \\(δ^{13}C\\) value for each block (accounting for population and family) and a 95% confidence interval for each predicted value. Now let’s plot the effects (mean with 95% confidence interval) for each random factor, accounting for all other factors. Again, these are not simply means of observed trait values but marginal effects, accounting for all other factors in the model. Note: If the plots appear in the “Plot” tab in RStudio, use the arrows to scroll between plots. lattice::dotplot(ranef(mod4,condVar=TRUE)) ## $family ## ## $population We can see that there are considerable differences between populations, where “blk cyn” has the highest, and “mt watson” the lowest mean. We can extract the values with ranef. Again, they are deviations from the global intercept (block 1). We are now ready to correlate the population effect to the environmental data. prov.eff &lt;- lme4::ranef(mod4)$population fam.eff &lt;- lme4::ranef(mod4)$family prov.eff ## (Intercept) ## armstrong -0.16372457 ## blk cyn 0.56424812 ## echo lk -0.10480714 ## flume 0.37270696 ## hvn -0.25055204 ## incline lk 0.21873458 ## jakes 0.03776884 ## meiss mdw -0.18557714 ## montreal cyn -0.02673127 ## mt watson -0.46206634 head(fam.eff) ## (Intercept) ## 59 0.269660618 ## 60 -0.036688490 ## 61 -0.007035082 ## 63 -0.259031610 ## 64 0.087029419 ## 65 -0.168188771 b. Correlation of population effect with environment We have population effects estimated for 10 populations, and bioclimatic data for 30 plots (3 plots x 10 populations). If we used all 157 rows of the site data set, this would inflate our sample size. To avoid this, we first extract 30 unique rows (one per plot) from the site data WWP.ecogen@E. Then we use match to find for each plot the corresponding population. This returns the row number (index) and we use it to extract the corresponding population effect. Plot.data &lt;- dplyr::distinct(data.frame(WWP.ecogen@E, population=WWP.ecogen@S$population)) index &lt;- match(Plot.data$population, row.names(prov.eff)) dat2 &lt;- data.frame(prov.eff=prov.eff[index,1], Plot.data[,-8]) Now we estimate the correlation between the population effect and each bioclimatic variable. We summarize the results in a correlation matrix plot. pairs(dat2, lower.panel=panel.smooth, upper.panel=panel.cor, diag.panel=panel.hist) How to read this plot: Each diagonal cell shows the name and histogram of one variable. Each cell in the upper triangle shows the correlation coefficient r, and the p-value for the correlation, between the two variables that define the corresponding row and column. (A larger font indicates a stronger correlation, though this seems to be inconsistent). Each cell in the lower triangle shows a scatterplot of the two variables that define the corresponding row and column. A smooth regression line is shown in red. Questions: Which site variables show the strongest correlation with the population effect? Which site variables are significantly correlated with the population effect? OVerall, does the population effect appear to be related to the environment? "],["r-exercise-week-6.html", "R Exercise Week 6", " R Exercise Week 6 Task: Test whether observed heterozygosity of Pulsatilla vulgaris adults depends on census population size. Fit a model at the individual level where you include a random effect for population. Hints: Load packages: Please install the package inbreedR (to calculate individual measures of heterozygosity) from CRAN, if it is not yet installed. You may want to load the packages dplyr and ggplot2. Alternatively, you can use :: to call functions from packages. Import data and extract adults: Use the code below to import the data. Use dplyr::filter to extract adults with OffID == 0. Pulsatilla &lt;- read.csv(system.file(&quot;extdata&quot;,&quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;)) Calculate multilocus heterozygosity: Use package inbreedR to calculate multilocus heterozygosity for each adult. Use the function inbreedR::convert_raw(x), where x is the matrix of genotypes only (no ID or other non-genetic data), with two columns per locus. Check the help file of the function convert_raw. Use the function inbreedR::MLH to calculate observed heterozygosity for each individual. Add the result as a variable het to the adults dataset. Example code from inbreedR::MLH help file: data(mouse_msats) genotypes &lt;- convert_raw(mouse_msats) het &lt;- MLH(genotypes) Add population-level data: Import the file “pulsatilla_population.csv” with the code below. Do a left-join to add the data to your adults dataset. Check the dataset. Pop.data &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;pulsatilla_population.csv&quot;, package = &quot;LandGenCourse&quot;)) Scatterplot with regression line: Use ggplot2 to create a scatterplot of adult heterozygosity against census population size (population.size), with a regression line. Fit linear mixed model: Adapt code from section 3.c to perform a regression of individual-level observed heterozygosity (response variable) on census population size (predictor), including population as a random effect. Fit the model with REML and print a summary. Test fixed effect: Adapt code from section 2.f to test the fixed effect with function car::Anova. Check residual plots: Adapt code from section 2.d to create residual plots. Questions: There is one influential point in the regression analysis: What was the direction of the relationship, did heterozygosity increase or decrease with census population size? Was the fixed effect statistically significant? Was the model valid, or was there a problem with the residual plots? What would be the main issue, and what remedy could you suggest? "],["Week7.html", "10 Lab 7: Spatial Linear Models", " 10 Lab 7: Spatial Linear Models In this week’s computer lab, we will test the effect of landscape data on genetic data. Along the way, we will explore different ways how to build spatial autocorrelation into statistical models. View Course Video Interactive Tutorial 7 Worked Example R Exercise Week 7 Bonus Vignette: Calculate the Si index Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_7.html", "10.1 View Course Video", " 10.1 View Course Video 1. Embedded Video External link: Week 7 video (Part 1); Week 7 video (Part 2) Transcript: Download transcript Video, Part 1 iframe not supported Video, Part 2 iframe not supported Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_7.html", "10.2 Interactive Tutorial 7", " 10.2 Interactive Tutorial 7 1. List of R commands covered this week Function Package gabrielneigh spdep graph2nb spdep log base coord_trans ggplot2 par(mfrow) graphics par(mar) graphics residuals stats fitted stats geom_histogram ggplot2 lm.morantest spdep errorsarlm spdep as.data.frame base gls nlme Variogram nlme corExp nlme update stats 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_7.html", "10.3 Worked Example", " 10.3 Worked Example Helene Wagner and Yessica Rico 1. Overview of Worked Example a. Goals This worked example shows: How to test regression residuals for spatial autocorrelation. How to fit a model with spatially autocorrelated errors (GLS). How to fit a spatial simultaneous autoregressive error model (SAR). How to perform spatial filtering with Moran eigenvector maps (MEM). How to fit a spatially varying coefficients model (SVC). b. Data set Here we analyze population-level data of the wildflower Dianthus carthusianorum (common name: Carthusian pink) in 65 calcareous grassland patches in the Franconian Jura, Germany (Rico et al. 2013): Dianthus: ‘sf’ object with population-level data (patch characteristics, grazing regime, genetic diversity, 15 alternative connectivity indices Si) for sampling locations, included in package ‘LandGenCourse’. To load the data, type (without quotes): ‘data(Dianthus)’. For a definition of the variables, type: ‘?Dianthus’. c. Required R packages Note: the function ‘library’ will always load the package, even if it is already loaded, whereas ‘require’ will only load it if it is not yet loaded. Either will work. library(LandGenCourse) #library(here) #libraryspdep) library(nlme) #library(lattice) #library(MuMIn) #library(gridExtra) library(dplyr) library(spatialreg) library(ggplot2) library(tmap) #library(sf) source(system.file(&quot;extdata&quot;, &quot;panel.cor.r&quot;, package = &quot;LandGenCourse&quot;)) Package ‘spmoran’ not automatically installed with ‘LandGenCourse’: if(!require(spmoran)) install.packages(&quot;spmoran&quot;, repos=&#39;http://cran.us.r-project.org&#39;) #require(spmoran) 2. Explore data set We will model allelic richness ‘A’ as a function of the following predictors: IBD: connectivity index Si (‘Eu_pj’) based on Euclidean distance between source and focal patch. This represents a hypothesis of isolation by distance (IBD). IBR: connectivity index Si (‘Sheint_pj’) based on the number of continuously or intermittently grazed patches between source and focal patch. This represents a hypothesis of isolation by resistance (IBR). Specifically, this model assumes connectivity via sheep-mediated seed dispersal, where seeds are likely to be transported from patch to patch within the same grazing system (shepherding route). Seeds are assumed to disperse most likely to the next patch (in either direction) along the grazing route, and less likely to more remote patches along the route. PatchSize: Logarithm of calcareous grassland patch size in ha. Bonus Materials: The connectivity indices Si were calculated for each focal patch i, integrating over all other patches j where the species was present (potential source patches) using Hanski’s incidence function. See the Week 7 Bonus Material for how this was done! a. Import data data(Dianthus) Allelic richness ‘A’ was not calculate for populations with &lt; 5 individuals. Here we extract only the patches with ‘A’ values, and the variables needed, and store them in a data frame ‘Dianthus.df’. b. Create a map With tmap (see Week 3 Worked Example), mapping the points is easy. Here, we use color to indicate allelelic richness. Note that tmap internally converts the CRS to lat/long to plot the points on a basemap from the internet. See here for a list of available basemaps: https://leaflet-extras.github.io/leaflet-providers/preview/ tmap_mode(&quot;view&quot;) ## ℹ tmap modes &quot;plot&quot; - &quot;view&quot; ## ℹ toggle with `tmap::ttm()` tm_basemap(c(&quot;Esri.WorldTopoMap&quot;, &quot;Esri.WorldStreetMap&quot;, &quot;Esri.WorldShadedRelief&quot;)) + tm_shape(Dianthus) + tm_sf(col=&quot;A&quot;) #legend01 { background: #FFFFFF; opacity: 1} Toggle between the basemaps to visualize the topographic relief and forest cover. As you can see from the shaded relief, most sites lie on the steep slopes between an upper and a lower Jurassic plateau. A few sites lie at the forest edge on the upper plateau, typically in areas where the soil is too shallow to allow crop farming. With in the study area, all known sites were sampled. Additional sites are expected to be found mainly in the valley system in the Southwest. c. Explore correlations When fitting linear models, it is always a good idea to look at the correlations first. Dianthus.df &lt;- data.frame(A=Dianthus$A, IBD=Dianthus$Eu_pj, IBR=Dianthus$Sheint_pj, PatchSize=log(Dianthus$Ha), System=Dianthus$System, Longitude=Dianthus$Longitude, Latitude=Dianthus$Latitude, st_coordinates(Dianthus)) # Define &#39;System&#39; for ungrazed patches Dianthus.df$System=as.character(Dianthus$System) Dianthus.df$System[is.na(Dianthus.df$System)] &lt;- &quot;Ungrazed&quot; Dianthus.df$System &lt;- factor(Dianthus.df$System, levels=c(&quot;Ungrazed&quot;, &quot;East&quot;, &quot;South&quot;, &quot;West&quot;)) # Remove patches with missing values for A Dianthus.df &lt;- Dianthus.df[!is.na(Dianthus.df$A),] dim(Dianthus.df) ## [1] 59 9 Create a scatterplot matrix. The variables are plotted against each other and labeled along the diagonal. You will find histograms on the diagonal, scatterplots and a smooth line in the lower triangle, and the linear correlation coefficient r (with p-value) in the upper triangle. Stronger correlations are indicated with a larger font. You may ignore any warnings about graphical parameters. graphics::pairs(Dianthus.df[,-c(5:7)], lower.panel=panel.smooth, upper.panel=panel.cor, diag.panel=panel.hist) Questions: How strong is the linear relationship between ‘Eu_pj’ and ‘A’? What does this suggest about the hypothesis of IBD? How strong is the linear relationship between ‘Sheint_pj’ and ‘A’? What does this suggest about the hypothesis of sheep-mediated gene flow (IBR)? Which variable seems to be a better predictor of allelic richness: patch size ‘Ha’ or the logarithm of patch size, ‘PatchSize’? Is logHa correlated with ‘IBD’ or ‘IBR’? Are any of the variables correlated with the spatial coordinates X and Y? Do the three grazing systems, and the ungrazed patches, differ in allelic richness A? Also, let’s check the association between patch size and population size. Here we create boxplots that show the individual values as dots. We add a horizontal jitter to avoid overlapping points. Boxplot1 &lt;- ggplot(Dianthus, aes(x=System, y=A)) + geom_boxplot() + xlab(&quot;Grazing system&quot;) + ylab(&quot;Allelic richness (A)&quot;) + geom_jitter(shape=1, position=position_jitter(0.1), col=&quot;blue&quot;) Boxplot2 &lt;- ggplot(Dianthus, aes(x=factor(pop09), y=log(Ha))) + geom_boxplot() + xlab(&quot;Population size class&quot;) + ylab(&quot;PatchSize (log(Ha))&quot;) + geom_jitter(shape=1, position=position_jitter(0.1), col=&quot;blue&quot;) gridExtra::grid.arrange(Boxplot1, Boxplot2, nrow=1) ## Warning: Removed 6 rows containing non-finite outside the scale range ## (`stat_boxplot()`). ## Warning: Removed 6 rows containing missing values or values outside the scale range ## (`geom_point()`). Even though the population size categories were very broad, there appears to be a strong relationship between populations size (category) and (the logarithm of) patch size. Despite this relationship, connectivity models Si that only considered Dianthus carthusianorum presence/absence (‘pj’) in source patches ‘j’ were better supported than those Si models that took into account source patch area (‘Aj’) or population size (‘Nj’). We can check this by calculating the correlation of allelelic richness ‘A’ with each of the 15 connectivity models ‘Si’ in the data set. round(matrix(cor(Dianthus$A, st_drop_geometry(Dianthus)[,15:29], use=&quot;pairwise.complete.obs&quot;), 5, 3, byrow=TRUE, dimnames=list(c(&quot;Eu&quot;, &quot;Shecte&quot;, &quot;Sheint&quot;, &quot;Shenu&quot;, &quot;Forest&quot;), c(&quot;pj&quot;, &quot;Aj&quot;, &quot;Nj&quot;))),3) ## pj Aj Nj ## Eu 0.024 0.207 0.098 ## Shecte 0.375 0.273 0.369 ## Sheint 0.403 0.196 0.369 ## Shenu 0.135 -0.045 0.105 ## Forest 0.137 0.168 0.138 Correlations with ‘A’ are highest for the two ‘IBR’ models that assume seed dispersal over a limited number of patches along shepherding routes (‘Shecte’ and ‘Sheint’. These two models include only continuously grazed, or both continuously and intermittently grazed patches, respectively. Correlations for models that take in to account population size (‘Nj’) are only slightly lower, whereas those that use patch size (‘Aj’) as a proxy for the size of the seed emigrant pool had lower correlations. 3. Test regression residuals for spatial autocorrelation a. Fit regression models Here we fit three multiple regression models to explain variation in allelic richness: mod.lm.IBD: IBD model of connectivity ‘Eu_pj’. mod.lm.IBR: IBR model shepherding connectivity ‘Sheint_pj’. mod.lm.PatchSize: log patch size and IBR model. mod.lm.IBD &lt;- lm(A ~ IBD, data = Dianthus.df) summary(mod.lm.IBD) ## ## Call: ## lm(formula = A ~ IBD, data = Dianthus.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.68545 -0.10220 0.03883 0.16178 0.36100 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.070960 0.064061 63.549 &lt;2e-16 *** ## IBD 0.008454 0.047460 0.178 0.859 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2324 on 57 degrees of freedom ## Multiple R-squared: 0.0005563, Adjusted R-squared: -0.01698 ## F-statistic: 0.03173 on 1 and 57 DF, p-value: 0.8593 This model does not fit the data at all! mod.lm.IBR &lt;- lm(A ~ IBR, data = Dianthus.df) summary(mod.lm.IBR) ## ## Call: ## lm(formula = A ~ IBR, data = Dianthus.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.66844 -0.11251 0.03418 0.12219 0.41760 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.92306 0.05499 71.348 &lt; 2e-16 *** ## IBR 0.25515 0.07672 3.326 0.00155 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2128 on 57 degrees of freedom ## Multiple R-squared: 0.1625, Adjusted R-squared: 0.1478 ## F-statistic: 11.06 on 1 and 57 DF, p-value: 0.001547 This model fits much better. Let’s check the residuals plots. par(mfrow=c(2,2), mar=c(4,4,2,1)) plot(mod.lm.IBR) par(mfrow=c(1,1)) The residuals show some deviation from a normal distribution. Specifically, the lowest values are lower than expected. mod.lm.PatchSize &lt;- lm(A ~ PatchSize + IBR, data = Dianthus.df) summary(mod.lm.PatchSize) ## ## Call: ## lm(formula = A ~ PatchSize + IBR, data = Dianthus.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.68359 -0.08844 0.02538 0.11705 0.41705 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.05158 0.07781 52.069 &lt;2e-16 *** ## PatchSize 0.04266 0.01888 2.260 0.0277 * ## IBR 0.11338 0.09709 1.168 0.2479 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2055 on 56 degrees of freedom ## Multiple R-squared: 0.2325, Adjusted R-squared: 0.2051 ## F-statistic: 8.482 on 2 and 56 DF, p-value: 0.0006058 This combinde model explains more variation in allelic richness than the IBR model alone. Moreover, after adding PatchSizes, the IBR term is no longer statistically significant! Has the distribution of residuals improved as well? par(mfrow=c(2,2), mar=c(4,4,2,1)) plot(mod.lm.PatchSize) par(mfrow=c(1,1)) Not really! b. Test for spatial autocorrelation (Moran’s I) Before we interpret the models, let’s check whether the assumption of independent residuals is violated by spatial autocorrelation in the residuals. To calculate and test Moran’s I, we first need to define neighbours and spatial weights. Here we use a Gabriel graph to define neighbours. We define weights in three ways (see Week 5 video and tutorial for explanation of code): listw.gab: 1 = neighbour, 0 = not a neighbour. listw.d1: inverse distance weights: neighbour j with weight 1/dij listw.d2: inverse squared distance weights: neighbour j with weight 1/dij^2 In each case, we row-standardize the weights with the option ‘style = “W”’. Note: when using ‘graph2nb’, make sure to use the argument ‘sym=TRUE’. This means that if A is a neighbour of B, B is also a neighbour of A. The default is ‘sym=FALSE’, which may result in some sites not having any neighbours assigned (though this would not be evident from the figure!). xy &lt;- data.matrix(Dianthus.df[,c(&quot;X&quot;, &quot;Y&quot;)]) nb.gab &lt;- spdep::graph2nb(spdep::gabrielneigh(xy), sym=TRUE) par(mar=c(0,0,0,0)) plot(nb.gab, xy) listw.gab &lt;- spdep::nb2listw(nb.gab) dlist &lt;- spdep::nbdists(nb.gab, xy) dlist &lt;- lapply(dlist, function(x) 1/x) listw.d1 &lt;- spdep::nb2listw(nb.gab, style = &quot;W&quot;, glist=dlist) dlist &lt;- lapply(dlist, function(x) 1/x^2) listw.d2 &lt;- spdep::nb2listw(nb.gab, style = &quot;W&quot;, glist=dlist) Now we can quantify and test Moran’s I for each variable to test for spatial autocorrelation in response and predictor variables. For now, we’ll take the simple weights ‘listw.gab’. Allelic richness A: spdep::moran.test(Dianthus.df$A, listw.gab) ## ## Moran I test under randomisation ## ## data: Dianthus.df$A ## weights: listw.gab ## ## Moran I statistic standard deviate = 2.881, p-value = 0.001982 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.29180913 -0.01724138 0.01150691 IBD: spdep::moran.test(Dianthus.df$IBD, listw.gab) ## ## Moran I test under randomisation ## ## data: Dianthus.df$IBD ## weights: listw.gab ## ## Moran I statistic standard deviate = 5.9689, p-value = 1.194e-09 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.62709841 -0.01724138 0.01165320 IBR: spdep::moran.test(Dianthus.df$IBR, listw.gab) ## ## Moran I test under randomisation ## ## data: Dianthus.df$IBR ## weights: listw.gab ## ## Moran I statistic standard deviate = 3.008, p-value = 0.001315 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.30918167 -0.01724138 0.01177588 PatchSize: spdep::moran.test(Dianthus.df$PatchSize, listw.gab) ## ## Moran I test under randomisation ## ## data: Dianthus.df$PatchSize ## weights: listw.gab ## ## Moran I statistic standard deviate = 3.4257, p-value = 0.0003066 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.35490160 -0.01724138 0.01180103 Questions: Which variables showed statistically signficant spatial autocorrelation? Which variables showed the strongest autocorrelation? Is this surprising? Next, let’s test each model for autocorrelation in the residuals: IBD: spdep::lm.morantest(mod.lm.IBD, listw.gab) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = A ~ IBD, data = Dianthus.df) ## weights: listw.gab ## ## Moran I statistic standard deviate = 2.9439, p-value = 0.00162 ## alternative hypothesis: greater ## sample estimates: ## Observed Moran I Expectation Variance ## 0.28889983 -0.02854559 0.01162751 IBR: spdep::lm.morantest(mod.lm.IBR, listw.gab) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = A ~ IBR, data = Dianthus.df) ## weights: listw.gab ## ## Moran I statistic standard deviate = 1.883, p-value = 0.02985 ## alternative hypothesis: greater ## sample estimates: ## Observed Moran I Expectation Variance ## 0.18032443 -0.02296810 0.01165614 PatchSize: spdep::lm.morantest(mod.lm.PatchSize, listw.gab) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = A ~ PatchSize + IBR, data = Dianthus.df) ## weights: listw.gab ## ## Moran I statistic standard deviate = 1.7484, p-value = 0.04019 ## alternative hypothesis: greater ## sample estimates: ## Observed Moran I Expectation Variance ## 0.16223745 -0.02708922 0.01172530 Quite a bit of the spatial autocorrelation in allelic richness can be explained by the spatial structure in the predictors IBR and PatchSize. There is still statistically significant spatial autocorrelation in the residuals, though it is not strong any more. 4. Fit models with spatially correlated error (GLS) with package ‘nlme’ One way to account for spatial autocorrelation in the residuals is to fit a Generalized Least Squares model (GLS) with a spatially autocorrelated error structure. See also: http://rfunctions.blogspot.ca/2017/06/how-to-identify-and-remove-spatial.html a. Plot empirical variogram The error structure in a GLS is defined in a geostatistical framework, based on a variogram and as a function of distance between observations. Hence we start with plotting an empirical variogram of the residuals, with a smooth line. Here we specify ’resType = “normalized”, which means that the variogram will be fitted to the normalized residuals of the model. The expected value of the semivariance will be 1. Hence it would make sense to add a horizontal line at 1. However, this is cumbersome with the trellis graphics (using package ‘lattice’) used by ‘nlme’. model.lm &lt;- nlme::gls(A ~ IBR + PatchSize, data = Dianthus.df, method=&quot;REML&quot;) semivario &lt;- nlme::Variogram(model.lm, form = ~X + Y, resType = &quot;normalized&quot;) If you want to create your own figure, e.g. with ‘ggplot2’, you can access the values stores in the data frame ‘semivario’ to plot the points, and add a smooth line yourself. Then we can add a horizontal line with ‘geom_hline’. ggplot(data=semivario, aes(x=dist, y=variog)) + geom_point() + geom_smooth(se=FALSE) + geom_hline(yintercept=1) + ylim(c(0,1.3)) + xlab(&quot;Distance&quot;) + ylab(&quot;Semivariance&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; Question: What do you conclude from this empirical variogram? Estimate the range of the variogram from the intersection of the smooth line with the horizontal line. Estimate the nugget effect from the intercept at Distance = 0. b. Fit variogram models We can ask R to fit different types of variogram models to this empirical variogram. The model family (e.g., exponential, gaussian, spherical) determines the general shape of the curve that will be fitted. With ‘nugget=T’, we indicate that a nugget effect should be fitted. Note: Here we want to compare mixed models with the same fixed effects but different random effect structures defined by correlation. For this, we use REML. If we wanted to compare models with the same random effects but different fixed effects (as in Week 6), we should use maximum likelihood. With function lme4::lmer, we can set REML=TRUE for REML and REML=FALSE for ML. Here with nlme::gls, we set method=\"REML\" for REML and method=\"ML\" for ML. For update to work here, you’ll need to load the library nlme(or use this, with three coons to access a function ‘update’, for ‘lme’ objects, that is not ‘exported’ from the package: nlme:::update.lme) model.lm &lt;- nlme::gls(A ~ IBR + PatchSize, data = Dianthus.df, method=&quot;REML&quot;) mod.corExp &lt;- update(model.lm, correlation = nlme::corExp(form = ~ X + Y, nugget=T)) mod.corGaus &lt;- update(model.lm, correlation = nlme::corGaus(form = ~ X + Y, nugget=T)) mod.corSpher &lt;- update(model.lm, correlation = nlme::corSpher(form = ~ X + Y, nugget=T)) mod.corRatio &lt;- update(model.lm, correlation = nlme::corRatio(form = ~ X + Y, nugget=T)) #mod.corLin &lt;- update(model.lm, correlation = nlme::corLin(form = ~ X + Y, nugget=T)) c. Select best-fitting model Now we compare all models for which we did not get an error message: MuMIn::model.sel(model.lm, mod.corExp, mod.corGaus, mod.corSpher, mod.corRatio) ## Model selection table ## (Intrc) IBR PtchS correlation df logLik AICc delta weight ## mod.corExp 4.044 0.08468 0.04269 n::cE(X+Y,T) 6 6.283 1.1 0.00 0.336 ## mod.corRatio 4.032 0.09704 0.04129 n::cR(X+Y,T) 6 6.229 1.2 0.11 0.319 ## model.lm 4.052 0.11340 0.04266 4 3.702 1.3 0.29 0.291 ## mod.corGaus 4.047 0.11840 0.04247 n::cG(X+Y,T) 6 3.763 6.1 5.04 0.027 ## mod.corSpher 4.048 0.11710 0.04242 n::cS(X+Y,T) 6 3.760 6.1 5.04 0.027 ## Abbreviations: ## correlation: n::cE(X+Y,T) = &#39;nlme::corExp(~X+Y,T)&#39;, ## n::cG(X+Y,T) = &#39;nlme::corGaus(~X+Y,T)&#39;, ## n::cR(X+Y,T) = &#39;nlme::corRatio(~X+Y,T)&#39;, ## n::cS(X+Y,T) = &#39;nlme::corSpher(~X+Y,T)&#39; ## Models ranked by AICc(x) The list sorts the models, with the best model on top. The last column ‘weight’ contains the model weight, which indicate how much support there is for each model, given all other models in the set (see Week 12). Here, the exponential model fitted best, though the ratio model and the model without a spatially correlated error structure fitted the data almost equally well. The top three models have delta values within 2 (in fact, close to 0). We refit the best model with maximum likelihood to test the fixed effects. mod.corExp.ML &lt;- nlme::gls( A ~ PatchSize + IBR, data = Dianthus.df, method=&quot;ML&quot;, correlation = nlme::corExp(form = ~ X + Y, nugget=T)) car::Anova(mod.corExp.ML) ## Analysis of Deviance Table (Type II tests) ## ## Response: A ## Df Chisq Pr(&gt;Chisq) ## PatchSize 1 5.4912 0.01911 * ## IBR 1 0.8710 0.35067 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The fitted model with the exponential error structure shows a significant effect for PatchSize but not for the IBR term. We don’t get an R-squared value directly, but we can calculate a pseudo R-squared from a regression of the response ‘A’ on the fitted values (using the model fitted with REML). Let’s compare it to the R-squared from the lm model. summary(lm(A ~ fitted(mod.corExp), data = Dianthus.df))$r.squared ## [1] 0.2317629 summary(mod.lm.PatchSize)$r.squared ## [1] 0.2324922 The pseudo R-squared is almost identical to the R-squared of the non-spatial lm model. Let’s check the residual plots: predictmeans::residplot(mod.corExp) ## Registered S3 methods overwritten by &#39;broom&#39;: ## method from ## nobs.fitdistr MuMIn ## nobs.multinom MuMIn The normal probability plot still looks about the same. Note that the function residplot recognized that we have a gls model and added a plot of the auto-correlation function, ACF. Here we have a value of 1 for the distance lag 0, which is the comparison of each value with itself. All other values are low. Recall that the autcorrelation is inversely related to the semivariogram (which does not report the value for lag = 0): semivario &lt;- nlme::Variogram(mod.corExp, form = ~ X + Y, resType = &quot;normalized&quot;) plot(semivario, smooth = TRUE) The variogram of the residuals (after accounting for spatial autocorrelation as modeled by the variogram model) does look better! d. Plot fitted variogram model How can we plot the fitted variogram? Let’s first store it in an object ‘Fitted.variog’, then plot it. Note that the fitted variogram itself has two classes, “Variogram” and “data.frame”. The plot created by plot(Fitted.variog) is a “trellis” object. Fitted.variog &lt;- nlme::Variogram(mod.corExp) class(Fitted.variog) ## [1] &quot;Variogram&quot; &quot;data.frame&quot; class(plot(Fitted.variog)) ## [1] &quot;trellis&quot; plot(Fitted.variog) That was easy. However, trellis plots are difficult to tweak, and we may want to create our own plot with ggplot2. For this, we need to access the fitted variogram values (i.e. the exponential model curve values), which is a bit more involved. The object ‘Fitted.variog’ is a data frame (S3) with additional attributes. This raises a challenge, because we access attributes of S3 objects with $, but we also use $ to access columns in a data frame. If we just print Fitted.variog, we only see the data frame. head(Fitted.variog) ## variog dist n.pairs ## 1 0.6782704 620.5202 85 ## 2 0.6112605 1236.8297 86 ## 3 0.7415493 1827.3367 85 ## 4 0.5751311 2269.6729 86 ## 5 1.0503017 2691.2494 85 ## 6 0.7493939 3074.7370 86 We can see the attributes listed by using str: str(Fitted.variog) ## Classes &#39;Variogram&#39; and &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ variog : num 0.678 0.611 0.742 0.575 1.05 ... ## $ dist : num 621 1237 1827 2270 2691 ... ## $ n.pairs: int [1:20(1d)] 85 86 85 86 85 86 85 86 85 86 ... ## - attr(*, &quot;modelVariog&quot;)=Classes &#39;Variogram&#39; and &#39;data.frame&#39;: 50 obs. of 2 variables: ## ..$ variog: num [1:50] 0.598 0.637 0.673 0.705 0.734 ... ## ..$ dist : num [1:50] 106 385 665 945 1224 ... ## - attr(*, &quot;collapse&quot;)= logi TRUE The line we are looking for is: - attr(*, \"modelVariog\"). The attribute modelVariog has 50 rows (obs.) and 2 variables: $variog and $dist. These are the fitted values (i.e., values of the exponential variogram model for 50 distance values). The notation attr(*, \"modelVariog\") is a cryptic way of telling us how to access the attribute: use the function attr and provide two arguments: the object names Fitted.variog (represented by the asterisk), and the name of the attribute, in quotes: attr(Fitted.variog, \"modelVariog\"). tibble::as_tibble(attr(Fitted.variog, &quot;modelVariog&quot;)) ## # A tibble: 50 × 2 ## variog dist ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.598 106. ## 2 0.637 385. ## 3 0.673 665. ## 4 0.705 945. ## 5 0.734 1224. ## 6 0.760 1504. ## 7 0.784 1783. ## 8 0.805 2063. ## 9 0.824 2342. ## 10 0.841 2622. ## # ℹ 40 more rows This is useful to know if you want to create your own figures, e.g. with ggplot2. ggplot(data=Fitted.variog, aes(x=dist, y=variog)) + geom_point() + ylim(c(0,1.3)) + xlab(&quot;Distance&quot;) + ylab(&quot;Semivariance&quot;) + geom_line(data=attr(Fitted.variog, &quot;modelVariog&quot;), aes(x=dist, y=variog), color=&quot;blue&quot;) + geom_hline(yintercept=1,linetype=&quot;dashed&quot;) e. Add random factor The package nlme allows us also to include random factors. Here we add System as a random factor and test whether this would improve the model fit. Instead of function nlme::gls, we use the function nlme::lme. In nlme, random effects are specified differently from lmer::lme4 (Week6): random = ~ 1 | System. The correlation structure is specified exactly as with gls. mod.lme.corExp &lt;- nlme::lme( A ~ PatchSize + IBR, random = ~ 1 | System, data = Dianthus.df, correlation = nlme::corExp(form = ~ X + Y, nugget=T), method=&quot;REML&quot;) summary(mod.lme.corExp) ## Linear mixed-effects model fit by REML ## Data: Dianthus.df ## AIC BIC logLik ## -0.04236554 14.1351 7.021183 ## ## Random effects: ## Formula: ~1 | System ## (Intercept) Residual ## StdDev: 0.08165681 0.1947089 ## ## Correlation Structure: Exponential spatial correlation ## Formula: ~X + Y | System ## Parameter estimate(s): ## range nugget ## 385.7379348 0.1971837 ## Fixed effects: A ~ PatchSize + IBR ## Value Std.Error DF t-value p-value ## (Intercept) 4.084490 0.08657382 53 47.17928 0.0000 ## PatchSize 0.049722 0.01860186 53 2.67296 0.0100 ## IBR 0.083831 0.10438753 53 0.80307 0.4255 ## Correlation: ## (Intr) PtchSz ## PatchSize 0.560 ## IBR -0.782 -0.485 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.156087746 -0.303811921 0.006632753 0.524431832 1.893761278 ## ## Number of Observations: 59 ## Number of Groups: 4 The nature of the results did not change: PatchSize is still significant but IBR is not. As in Week 6, we can obtain marginal (fixed effects) and conditional R-squared values (fixed + random): MuMIn::r.squaredGLMM(mod.lme.corExp) ## R2m R2c ## [1,] 0.2294698 0.3447197 Now we can include this model in the model comparison from above. Notes: As discussed in the Week 6 video, we should fit the mixed model with maximum likelihood (method = “ML”) to test fixed effects and to compare its AIC to the other models. The Week 6 video followed the philosphy that random effect should only be fitted for factors with &gt;5 levels, whereas here, we are using a factor with 4 levels. MuMIn::model.sel(model.lm, mod.corExp, mod.corRatio, mod.lme.corExp) ## Model selection table ## (Intrc) IBR PtchS class correlation random df logLik AICc ## mod.corExp 4.044 0.08468 0.04269 gls n::cE(X+Y,T) 6 6.283 1.1 ## mod.corRatio 4.032 0.09704 0.04129 gls n::cR(X+Y,T) 6 6.229 1.2 ## model.lm 4.052 0.11340 0.04266 gls 4 3.702 1.3 ## mod.lme.corExp 4.084 0.08383 0.04972 lme n::cE(X+Y,T) S 7 7.021 2.2 ## delta weight ## mod.corExp 0.00 0.295 ## mod.corRatio 0.11 0.280 ## model.lm 0.29 0.256 ## mod.lme.corExp 1.10 0.170 ## Abbreviations: ## correlation: n::cE(X+Y,T) = &#39;nlme::corExp(~X+Y,T)&#39;, ## n::cR(X+Y,T) = &#39;nlme::corRatio(~X+Y,T)&#39; ## Models ranked by AICc(x) ## Random terms: ## S: 1 | System Questions: How may degrees of freedom (df) were used for the random effect? How can you see from the table that System was fitted as a random effect, and what method was used (REML vs. ML)? Was the model with the random effect ranked higher than the model without it? Compare the estimates of the slope coefficients for IBR between the models. How did accounting for spatial autocorrelation affect the slope coefficient, compared to model.lm? How large was the difference due to using different variogram models? And how much of a difference was related to including the random effect? How about the slope estimate for PatchSize? 5. Fit spatial simultaneous autoregressive error models (SAR) An alternative way to account for spatial autocorrelation in the residuals is spatial regression with a simultaneous autoregressive error model (SAR). a. Fit and compare alternative SAR models The method errorsarlm fits a simultaneous autoregressive model (‘sar’) to the error (‘error’) term of a ‘lm’ model. This approach is based on spatial neighbours and weights. We have already defined them in three versions of a listw object. Let’s see which one fits the data best. First, we fit the three models: mod.sar.IBR.gab &lt;- spatialreg::errorsarlm(A ~ PatchSize + IBR, data = Dianthus.df, listw = listw.gab) mod.sar.IBR.d1 &lt;- spatialreg::errorsarlm(A ~ PatchSize + IBR, data = Dianthus.df, listw = listw.d1) mod.sar.IBR.d2 &lt;- spatialreg::errorsarlm(A ~ PatchSize + IBR, data = Dianthus.df, listw = listw.d2) Due to some issues when using model.sel with these objects, here we manually compile AICc and delta values and sort the models by delta: #MuMIn::model.sel(mod.lm.IBR, mod.sar.IBR.gab, mod.sar.IBR.d1, mod.sar.IBR.d2) Models &lt;- list(mod.lm.IBR=mod.lm.IBR, mod.sar.IBR.gab=mod.sar.IBR.gab, mod.sar.IBR.d1=mod.sar.IBR.d1, mod.sar.IBR.d2=mod.sar.IBR.d2) data.frame(AICc = sapply(Models, MuMIn::AICc)) %&gt;% mutate(delta = AICc - min(AICc)) %&gt;% arrange(delta) ## AICc delta ## mod.sar.IBR.d1 -13.86736 0.0000000 ## mod.sar.IBR.gab -13.29796 0.5694002 ## mod.sar.IBR.d2 -11.96555 1.9018039 ## mod.lm.IBR -10.77004 3.0973135 The best model (‘mod.sar.IBR.d1’) is the one with (row-standardized) inverse-distance weights (‘listw.d1’). It is only slightly better than the model with the (row-standardized) binary weights (‘listw.gab’), whereas the nonspatial model and the one with (row-standardized) inverse squared distance weights have much less support. b. Interpret best-fitting SAR model Let’s have a look at the best model. With the argument Nagelkerke = TRUE, we request a pseudo R-squared. summary(mod.sar.IBR.d1, Nagelkerke = TRUE) ## ## Call:spatialreg::errorsarlm(formula = A ~ PatchSize + IBR, data = Dianthus.df, ## listw = listw.d1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.654613 -0.085961 0.016066 0.091687 0.389419 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.067066 0.077809 52.2696 &lt;2e-16 ## PatchSize 0.041416 0.018554 2.2322 0.0256 ## IBR 0.090578 0.094946 0.9540 0.3401 ## ## Lambda: 0.22322, LR test value: 2.6442, p-value: 0.10393 ## Asymptotic standard error: 0.13659 ## z-value: 1.6343, p-value: 0.1022 ## Wald statistic: 2.6708, p-value: 0.1022 ## ## Log likelihood: 12.49972 for error model ## ML residual variance (sigma squared): 0.037595, (sigma: 0.19389) ## Nagelkerke pseudo-R-squared: 0.26613 ## Number of observations: 59 ## Number of parameters estimated: 5 ## AIC: -14.999, (AIC for lm: -14.355) Again, PatchSize is significant but not IBR. The section starting with ‘Lamba’ summarizes the fitted spatial autocorrelation term. It is not statistically significant (p-value = 0.1039 for the Likelihood Ratio test LR). 6. Spatial filtering with MEM using package ‘spmoran’ See tutorial for ‘spmoran’ package: https://arxiv.org/ftp/arxiv/papers/1703/1703.04467.pdf Both GLS and SAR fitted a spatially correlated error structure of a relatively simple form to the data. Gene flow could be more complex and for example, could create spatial autocorrelation structure that is not the same in all directions or in all parts of the study area. Moran Eigenvector Maps (MEM) allows a more flexible modeling of spatial structure in the data. In spatial filtering, we use MEM spatial eigenvectors to account for any spatial structure while fitting and testing the effect of our predictors. a. Default method The new package spmoran makes this really easy. First, we create the MEM spatial eigenvectors. This implies defining neighbors and weights, but this is well hidden in the code below. The function meigen here takes the coordinates, calculates a minimum spanning tree (so that each site has at least one neighbour), and finds the maximum distance ‘h’ from the spanning tree. It then calculates neighbor weights as exp(-dij / h). Note: if you have many sites (&gt; 200), the function meigen_f may be used instead of meigen, it should even work for &gt;1000 sites. The function esf then performs the spatial filtering. Here it uses stepwise selection of MEM spatial eigenvectors using an R-squared criterion (fn = \"r2\"). # lm model: using truncated distance matrix (max of min spanning tree distance) meig &lt;- spmoran::meigen(coords=xy) ## 9 spatial eigen-pairs sfd.res &lt;- spmoran::esf( y=Dianthus.df$A, x=Dianthus.df[,c(&quot;PatchSize&quot;, &quot;IBR&quot;)], meig=meig, fn = &quot;r2&quot; ) ## 5/9 eigenvectors are selected The objects created by functions ‘meigen’ and ‘esf’ contain a lot of information: meigW: a list returned by function ‘meigen’, with the following attributes: sf: Matrix of retained spatial eigenvectors. ev: Eigenvalues of retained spatial eigenvectors. ev_full: All (n - 1) eigenvalues. sfd.res: a list returned by function ‘esf’, with the following attributes: b: Table with regression results for predictors X. r: Table with regression results the selected MEM spatial eigenvectors (based on step-wise eigenvector selection). e: Summary statistics for the entire model. vif: Variance inflation factors. sf: Fitted spatially dependent component (i.e., fitted value based on significant MEM spatial eigenvectors) pred: Fitted values. resid: Residuals. Let’s look at the table ‘b’ with regression results for the predictors first: sfd.res$b ## Estimate SE t_value p_value ## (Intercept) 4.08678879 0.06759658 60.4585130 3.838659e-49 ## PatchSize 0.03642554 0.01673612 2.1764625 3.417603e-02 ## IBR 0.04687286 0.08602602 0.5448685 5.882188e-01 Again, PatchSize is statistically significant but not IBR. Next, we look at the table ‘r’ with regression results for MEM spatial eigenvectors: sfd.res$r ## Estimate SE t_value p_value ## sf6 0.5938646 0.1782765 3.331144 0.001613967 ## sf1 0.3962440 0.1788408 2.215624 0.031207615 ## sf7 -0.4065267 0.1895836 -2.144313 0.036795842 ## sf3 0.3474148 0.1760289 1.973624 0.053854181 ## sf5 -0.3290397 0.1757533 -1.872168 0.066922529 Five MEM spatial eigenvectors were important enough to be included in the model. Here they are ranked by their (absolute value of) slope coefficient, and thus by the strength of their association with the response variable. Eigenvector ‘sf6’ was by far the most important. Note: some eigenvectors are included despite having a p-value &gt; 0.05. This may have two reasons. First, the eigenvectors were selected without taking into account predictors X. Second, a different test was used in the stepwise eigenvector selection. The type of test can be specified with an argument fn (see ‘?esf’ helpfile and ‘spmoran’ tutorial). Finally, let’s look at the summary results for the fitted model: sfd.res$e ## stat ## resid_SE 0.1728755 ## adjR2 0.4374576 ## logLik 24.1369653 ## AIC -30.2739307 ## BIC -11.5760937 Here, adjR2 is rather high (0.437), but this includes the selected MEM spatial eigenvectors! b. Using a custom connectivity matrix We know already that listw.d1 fit the data well, so let’s re-run the model with our own definition of spatial weights. With the funciton ‘listw2mat’, we convert from listw format to a full connnectivity matrix. cmat.d1 &lt;- spdep::listw2mat( listw.d1) meigw &lt;- spmoran::meigen( cmat = cmat.d1 ) ## Note: cmat is symmetrized by ( cmat + t( cmat ) ) / 2 ## 27 spatial eigen-pairs sfw.res &lt;- spmoran::esf( y=Dianthus.df$A, x=Dianthus.df[,c(&quot;PatchSize&quot;, &quot;IBR&quot;)], meig=meigw, fn = &quot;r2&quot; ) ## 15/27 eigenvectors are selected sfw.res$b ## Estimate SE t_value p_value ## (Intercept) 4.05985212 0.06875531 59.0478330 2.716097e-41 ## PatchSize 0.02051541 0.01679035 1.2218572 2.287404e-01 ## IBR 0.06583796 0.09048375 0.7276219 4.709800e-01 tibble::as_tibble(sfw.res$r) ## # A tibble: 15 × 4 ## Estimate SE t_value p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.712 0.172 -4.14 0.000166 ## 2 0.511 0.166 3.08 0.00369 ## 3 -0.366 0.151 -2.43 0.0196 ## 4 0.284 0.152 1.87 0.0693 ## 5 -0.278 0.155 -1.79 0.0806 ## 6 0.236 0.167 1.41 0.166 ## 7 0.270 0.150 1.80 0.0787 ## 8 0.309 0.159 1.94 0.0594 ## 9 -0.289 0.160 -1.80 0.0786 ## 10 0.227 0.152 1.49 0.143 ## 11 0.233 0.151 1.55 0.130 ## 12 0.217 0.151 1.43 0.159 ## 13 -0.189 0.151 -1.25 0.217 ## 14 -0.196 0.152 -1.29 0.204 ## 15 0.166 0.161 1.03 0.309 sfw.res$e ## stat ## resid_SE 0.1493144 ## adjR2 0.5803455 ## logLik 39.2199714 ## AIC -40.4399428 ## BIC -0.9667313 Note: the messages tell us that ‘cmat’ has been made symmetric before analysis, that 27 out of 59 MEM spatial eigenvector (and their eigenvalues, hence ‘pairs’) were retained initially and subjected to stepwise selection, which then returned 15 statistically significant MEM eigenvectors that were included in the regression model with the predictor variables X (PatchSize and IBR). Questions: Does this model fit the data better? Look for a lower AIC. In addition, you can compare the adjusted R-squared. What could cause a difference in model performance? Does this affect the results for PatchSize and IBR? Compare both parameter estimates and p-value between two two models. c. Plot spatial eigenvectors So far, we have treated the MEM spatial eigenvectors as a black box. What kind of patterns do they represent? First, we plot all the selected (significant) eigenvectors. A convenient way to do so is converting to an sf object and then use the function plot. Here we need tweak the column names of the eigenvectors, which are called “X1”, “X2” etc., to show which spatial eigenvectors (sf1, sf2, etc.) are being plotted. We will plot all 15 eigenvectors that were selected above, in order of importance. In the first line, we create a data frame ‘MEM’ that combines the coordinates from ‘xy’ with the eigenvectors from ‘meigw’, ordered by importance. We add the names for the eigenvectors. sfw.res$other$sf_id contains the numbers (ID’s) of the selected eigenvectors. Then we convert the data frame MEM to an sf object with st_as_sf. The plot function for sf objects will plot each attribute. Here we specify that the axes should be plotted (axes=TRUE), but no ticks along the axes (xaxt, yaxt) should be shown - thus, only a box will be drawn around each plot. By default, the first ten variables will be plotted. To show all variables, we use the argument max.plot = ncol(MEM) - 1. MEM &lt;- data.frame(xy, meigw$sf[,sfw.res$other$sf_id]) names(MEM)[-c(1,2)] &lt;- paste0(&quot;sf&quot;, sfw.res$other$sf_id) MEM &lt;- st_as_sf(MEM, coords=c(&quot;X&quot;, &quot;Y&quot;)) plot(MEM, axes=TRUE, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, max.plot = ncol(MEM) - 1) The most important spatial eigenvector (sf6) is plotted at the top left, the second most important (sf23) second from left, etc. The smallest numbers are patterns with the largest spatial scale (sf1), which here shows a gradient from East to West. The most important eigenvector (sf6) shows a finer-scale pattern with the highest values (yellow) in the center, lowest values East and West, and intermediate values North and South. However, these patterns individually are not meaningful. More importantly, we can plot the total spatial component in the response as a weighted sum of these component patterns (MEM$wmean), where the weights correspond to the regression coefficients (Estimate) in table sfw.res$r. Here we create a panel with two plots, the modeled spatial components MEM.w on the left and the response allelic richness A on the right (the mean has been removed to make values comparable). Note: you could calculate the weighted mean sfw.res$sf yourself as follows: data.matrix(st_drop_geometry(MEM[,1:15])) %*% sfw.res$r$Estimate MEM$wmean &lt;- sfw.res$sf #MEM$pred &lt;- sfw.res$pred MEM$A &lt;- scale(Dianthus.df$A, scale = FALSE) plot(MEM[c(&quot;wmean&quot;, &quot;A&quot;)]) Obviously, a big part of the variation in allelic richness is already captured by the weighted mean MEM$wmean. In essence the model then tries to explain the difference between these two sets of values by the predictors “PatchSize” and “IBR”. Let’s quantify the correlation of this spatial component with allelic richness, and compare the correlation between the two models: cor(Dianthus.df$A, data.frame(sfd=sfd.res$sf, sfw=sfw.res$sf)) ## sfd sfw ## [1,] 0.6252594 0.8054356 With the default method (defining neighbors based on a distance cut-off), the spatial component modeled by the significant MEM spatial eigenvectors showed a correlation of 0.625 with the response variable. Using a Gabriel graph with inverse distance weights increased this correlation to 0.805. This means that the spatial eigenvectors derived from the Gabriel graph were more effective at capturing the spatial variation in allelic richness than the default method. This spatial component is then controlled for when assessing the relationship between allelic richness and the predictors (PatchSize and IBR). d. Random effect model The previous model selected 15 MEM spatial eigenvectors, and thus fitted 15 additional models. Just like the random effects for family and population in Week 6 lab, we can save a few parameters here by fitting the set of MEM eigenvectors as a random effect. This is done by the function ‘resf’. sfr.res &lt;- spmoran::resf( y=Dianthus.df$A, x=Dianthus.df[,c(&quot;PatchSize&quot;, &quot;IBR&quot;)], meig = meigw, method = &quot;reml&quot; ) sfr.res$b ## Estimate SE t_value p_value ## (Intercept) 4.06995276 0.06543821 62.1953532 0.00000000 ## PatchSize 0.04119329 0.01635107 2.5193029 0.01550778 ## IBR 0.08142357 0.08374188 0.9723159 0.33626587 tibble::as_tibble(sfr.res$r) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if ## `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## # A tibble: 27 × 1 ## V1 ## &lt;dbl&gt; ## 1 -0.217 ## 2 -0.0376 ## 3 -0.0694 ## 4 -0.142 ## 5 0.0776 ## 6 -0.370 ## 7 -0.0642 ## 8 -0.138 ## 9 0.153 ## 10 0.126 ## # ℹ 17 more rows sfr.res$e ## stat ## resid SE 0.1475148 ## adjR2(cond) 0.5588925 ## rlogLik 6.1272940 ## AIC -0.2545881 ## BIC 12.2106366 sfr.res$s ## (Intercept) ## random SD: Spatial 0.2065325 ## Moran.I/max(Moran.I) 0.7535769 As in Week 6 lab, the conditional R-squared is the variance explained by the fixed effects (PatchSize and IBR) and the random effects (significant MEM spatial eigenvectors) together. It is adjusted for the number of effects that were estimated. Note: we can’t compare AIC with the previous models, as the model was fitted with ‘reml’. We get an additional output ‘sfr.res$s’ with two variance parameters: random_SE: standard error of the random effect (spatial component). Moran.I/max(Moran.I): Moran’s I of the spatial component, rescaled by the maximum possible value. From the help file: “Based on Griffith (2003), the scaled Moran’I value is interpretable as follows: 0.25-0.50:weak; 0.50-0.70:moderate; 0.70-0.90:strong; 0.90-1.00:marked.” 7. Fit spatially varying coefficients model with package ‘spmoran’ See: https://arxiv.org/ftp/arxiv/papers/1703/1703.04467.pdf Now comes the coolest part! So far, we have fitted the same model for all sites. Geographically weighted regression (GWR) would allow relaxing this. Spatial filtering with MEM can be used to accomplish the same goal, and the ‘spmoran’ tutorial calls this a ‘Spatially Varying Coefficients’ model (SVC). The main advantage is that we can visualize how the slope parameter estimates, and their p-values, vary across the study area! This is a great exploratory tool that can help us better understand what is going on. Model with PatchSize and IBR We fit the model with ‘resf_vc’. rv_res &lt;- spmoran::resf_vc( y=Dianthus.df$A, x = Dianthus.df[,c(&quot;PatchSize&quot;, &quot;IBR&quot;)], xconst = NULL, meig = meigw, method = &quot;reml&quot;, x_sel = FALSE) ## [1] &quot;------- Iteration 1 -------&quot; ## [1] &quot;1/3&quot; ## [1] &quot;2/3&quot; ## [1] &quot;3/3&quot; ## [1] &quot;BIC: 27.123&quot; ## [1] &quot;------- Iteration 2 -------&quot; ## [1] &quot;1/3&quot; ## [1] &quot;2/3&quot; ## [1] &quot;3/3&quot; ## [1] &quot;BIC: 24.958&quot; ## [1] &quot;------- Iteration 3 -------&quot; ## [1] &quot;1/3&quot; ## [1] &quot;2/3&quot; ## [1] &quot;3/3&quot; ## [1] &quot;BIC: 24.663&quot; ## [1] &quot;------- Iteration 4 -------&quot; ## [1] &quot;1/3&quot; ## [1] &quot;2/3&quot; ## [1] &quot;3/3&quot; ## [1] &quot;BIC: 24.641&quot; ## [1] &quot;------- Iteration 5 -------&quot; ## [1] &quot;1/3&quot; ## [1] &quot;2/3&quot; ## [1] &quot;3/3&quot; ## [1] &quot;BIC: 24.64&quot; ## [1] &quot;------- Iteration 6 -------&quot; ## [1] &quot;1/3&quot; ## [1] &quot;2/3&quot; ## [1] &quot;3/3&quot; ## [1] &quot;BIC: 24.64&quot; Instead of one slope estimate for each predictor, we now get a different estimate for each combination of parameter and site (sounds like overfitting?). Here’s a summary of the distribution of these estimates. summary( rv_res$b_vc ) ## (Intercept) PatchSize IBR ## Min. :4.106 Min. :0.01704 Min. :-0.66587 ## 1st Qu.:4.106 1st Qu.:0.03130 1st Qu.:-0.06984 ## Median :4.106 Median :0.04139 Median : 0.07449 ## Mean :4.106 Mean :0.04501 Mean : 0.01515 ## 3rd Qu.:4.106 3rd Qu.:0.05196 3rd Qu.: 0.14466 ## Max. :4.106 Max. :0.10385 Max. : 0.22941 The slope estimate for PatchSize varied between 0.017 and 0.1, with a mean of 0.045. The slope estimate for the ‘IBR’ term varied between -0.66 and 0.23, with a mean close to 0! That is an astounding range of variation. Keep in mind that we really expect a positive relationship, there is no biological explanation for a negative relationship. Here is a similar summary of the p-values: summary( rv_res$p_vc ) ## (Intercept) PatchSize IBR ## Min. :0 Min. :0.003329 Min. :0.01822 ## 1st Qu.:0 1st Qu.:0.621329 1st Qu.:1.00000 ## Median :0 Median :1.000000 Median :1.00000 ## Mean :0 Mean :0.786242 Mean :0.95055 ## 3rd Qu.:0 3rd Qu.:1.000000 3rd Qu.:1.00000 ## Max. :0 Max. :1.000000 Max. :1.00000 For both variables, most sites do not show a significant effect (i.e., only few sites show a p-value &lt; 0.05). We could print these results by site (type rv_res$b_vc or rv_res$p_vc). Even better, we can plot them in space. We start with combining the data (‘Dianthus.df’) and the results into one data frame ‘Results’. By specifying b=rv_res$b_vc and p=rv_res$p_vc, R will create column names that start with ‘b’ or ‘p’, respectively. Result &lt;- data.frame(Dianthus.df, b=rv_res$b_vc, p=rv_res$p_vc) names(Result) ## [1] &quot;A&quot; &quot;IBD&quot; &quot;IBR&quot; &quot;PatchSize&quot; ## [5] &quot;System&quot; &quot;Longitude&quot; &quot;Latitude&quot; &quot;X&quot; ## [9] &quot;Y&quot; &quot;b..Intercept.&quot; &quot;b.PatchSize&quot; &quot;b.IBR&quot; ## [13] &quot;p..Intercept.&quot; &quot;p.PatchSize&quot; &quot;p.IBR&quot; Let’s start with PatchSize. Here, we first plot PatchSize in space, with symbol size as a function of patch size. In a second plot, we color sites by statistical significance and the size of the symbols represents the parameter estimate of the regression slope coefficient for Patch Size. The layer ‘coord_fixed’ keeps controls the aspect ratio between x- and y-axes. require(ggplot2) ggplot(as.data.frame(Result), aes(X, Y, size=PatchSize)) + geom_point(color=&quot;darkblue&quot;) + coord_fixed() ggplot(as.data.frame(Result), aes(X, Y, col=p.PatchSize &lt; 0.05, size=b.PatchSize)) + geom_point() + coord_fixed() Let’s do the same for ‘IBR’: require(ggplot2) ggplot(as.data.frame(Result), aes(X, Y, size=IBR)) + geom_point(color=&quot;darkgreen&quot;) + coord_fixed() ggplot(as.data.frame(Result), aes(X, Y, col=p.IBR &lt; 0.05, size=b.IBR)) + geom_point() + coord_fixed() The very small dots in the first map are the ungrazed patches. From the second map, it looks like the significant values were the one with negative slope estimates, for which we don’t have a biological interpretation. Model with IBR only Keep in mind that ‘IBR’ and ‘PatchSize’ showed a strong correlation. The parameter estimates could therefore depend quite a bit on the other variables. To help with the interpretation, let’s repeat the last analysis just with ‘IBR’, without ‘PatchSize’. rv_res &lt;- spmoran::resf_vc( y=Dianthus.df$A, x = Dianthus.df[,c(&quot;IBR&quot;)], xconst = NULL, meig = meigw, method = &quot;reml&quot;, x_sel = FALSE) ## [1] &quot;------- Iteration 1 -------&quot; ## [1] &quot;1/2&quot; ## [1] &quot;2/2&quot; ## [1] &quot;BIC: 13.454&quot; ## [1] &quot;------- Iteration 2 -------&quot; ## [1] &quot;1/2&quot; ## [1] &quot;2/2&quot; ## [1] &quot;BIC: 13.073&quot; ## [1] &quot;------- Iteration 3 -------&quot; ## [1] &quot;1/2&quot; ## [1] &quot;2/2&quot; ## [1] &quot;BIC: 13.073&quot; summary( rv_res$b_vc ) ## (Intercept) V1 ## Min. :3.763 Min. :-0.1331 ## 1st Qu.:3.923 1st Qu.: 0.1329 ## Median :3.959 Median : 0.2340 ## Mean :3.950 Mean : 0.2000 ## 3rd Qu.:4.001 3rd Qu.: 0.2826 ## Max. :4.090 Max. : 0.3758 Now the range of slope estimates is smaller, most sites have a positive estimate, and the mean is approx. 0.21. summary( rv_res$p_vc ) ## (Intercept) V1 ## Min. :0 Min. :0.1325 ## 1st Qu.:0 1st Qu.:0.1779 ## Median :0 Median :0.5969 ## Mean :0 Mean :0.5628 ## 3rd Qu.:0 3rd Qu.:1.0000 ## Max. :0 Max. :1.0000 Also, a larger proportion of sites nows has p-values &lt; 0.05. Let’s plot the results onto a gray-scale, stamen terrain map to facilitate interpretation. Note: here the zoom level zoom = 12 covers the entire study area, whereas the default value would actually cut off a large number of sites. We use the argument force=TRUE to force the map to be downloaded again (otherwise the argument color=\"bw\" may not have an effect if we already downloaded the terrain map in color). Result &lt;- data.frame(Dianthus.df, b=rv_res$b_vc, p=rv_res$p_vc, resid=rv_res$resid) ggplot(as.data.frame(Result), aes(X, Y, col=p.V1 &lt; 0.05, size=b.V1)) + geom_point() + coord_fixed() This is a very different map of results! Most sites now show significant effects. The sites with larger positive estimates show significant effects, whereas those with small or negative estimates show non-significant effects. There are 3 - 4 clusters of sites where the IBR models is not effective at explaining variation in allelic richness: in the very East, in the South-East, and one area in the South-West. Knowing the study area, these are distinct regions (e.g. valleys) that may suggest further biological explanations. Result.sf &lt;- st_as_sf(Result, coords=c(&quot;X&quot;, &quot;Y&quot;), crs=st_crs(Dianthus)) Result.sf$Significant &lt;- Result.sf$p.V1 &lt; 0.05 tmap_mode(&quot;view&quot;) tm_shape(Result.sf) + tm_bubbles(size=&quot;b.V1&quot;, col=&quot;Significant&quot;) #legend01 { background: #FFFFFF; opacity: 1} #legend02 { background: #FFFFFF; opacity: 1} We can compare this to a model with fixed 8. Conclusions We moved from pair-wise distance matrices (link-based) to node-based analysis by integrating the explanatory distance matrices for IBD and IBR into patch-level connectivity indices Si (neighborhood analysis). We found no support for the IBD model, and strong support for the IBR model when tested without additional predictors. The site-level predictors ‘PatchSize’ (log(‘Ha’)) was strongly correlated with our IBR model, and when PatchSize was added to the model, ‘IBR’ was no long statistically significant and its slope estimate changed considerably. The MEM analogue to spatially weighted regression showed very different patterns for ‘IBR’ depending on whether or not ‘PatchSize’ was included in the model. Withouth ‘PatchSize’, ‘IBR’ showed significant positive correlation with allelic richness across the study area, except for three sub-areas. In practical terms, this may suggest that the management strategy of maintaining plant functional connectivity through shepherding seems to be working for this species overall, though there are three parts of the study area where this may not be sufficient to maintain gene flow. The evidence is not conclusive, however, the observed patterns could also be explained by population size, which in this species seems to be associated with patch size. This makes sense if smaller patches contain smaller populations with higher rates of genetic drift. "],["r-exercise-week-7.html", "10.4 R Exercise Week 7", " 10.4 R Exercise Week 7 The Pulsatilla vulgaris dataset that we’ve been analyzing in the R exercises has two variables that were observed or calculated for each sampled mother plant (i.e., those plants from which seeds were collected; see DiLeo et al. 2017, Journal of Ecology): flower.density: he number of flowers within 2 m of the mother plant. A radius of 2 m around mother plants was chosen as it gave the strongest correlation with selfing rates and pollination distances compared to lower (1 m) and higher (3 m) tested values. mom.isolation: the mean distance of the mother plant to all other plants within the population (mean neighbour distance). We would expect the following: A negative relationship, where floral density within 2 m of isolated mother plants is low. Likely (right-)skewed distributions for both variables. Positive spatial autocorrelation of both variables within each patch. Values within each patch likely more similar than between patches. Consider how the points listed above may violate different assumptions of a linear regression model fitted with least squares (lm). How can we test and account for these potential violations and fit a valid model? Task: Test the regression of flower density on the isolation of the sampled mother plants of Pulsatilla vulgaris. Account for the sampling of multiple mothers from each of seven patches, and for residual spatial autocorrelation (if statistically significant). Hints: Load packages: You may want to load the packages dplyr, ggplot2 and ‘nlme’. Alternatively, you can use :: to call functions from packages. Import data, add spatial coordinates. Use the code below to import the data, extract moms, add spatial coordinates, and remove replicate flowers sampled from the same mother. library(dplyr) # Dataset with variables &#39;flower.density&#39; and &#39;mom.isolation&#39; for each mom: Moms &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;pulsatilla_momVariables.csv&quot;, package = &quot;LandGenCourse&quot;)) # Dataset with spatial coordinates of individuals: Pulsatilla &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;)) Adults &lt;- Pulsatilla %&gt;% filter(OffID == 0) # Combine data Moms &lt;- left_join(Moms, Adults[,1:5]) # Remove replicate flowers sampled from the same mother Moms &lt;- Moms %&gt;% filter(OffID == 0) Explore data. How many mother plants are there in total, and per population? Are the distributions of flower.density and of mom.isolation skewed? Create scatterplots: Use ggplot to create a scatterplot of flower.density (y) against mom.isolation (x). Modify the plot with coord_trans to apply a log-transformation to each axis. Will this make the relationship more linear? - Note: using geom_smooth together with coord_trans can create problems. You may adapt the following code to plot two ggplot-type plots side-by-side: gridExtra::grid.arrange(myPlot1, myPlot2, nrow = 1) Scatterplot with line: Instead of using coord_trans, create a scatterplot with log-transformed variable (log(y) vs. log(x)). Add a regression line. Fit non-spatial models: Adapt code from section 4 to fit two models with the response log(flower.density)and the fixed factor log(mom.isolation), using funcions from package nmle: Basic model: you can fit a simple model by adapting this code: nlme::gls(Response ~ FixedEffect, data=Data, method=REML) Random effect: add a random effect for Population, use nlme::lme instead of gls. nlme::lme(Response ~ FixedEffect, random = ~ 1| RandomEffect, data=Data, method=REML) For now, omit the correlation term (no spatial correlation structure). Plot residual variograms: Plot variograms for the two models. Inspect the x-axes of the plots. What is the effect of including the random effect Population on the fitting of the variogram? Recall the sampling design. Was there spatial autocorrelation within populations? Hints: Check the variable names in Moms to adapt the names of the x and y coordinates as needed in the term form= ~ xcoord + ycoord. Print each variogram object to check the number of pairs per lag. Ideally, this should be around 100. These variogram plots are trellis plots. Fortunately, you can again use gridExtra::grid.arrange to plot them side by side. Write each plot into an object first. Add correlation structure: Add a term of the type correlation = nlme::corExp(form = ~ x + y, nugget=T) to the mixed model fitted with REML. Adapt code from section 4.b to evaluate different variogram functions (exponential, spherical, Gaussian, ratio) and use AIC (with REML) to choose the best-fitting variogram model. Check residual plots: For the best model (fitted with REML) and check the residuals. Plot a variogram of the residuals, and the fitted variogram. Test fixed effect: Refit the best model with maximum likelihood to test the fixed effect with car::Anova. Give the model a new name to keep them apart and avoid overwriting. Determine the marginal R-squared. For the best model (fitted with REML), use MuMIn::r.squaredGLMM to determine the marginal and conditional R-squared. Questions: Justify your answers to the following questions: Did you find a statistically significant, negative relationship between local floral density and mom isolation? If so, how strong was it? Was it necessary to account for skewness in both variables? Was it necessary to account for spatial autocorrelation? Was it necessary to account for population? "],["bonus_7a.html", "10.5 Bonus: Si index", " 10.5 Bonus: Si index Yessica Rico and Helene Wagner 1. Overview of Bonus Materials The Week 7 Worked Example used patch-level values of an Si index as predictors. Here we show how these were calculated from distance matrices. The main steps are: Import ecological distance matrices Optimize the scaling parameter alpha Calculate Hanski’s index Si with source patch parameters The following data will be imported: dModels.rds: a list of 5 distance matrices (stored as dist objects) among all 106 calcareous grassland patches in the study area. Each distance matrix represents a biological hypothesis about functional connectivity of D. carthusianorum (see below). PATCH_XY_Dianthus.csv: data frame with patch attributes for all 106 calcareous grassland patches in the study area. We will use the following attributes: Dc.89, Dc.09: binary variables indicating whether or not D. carthusianorum was recorded in the patch in the first survey in 1989, or the second survey in 2009, respectively. Ha: patch area in ha. pop09: population size in 2009 (categorical, 4 levels, see below). #library(tibble) library(dplyr) library(LandGenCourse) 2. Import ecological distance matrices Each distance matrix represents one hypothesis about gene flow in this system (see Week 7 video). Eu: Euclidean (geographic) distance, representing IBD Shecte: Connectivity by shepherding among consistently grazed patches only, distance-dependent: this counts the number of patches that sheep traverse to get from patch A to patch B. Sheint: Connectivity by shepherding, among consistently or intermittently grazed patches, distance-dependent: again, this counts the number of patches that sheep traverse to get from patch A to patch B. Shenu: Connectivity by shepherding, no distance effect. This considers only whether or not two patches are part of the same grazing system. Forest: This model is similar to Eu but considers forest as a barrier. Here, we import these distance matrices as a list of dist objects and check their names and dimensions: each has 106 rows and columns. dModels &lt;- readRDS(system.file(&quot;extdata&quot;, &quot;dModels.rds&quot;, package = &quot;LandGenCourse&quot;)) lapply(dModels, function(ls) dim(as.matrix(ls))) ## $Eu ## [1] 106 106 ## ## $Shecte ## [1] 106 106 ## ## $Sheint ## [1] 106 106 ## ## $Shenu ## [1] 106 106 ## ## $Forest ## [1] 106 106 Note: the distance matrices contain 106 patches. All of these are considered as source populations. However, we will only calculate Si indices for the 65 patches included in the Dianthus dataset. 3. Optimize the scaling parameter alpha In order to calculate Hanski’s index, we first need to optimize the value of alpha for each distance model, using presence-absence data for the two time steps of 1989 and 2009. Import the patch-level data for all 106 patches in the study area. Patches &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;PATCH_XY_Dianthus.csv&quot;, package = &quot;LandGenCourse&quot;), header=TRUE, row.names=1) tibble::as_tibble(Patches) ## # A tibble: 106 × 20 ## patch id Elements2008 Elements2008.4 cat type Ha area x ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A01 1 1 1 3 A 0.136 0.0014 4431470. ## 2 A02 2 2 2 4 B 0.182 0.0018 4431614. ## 3 A03 3 3 3 4 B 0.206 0.0021 4431320. ## 4 A04 4 0 0 4 B 0.142 0.0014 4431246. ## 5 A05 5 2 2 4 B 0.134 0.0013 4431037. ## 6 A06 6 2 2 4 B 0.201 0.002 4431027. ## 7 A07 7 0 0 3 N 0.0213 0.0002 4431284. ## 8 A07a 8 1 1 4 A 0.200 0.002 4431362. ## 9 A08 9 3 2 5 F 0.464 0.0046 4428814. ## 10 A09 10 4 3 3 F 0.0704 0.0007 4428809. ## # ℹ 96 more rows ## # ℹ 11 more variables: y &lt;dbl&gt;, size.cat &lt;int&gt;, sizeCat &lt;chr&gt;, size.cat2 &lt;chr&gt;, ## # pj08 &lt;dbl&gt;, pj89 &lt;dbl&gt;, pop09 &lt;int&gt;, grazing &lt;int&gt;, Dc.89 &lt;int&gt;, ## # Dc.09 &lt;int&gt;, Sampled &lt;int&gt; Check that the patch IDs match between Patches and the distance matrices. Here we tabulate the number of rows for which the patch names match perfectly. This is the case for all 106 rows, which is great. table(Patches$patch == rownames(dModels$Eu)) ## ## TRUE ## 106 The function ‘get.alphafit’ optimizes alpha for each model and stores the values in table ’table.alpha. First, we create an empty matrix (with a single column and five rows, one per model) to hold the optimized alpha value for each model (i.e., for each distance matrix). We define the list of values of alpha that should be considered. Here we will use nseq=100 values between 0.1 and 2.5. Then we define the function get.alphafit that will take four arguments: alpha: scaling parameter of the exponential function d: distance matrix pj: whether or not the species was recorded in a patch in the second survey. Op: number of surveys (out of two) that the species was recorded in the patch. table.alpha &lt;- matrix(NA, nrow=5) dimnames(table.alpha) &lt;- list(models=(names(dModels))) nseq = 100 alpha = seq(0.1,2.5, length = nseq) get.alphafit &lt;- function(alpha, d, pj, Op) { expo&lt;-exp(-alpha* d ) diag(expo)&lt;-0 matr&lt;-sweep(expo,2, pj, &quot;*&quot;) Si &lt;-rowSums(sweep(matr, 2, Op/2, &quot;*&quot;), na.rm=TRUE) mod&lt;- glm(cbind(Op,2 -Op) ~ Si, family=binomial) deviance(mod) } Apply the function to optimize alpha. pj &lt;- Patches$Dc.09 Op &lt;- (Patches$Dc.89 + Patches$Dc.09) for(m in 1:length(dModels)) { table.alpha[m] &lt;- (optimize(get.alphafit, interval=alpha, d=as.matrix(dModels[[m]]), pj, Op)$minimum) } table.alpha ## ## models [,1] ## Eu 2.1708021 ## Shecte 0.6195857 ## Sheint 1.2055182 ## Shenu 1.7221953 ## Forest 2.4999322 These are the optimized scaling parameters alpha of the distance term (related to dispersal ability) in the incidence function \\(S_{i}\\), where \\(p_{j}\\) is 1 if the species is present in source population j and 0 if it is absent, and \\(d_{ij}\\) is the pairwise distance between source patch j and focal patch i: \\(S_{i}\\) = \\(\\sum_{j\\neq i} exp(-\\alpha d_{ij})p_{j}\\) 4. Calculate Hanski’s index Si with source patch parameters We will consider there alternative source patch parameters \\(p_{j}\\): whether or not the species was observed in 2009. \\(A_{j}\\): area of the patch in ha. \\(N_{j}\\): indicator of population size. This was recorded in the field in four ordinal categories: 1: &lt;4 (too few to treat as population) 2: 4-40 (small enough for complete sampling) 3: 41-100 (small enough to count) 4: &gt;100 (very large) Note that here, \\(N_{j}\\) is treated as a numeric variable. While this is an over-interpretation, though preliminary analyses showed that this linearized the relationships between variables. pj &lt;- Patches$Dc.09 Aj &lt;- Patches$Ha Nj &lt;- Patches$pop09 Compile three alternative source patch parameters for each patch. Note that we set the area Aj to zero of the species was not recorded in the patch. This is done by multiplication with pj. Source&lt;- data.matrix(data.frame(pj=pj, Aj=pj*Aj, Nj=Nj)) mod &lt;-rep(1:5, rep(3,5)) Prepare an empty table Si to hold the connectivity indices \\(S_{i}\\), one for each combination of focal patch i (rows), distance model \\(d_{ij}\\) and source patch parameter (15 columns: 5 distances x 3 parameters). Si &lt;- data.frame(matrix(NA,nrow(Patches),ncol=15)) dimnames(Si) &lt;- list(row.names(Patches), paste(rep(names(dModels), rep(3,5)), rep(c(&quot;pj&quot;, &quot;Aj&quot;, &quot;Nj&quot;),5), sep=&quot;_&quot;)) Define function get.Si with three arguments: alpha: the optimized alpha value from above d: the distance matrix Ap: the source patch parameter to be used. get.Si &lt;- function(alpha, d, Ap) { expo&lt;-exp(-alpha*d) diag(expo)&lt;-0 matr&lt;-sweep(expo,2, Ap, &quot;*&quot;) S &lt;- rowSums(sweep(matr, 2, Op/2, &quot;*&quot;), na.rm=TRUE) } Apply function get.Si to calculate Si values. sb indicates which column of Ap to use as source patch parameter. sb &lt;- rep(1:3,5) for (n in 1:ncol(Si)) { Si[,n] &lt;- get.Si(alpha=table.alpha[mod[n]], d=as.matrix(dModels[[mod[n]]]), Ap=Source[,sb[n]]) } Table with results: values of Si. Column names are a combination of distance matrix and source patch parameter. tibble::as_tibble(Si) ## # A tibble: 106 × 15 ## Eu_pj Eu_Aj Eu_Nj Shecte_pj Shecte_Aj Shecte_Nj Sheint_pj Sheint_Aj Sheint_Nj ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.636 1.21 1.76 8.58e-26 1.42e-25 2.47e-25 3.07e-51 5.07e-51 8.83e-51 ## 2 0.626 0.616 1.58 8.52e-26 1.42e-25 2.46e-25 3.05e-51 5.06e-51 8.79e-51 ## 3 0.943 0.638 2.32 8.52e-26 1.42e-25 2.46e-25 6.20e- 1 2.27e+ 0 2.05e+ 0 ## 4 1.27 0.847 3.30 8.52e-26 1.42e-25 2.46e-25 5.02e- 1 7.71e- 1 1.30e+ 0 ## 5 1.50 1.13 4.10 1.26e+ 0 3.41e+ 0 4.21e+ 0 5.04e- 1 4.09e- 1 1.29e+ 0 ## 6 1.67 1.44 4.91 2.13e+ 0 6.27e+ 0 7.40e+ 0 7.42e- 1 1.19e+ 0 2.43e+ 0 ## 7 2.12 1.98 6.50 8.58e-26 1.42e-25 2.47e-25 3.07e-51 5.07e-51 8.83e-51 ## 8 1.21 1.62 3.82 2.00e+ 0 6.83e+ 0 7.11e+ 0 8.76e- 1 1.60e+ 0 2.87e+ 0 ## 9 1.60 2.99 4.41 8.52e-26 1.41e-25 2.46e-25 1.00e+ 0 2.56e+ 0 3.32e+ 0 ## 10 1.82 2.14 4.38 8.52e-26 1.42e-25 2.46e-25 3.28e- 1 3.53e- 1 7.79e- 1 ## # ℹ 96 more rows ## # ℹ 6 more variables: Shenu_pj &lt;dbl&gt;, Shenu_Aj &lt;dbl&gt;, Shenu_Nj &lt;dbl&gt;, ## # Forest_pj &lt;dbl&gt;, Forest_Aj &lt;dbl&gt;, Forest_Nj &lt;dbl&gt; Note: the dataset Dianthus used in the Week 7 Worked Example only contains Si values for 65 patches with genetic data (i.e., the patches where D. carthusianorum was observed, and sampled, during the second survey 2009). "],["Week8.html", "11 Lab 8: Simulation Experiments", " 11 Lab 8: Simulation Experiments In this week’s computer lab, we will perform a landscape genetic simulation experiment. Along the way, we will learn how to design simulation experiments to test landscape genetic methods, and how to make our code more efficient (see also Bonus Vignette). View Course Video Interactive Tutorial 8 Worked Example R Exercise Week 8 Bonus Vignette: efficient R; parallel computing in R Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. "],["video_8.html", "11.1 View Course Video", " 11.1 View Course Video 1. Embedded Video External link: Week 8 video (Part 1); Week 8 video (Part 2) Transcript: Download transcript Video, Part 1 iframe not supported Video, Part 2 iframe not supported Preview Slides Download slides ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["tutorial_8.html", "11.2 Interactive Tutorial 8", " 11.2 Interactive Tutorial 8 1. List of R commands covered this week Function Package expand.grid base seq base plot(asp=1) graphics coord_fixed ggplot2 rnorm stats runif stats LETTERS base paste base paste0 base sprintf base grep base substr base strsplit base gsub base 2. General Instructions a) How to access tutorials Through RStudio Add-in: Install course Addins in RStudio: library(LandGenCourse) In RStudio, click on Addins (top menu bar) Follow instructions in the Console: type: require(swirl) type: swirl() follow prompts select course (“Landscape_Genetics_R_Course”) and tutorial (Weeks 1 - 8) b) How to complete tutorial Follow prompts in the RStudio Console. To stop and resume a tutorial: to stop and exit swirl, type: bye() to resume where you stopped, type: swirl() To restart tutorial from beginning: type:swirl() use a different name (simply add a number, like this: ‘MyName2’) c) How to submit answers (participating institutions only) The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. This feature is only available for students from participating institutions. If you choose ‘yes’, a form will open in your web browser. Complete and submit the form. You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use ‘skip’). If you used ‘skip’ because you could not answer a question, please contact your instructor for advice. "],["WE_8.html", "11.3 Worked Example", " 11.3 Worked Example Bernd Gruber, Erin Landguth &amp; Helene Wagner 1. Overview of Worked Example a. Goals This worked example shows: Simulate a metapopulation on a resistance landscape Evaluate the power of a partial Mantel test Compare partial Mantel test to ‘Sunder’ Run many simulations and synthesize results b. Data set We will simulate data using the ‘landgenreport’ function of the package ‘PopGenReport’. See: www.popgenreport.org c. Required R packages library(LandGenCourse) library(PopGenReport ) #load the package library(secr) #to create a random habitat #library(gdistance) #library(mmod) library(raster) #library(tibble) #library(here) #library(ggplot2) #library(MLPE) Package secr not automatically installed with ‘LandGenCourse’: if(!require(secr)) install.packages(&quot;secr&quot;, repos=&#39;http://cran.us.r-project.org&#39;) #library(secr) Install gstudio if missing: if(!requireNamespace(&quot;popgraph&quot;, quietly = TRUE)) { install.packages(c(&quot;RgoogleMaps&quot;, &quot;geosphere&quot;, &quot;proto&quot;, &quot;sampling&quot;, &quot;seqinr&quot;, &quot;spacetime&quot;, &quot;spdep&quot;), dependencies=TRUE) remotes::install_github(&quot;dyerlab/popgraph&quot;) } if(!requireNamespace(&quot;gstudio&quot;, quietly = TRUE)) remotes::install_github(&quot;dyerlab/gstudio&quot;) ## Registered S3 method overwritten by &#39;gstudio&#39;: ## method from ## print.locus genetics The following ‘setup chunk’ is used to set the root address of file paths to the root of the project folder. knitr::opts_knit$set(root.dir = normalizePath(&quot;..&quot;)) 2. Initialize a landscape a. Create a random landscape We will use the ‘randomHabitat’ function from the ‘secr’ package to create a random habitat map and assign resistance values to habitat and non-habitat. There are many alternative ways to define your map, e.g. simply load a png file or any other file format using the ‘raster’ function from package ‘raster’ (?raster::raster, see the examples in there). If your map is categorical, assign resistance values to the different values in the raster as shown below for the missing values. If your map is already a resistance surface, skip this step. Here we use the function ‘set.seed’ at the beginning of the simulation to make sure we get the same sequence of random numbers everytime we run this code. This makes the exact results reproducible even though we use a random process. The function ‘make.grid’ here creates a landscape of nx=50 times xy=50 gridpoints spaced 1 unit (meter) apart. This is returned as a data frame ‘tempgrid’ with two columns that represent ‘x’ and ‘y’ grid coordinates. nx=50 ny=50 set.seed(555) #(to make sure we have the same example running) #tempmask&lt;-secr::make.mask(nx=nx,ny=ny,spacing=1) tempgrid&lt;-secr::make.grid(nx=nx,ny=ny,spacing=1) In the function ‘randomHabitat’, the argument ‘A’ specifies the expected proportion of habitat, and ‘p’ controls the level of fragmentation, i.e., degree of spatial aggregation (sorry this is naming may be a bit confusing, but that’s what it says in the help file: ?randomHabitat). The function simulates a map with these parameters and returns a data frame with only those points from ‘tempgrid’ that are habitat. It expects an input object of class ‘mask’ (an object type specific to the ‘secr’ package), hence we pass ‘as.mask(tempgrid)’. Finally, we create a raster object (package raster``) calledr` with the simulated habitat. tmp &lt;- secr::randomHabitat(secr::as.mask(tempgrid), p = 0.5, A = 0.5) r &lt;- as.data.frame(tempgrid) r$resistance &lt;- 10 r$resistance[as.numeric(row.names(tmp))] &lt;- 1 r &lt;- raster::rasterFromXYZ(r) Let’s verify that we have two values in the raster: 1 for habitat, 10 for non-habitat table(values(r), exclude=&quot;&quot;) ## ## 1 10 ## 1309 1191 Plot the habitat map plot(r) We have thus created a numeric raster with a resistance surface where habitat cells (grey) have a resistance value of 1 and non-habitat cells (green) have a resistance value of 10. b. Add populations to the landscape (using minimal distance) We create a function that allows us to set up ‘n’ subpopulations in the habitat only (grid cells with value = 1). The sub-populations should be at least ‘minDist’ units apart, given any resistance surface ‘landscape’. We also include an option to plot a raster map with the sampled locations of the populations. We define a few variables within the function that help keep track. Note that we keep track of the cells by their raster cell number (which goes from 1:ncells). Here’s what the code does: Extract all cells that are habitat and store cell number in HabitatCells. Randomly sample one habitat cell and store its cell number in Selected. Store cell numbers of all remaining habitat cells in Remaining. Create a ‘while’ loop that continues until one of two things happens: Sample size ‘n’ is reached. There are no cells left in Remaining. Inside the loop: Randomly sample one habitat cell and store its number in Candidate. Remove the Candidate from Remaining (we don’t want to consider it twice). Calculate the Distance between Candidate and all populations in Selected. The function ‘xyFromCell’ gets the cell coordinates for each cell number, and the function ‘pointDistance’ calculates the distance between two sets of coordinates, here the coordinates for Candidate and for all cells in Selected. The argument ‘lonlat=FALSE’ tells ‘pointDistance’ that the coordinates are Euclidean. If the minimum of Distance is larger than ‘minDist’, add a population. This is done by appending the value in Candidate to the vector Selected. Repeat. If requested, the raster map is plotted, cell coordinates for all populations (Selected) are extracted and added to the map as points with point symbol pch=16 (filled circle). createpops &lt;- function(n=10, minDist=5, landscape=r, habitat=1, plot=TRUE) { HabitatCells &lt;- c(1:length(values(landscape)))[values(landscape)==habitat] Selected &lt;- sample(HabitatCells, 1) Remaining &lt;- HabitatCells[!is.element(HabitatCells, Selected)] while (length(Selected) &lt; n &amp; length(Remaining) &gt; 0) { Candidate &lt;- sample(Remaining, 1) Remaining &lt;- Remaining[!is.element(Remaining, Candidate)] Distances &lt;- raster::pointDistance(raster::xyFromCell(landscape, Candidate), raster::xyFromCell(landscape, Selected), lonlat=FALSE) if(min(Distances) &gt; minDist) { Selected &lt;- append(Selected, Candidate) } } if(plot==TRUE) { plot(landscape) points(xyFromCell(landscape, Selected), pch=16) } return(Selected) } Test the function above: createpops(n=8, minDist = 3, landscape = r, plot = TRUE) ## [1] 1328 476 395 2497 440 683 601 1189 c. Initialise a metapopulation We use the function ‘init.popgensim’ from package ‘PopGenReport’ to initialise a metapopulation based on the grid cells that we just selected. To do this we need to initialise a number of parameters (the locations of the subpopulations, the number of individuals per subpopulation, the number of loci and alleles per loci. For a full list check ‘?init.popgensim’). To store all the parameters we create a list called para where we store all of them 3. Define simulation parameters a. Define your metapopulation Define metapopulation: para&lt;- list() #Define populations (dynamics) para$n.pops=8 para$n.ind=100 para$sex.ratio &lt;- 0.5 #age distribution.... para$n.cov &lt;- 3 #number of covariates (before the loci in the data.frame, do not change this!!) Define population dynamics: #reproduction para$n.offspring = 2 #migration para$mig.rate &lt;- 0.1 #dispersal: exponential dispersal with maximal distance in map units para$disp.max=50 #average dispersal of an individual in meters para$disp.rate = 0.05 #proportion of dispersing individuals #Define genetics para$n.allels &lt;- 10 para$n.loci &lt;- 20 para$mut.rate &lt;- 0.001 Define cost distance method: par(mar=c(1,1,1,1)) para$method &lt;- &quot;leastcost&quot; #rSPDdistance, commute para$NN &lt;- 8 #number of neighbours for the cost distance method # Initialize simulation of populations from scratch landscape&lt;- r #&lt;-raster(system.file(&quot;external/rlogo.grd&quot;, package=&quot;raster&quot;)) # Define x and y locations para$cells &lt;- createpops(n=para$n.pops, minDist = 3, landscape = landscape, plot = FALSE) para$locs &lt;- raster::xyFromCell(landscape, para$cells) #give the population some names rownames(para$locs) &lt;- LETTERS[1:para$n.pops] # Create a matrix of pairwise cost distances... cost.mat &lt;- PopGenReport::costdistances(landscape, para$locs, para$method, para$NN) # ... and a matrix of pairwise Euclidean distances eucl.mat &lt;- as.matrix(dist(para$locs)) #needed for the analysis later # Plot your landscape with the populations.... plot(landscape) points(para$locs[,1], para$locs[,2], pch=16, cex=2, col=&quot;orange&quot;) text(para$locs[,1],para$locs[,2], row.names(para$locs), cex=1.5) # Check the parameter list para ## $n.pops ## [1] 8 ## ## $n.ind ## [1] 100 ## ## $sex.ratio ## [1] 0.5 ## ## $n.cov ## [1] 3 ## ## $n.offspring ## [1] 2 ## ## $mig.rate ## [1] 0.1 ## ## $disp.max ## [1] 50 ## ## $disp.rate ## [1] 0.05 ## ## $n.allels ## [1] 10 ## ## $n.loci ## [1] 20 ## ## $mut.rate ## [1] 0.001 ## ## $method ## [1] &quot;leastcost&quot; ## ## $NN ## [1] 8 ## ## $cells ## [1] 450 893 2358 388 2425 294 1393 679 ## ## $locs ## x y ## A 49 41 ## B 42 32 ## C 7 2 ## D 37 42 ## E 24 1 ## F 43 44 ## G 42 22 ## H 28 36 b. Initialise your population on the landscape Now finally we can initialise our population using the init function. We’ll call it ‘simpops.0’ to indicate that this is the initial generation. simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, para$sex.ratio, para$n.loci, para$n.allels, para$locs, para$n.cov ) You may want to check the simpops object, which is simply a list of our subpopulation and each individual is coded in a single run in one of the subpopulations. names(simpops.0) #the names of the subpopulations ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; head(simpops.0$A[,1:6]) ## a.list of the first 6 individuals and columns of population A ## pop sex age locus1A locus1B locus2A ## 1 1 female NA 6 10 2 ## 2 1 female NA 4 3 6 ## 3 1 female NA 2 10 9 ## 4 1 female NA 7 9 5 ## 5 1 female NA 3 1 9 ## 6 1 female NA 6 7 3 We can also analyse our simpop object. (e.g. calculate the pairwise Fst value between all the populations). To be able to do that we first need to convert it into a genind object (because many functions need this type of object as input). gsp &lt;- PopGenReport::pops2genind(simpops.0, locs =para$locs) gsp #check the genind object ## /// GENIND OBJECT ///////// ## ## // 800 individuals; 20 loci; 200 alleles; size: 738.5 Kb ## ## // Basic content ## @tab: 800 x 200 matrix of allele counts ## @loc.n.all: number of alleles per locus (range: 10-10) ## @loc.fac: locus factor for the 200 columns of @tab ## @all.names: list of allele names for each locus ## @ploidy: ploidy of each individual (range: 2-2) ## @type: codom ## @call: df2genind(X = res, sep = &quot;/&quot;, ind.names = rownames(res), pop = combine$pop) ## ## // Optional content ## @pop: population of each individual (group size range: 100-100) ## @other: a list containing: xy summary(gsp) #some summary statistics ## ## // Number of individuals: 800 ## // Group sizes: 100 100 100 100 100 100 100 100 ## // Number of alleles per locus: 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 ## // Number of alleles per group: 200 200 200 200 200 200 200 200 ## // Percentage of missing data: 0 % ## // Observed heterozygosity: 0.9 0.89 0.89 0.89 0.9 0.89 0.9 0.92 0.9 0.9 0.91 0.9 0.9 0.88 0.9 0.9 0.91 0.9 0.89 0.88 ## // Expected heterozygosity: 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 round(mmod::pairwise_Gst_Nei(gsp),5) ## A B C D E F G ## B -0.00012 ## C 0.00006 0.00001 ## D -0.00019 -0.00060 0.00022 ## E 0.00010 -0.00030 -0.00014 -0.00058 ## F 0.00016 -0.00031 0.00002 -0.00019 -0.00009 ## G -0.00020 -0.00046 0.00022 -0.00023 -0.00027 -0.00013 ## H -0.00045 -0.00013 -0.00009 -0.00008 -0.00007 -0.00033 -0.00020 Is there an effect of the landscape on the population structure (there should not be after initialisation)? The function ‘pairwise.fstb’ is around 150 times faster than mmod::pairwise_Gst_Nei, but slightly different. gen.mat &lt;- PopGenReport::pairwise.fstb(gsp) round(gen.mat ,5) ## A B C D E F G H ## A 0.00000 0.00239 0.00256 0.00231 0.00260 0.00266 0.00231 0.00205 ## B 0.00239 0.00000 0.00252 0.00191 0.00220 0.00220 0.00205 0.00238 ## C 0.00256 0.00252 0.00000 0.00273 0.00237 0.00253 0.00273 0.00241 ## D 0.00231 0.00191 0.00273 0.00000 0.00193 0.00232 0.00228 0.00243 ## E 0.00260 0.00220 0.00237 0.00193 0.00000 0.00241 0.00224 0.00244 ## F 0.00266 0.00220 0.00253 0.00232 0.00241 0.00000 0.00237 0.00218 ## G 0.00231 0.00205 0.00273 0.00228 0.00224 0.00237 0.00000 0.00230 ## H 0.00205 0.00238 0.00241 0.00243 0.00244 0.00218 0.00230 0.00000 Now we perform a two partial Mantel tests, one for the effect of the cost distance partialling out the effect of Euclidean distance (Gen ~cost | Euclidean), and one the other way round. The method ‘wassermann’ from the ‘PopGenReport’ package returns a data frame with two rows (one for each test) and three columns (model, r = Mantel r statistic, p = p-value), following this method: Wassermann, T.N., Cushman, S. A., Schwartz, M. K. and Wallin, D. O. (2010). Spatial scaling and multi-model inference in landscape genetics: Martes americana in northern Idaho. Landscape Ecology, 25(10), 1601-1612. PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab ## model r p ## 1 Gen ~cost | Euclidean 0.2056 0.157 ## 2 Gen ~Euclidean | cost -0.0571 0.603 Check the pairwise Fst values, why are they so low? Hints: How were genotypes assigned to the initial generation How many generations have we simulated thus far? At this point in the simulation, do you expect to see an effet of IBD, IBR, or neither? 4. Run simulations and analyze results a. Run your simulation over multiple time steps (years) Now we can run our simulation by simply passing our object ‘simpops’ to the function ‘run.popgensim’, with some additional parameters that are needed for the simulation. We specify the number of generations the simulation should run with the steps parameter. (Check ?run.popgensim for a description of all parameters). Important to understand is the idea of the cost.mat (which is the cost matrix that is used for the distance between subpopulation). The n.alleles, n.ind cannot be different from the initialisation. simpops &lt;- PopGenReport::run.popgensim(simpops.0, steps=3, cost.mat, n.offspring=para$n.offspring, n.ind=para$n.ind, para$mig.rate, para$disp.max, para$disp.rate, para$n.allels, para$mut.rate, n.cov=para$n.cov, rec=&quot;none&quot;) In essence we were running a metapopulation with 100 individuals per subpopulation on our resistance landscape for 3 generations. The question is now was that enough time to create an effect on population structure? b. Analyse your simulated population with a partial Mantel test Let’s check the pairwise Fst values and then do a landscape genetic analysis using partial Mantel tests. Convert to genind to calculate pairwise Fst. gsp &lt;- PopGenReport::pops2genind(simpops, para$locs, para$n.cov) Calculate your genetic distance matrix e.g. fst or D. gen.mat &lt;- PopGenReport::pairwise.fstb(gsp) round(gen.mat ,3) ## A B C D E F G H ## A 0.000 0.006 0.007 0.006 0.007 0.008 0.008 0.007 ## B 0.006 0.000 0.008 0.007 0.006 0.008 0.008 0.008 ## C 0.007 0.008 0.000 0.007 0.007 0.008 0.007 0.008 ## D 0.006 0.007 0.007 0.000 0.007 0.007 0.007 0.007 ## E 0.007 0.006 0.007 0.007 0.000 0.008 0.007 0.008 ## F 0.008 0.008 0.008 0.007 0.008 0.000 0.008 0.007 ## G 0.008 0.008 0.007 0.007 0.007 0.008 0.000 0.008 ## H 0.007 0.008 0.008 0.007 0.008 0.007 0.008 0.000 Partial Mantel test: PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab ## model r p ## 2 Gen ~Euclidean | cost 0.1857 0.161 ## 1 Gen ~cost | Euclidean -0.1086 0.74 We can extract a specific value from this result, e.g., the p-value of the test “Gen ~cost | Euclidean”. (Note that every time we call the function ‘wassermann’, a permutation test is performed (default: ‘nperm = 999’), and the p-value may thus vary somewhat). res &lt;- PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab res[res$model == &quot;Gen ~cost | Euclidean&quot;, &quot;p&quot;] ## [1] &quot;0.717&quot; c. Optional: Analyze your simulated populations with MLPE We will cover this method, and model selection, in more detail in Week 12. Install package corMLPE if needed: if(!requireNamespace(&quot;corMLPE&quot;, quietly = TRUE)) remotes::install_github(&quot;nspope/corMLPE&quot;) To run the model, we need to extract the vector of pairwise distances from the three distance matrices: eucl.vect &lt;- as.vector(as.dist(eucl.mat)) cost.vect &lt;- as.vector(as.dist(cost.mat)) gen.vect &lt;- as.vector(as.dist(gen.mat)) Define the two vectors of population effects: Pop &lt;- matrix(names(as.dist(eucl.mat))[[1]], nrow(eucl.mat), ncol(eucl.mat), byrow=F) pop1 &lt;-Pop[lower.tri(Pop)] pop2 &lt;-t(Pop)[lower.tri(Pop)] Assemble the link-based dataset: Link.data &lt;- data.frame(gen.vect, eucl.vect, cost.vect, pop1, pop2) Fit and compare three models: G: Geographic distance model (IBD) E: Ecological distance model (IBR) GE: Both We will compare the three models by AICc, the small-sample version of AIC (see Week 12). The lower the value of AICc, the better is the model fit. The method includes a correction for the number of predictors in the model, so that we can make a fair comparison between GE (two predictors) to G and E (one predictor each). Note that because we are comparing models with the same random effects (population effects pop1 and pop2, but with different fixed effects, we fit the models with maximum likelihood, ML (see Week 6). GE &lt;- nlme::gls(gen.vect ~ eucl.vect + cost.vect, correlation=corMLPE::corMLPE(form=~pop1+pop2), data=Link.data, method=&quot;ML&quot;) G &lt;- update(GE, ~ eucl.vect) E &lt;- update(GE, ~ cost.vect) MuMIn::AICc(GE, G, E) ## df AICc ## GE 5 -329.2276 ## G 4 -331.9044 ## E 4 -330.7569 Which is the best model? tmp &lt;- MuMIn::AICc(GE, G, E) row.names(tmp)[tmp$AICc == min(tmp$AICc)] ## [1] &quot;G&quot; Let’s combine all of this into our own function to extract the vectors, define the population effects, run MLPE, and extract the name of the best fitting model getMLPE &lt;- function(gen=gen.mat, eucl=eucl.mat, cost=cost.mat) { Pop &lt;- matrix(names(as.dist(eucl))[[1]], nrow(eucl), ncol(eucl), byrow=F) Link.data &lt;- data.frame( eucl.vect = as.vector(as.dist(eucl)), cost.vect = as.vector(as.dist(cost)), gen.vect = as.vector(as.dist(gen)), pop1 = Pop[lower.tri(Pop)], pop2 = t(Pop)[lower.tri(Pop)] ) GE &lt;- nlme::gls(gen.vect ~ eucl.vect + cost.vect, correlation=corMLPE::corMLPE(form=~pop1+pop2), data=Link.data, method=&quot;ML&quot;) G &lt;- update(GE, ~ eucl.vect) E &lt;- update(GE, ~ cost.vect) tmp &lt;- MuMIn::AICc(GE, G, E) return(row.names(tmp)[tmp$AICc == min(tmp$AICc)]) } Let’s test it: getMLPE() ## [1] &quot;G&quot; 5. Run simulator using a previously defined parameter set Once the simulator works and you are certain that you understand how the simulator needs to be set up for a single run, in almost all studies on simulations you want to be able to re-run the simulator in an automatized way. There are several reasons why you want to do that. You want to perform a sensitivity analysis on a single parameter, which means, try to find how much does the output (e.g. pairwise Fst between subpopulations) change when you vary an input parameter (e.g. number of loci). You want to explore the “complete” parameter space, which means, instead of changing values of a single input parameter you want to change all parameters (within certain levels) and run their combinations. Another reason is that you want to create a simulated test data set that forms the backbone of your future studies. So we would like to do the following. Specify and record all the parameter combinations that you would like to run. Run the simulator with every combination [Optional] save your complete simulation run (recommended, but sometimes prohibitive due to needed resources) or only a calculated summary. Read in your simulations, analyse them and synthesize your results via additional statistics, tests, plots. Publish an enormously important paper…. Admittedly there are several different approaches and as it seems every modeller has a slightly different way to achieve these steps. One approach is to create a parameter file that records all the parameter setting for each run. Another approach is to create so called scripts for every single run. The advantage here is that scripts can be easily distributed across different cores and machines and therefore this approach is easy to parallelise your runs, which sometimes is necessary. Finally the approach I will present here (also because of practical reasons) is to create an R data.frame that stores all the parameter settings and we run all combinations in serial mode instead of in parallel. Okay before we start we need to think about what kind of parameters we want to explore. I would like to do the following runs: Run our simulations as above (same parameter settings) for varying time steps (say between 5 to 45 years in steps of 20). We’ll keep the number of levels and the maximum number of steps low in this example to limit computation time. Feel free to expand! As output I would still like to record the Fst value, but also the full genetic data set and the parameters used to run the simulation. In addition I want to repeat each run 5 times (most often you would do more repetitions) to check how much general variation there is between runs with exactly the same parameter combination. a. Specify and record the parameter combinations Let’s define the varying numbers of time steps we would like to run the simulations. Here we define a sequence from 5 to 45 in steps of 20, which results in a series c(5, 25, 45). We will interpret these values as numeric, therefore we don’t convert to ‘factor’. timesteps &lt;- seq(from=5 , to=45, by=20) We also specify the number of repeats (replicate simulation runs). We want to do five replicate simulation runs per for each level of ‘time’, and we will label replicates from 1 through 5. These are essentially labels and we’ll save them as a factor: repeats &lt;- factor(1:5) Now we would like to have a data frame that stores all possible combinations for those two parameters. As simple way to do that in R, is to use the ‘expand.grid’ function. para.space &lt;- expand.grid(rep=repeats, time=timesteps) tibble::as_tibble(para.space) ## # A tibble: 15 × 2 ## rep time ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 5 ## 2 2 5 ## 3 3 5 ## 4 4 5 ## 5 5 5 ## 6 1 25 ## 7 2 25 ## 8 3 25 ## 9 4 25 ## 10 5 25 ## 11 1 45 ## 12 2 45 ## 13 3 45 ## 14 4 45 ## 15 5 45 As you can see this results in 15 combinations (3 time steps x 5 repeats). The beauty of this approach is that it is very flexible and adaptable to runs over other parameter combinations, as you can provide more than two parameter variables to ‘expand.grid’. b. Run the simulator over every parameter combination Remember our parameters are all defined in the ‘para’ object (a list) and we want to keep them constant, except for running the simulation for different number of years. This means that we only need to modify the argument ‘steps’. Summarizing the code from above, a single run of our simulator runs via: #initialize simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, para$sex.ratio, para$n.loci, para$n.allels, para$locs, para$n.cov ) #run for 20 generations simpops &lt;- PopGenReport::run.popgensim(simpops.0, steps=20, cost.mat, n.offspring=para$n.offspring, n.ind=para$n.ind, para$mig.rate, para$disp.max, para$disp.rate, para$n.allels, para$mut.rate, n.cov=para$n.cov, rec=&quot;none&quot;) We adapt this code as follows: Create a ‘for’ loop that cycles through every row ‘i’ in ‘para.space’ For each value of ‘i’: Initialize population ‘simpops.0.’ Run the simulation with argument ‘steps = para.space$time[i]’. We are not running the code just yet, hence it is commented-out with ‘#’. #for (i in 1:nrow(para.space)) #{ # #initialize # simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, # para$sex.ratio, para$n.loci, para$n.allels, # para$locs, para$n.cov ) # # #run for para.space$time[i] generations # simpops &lt;- PopGenReport::run.popgensim(simpops.0, # steps=para.space$time[i], cost.mat, # n.offspring=para$n.offspring, n.ind=para$n.ind, # para$mig.rate, para$disp.max, para$disp.rate, # para$n.allels, para$mut.rate, # n.cov=para$n.cov, rec=&quot;none&quot;) #} Have a close look at the change. Question: what changes between replicate runs, and what not? Consider the following aspects: Landscape Population locations Pairwise distances (cost, Euclidean) Initial populations with initial genotypes Migration and gene flow c. Save your complete simulation run (input and output) Simply running the simulation 15 times (number of rows in ‘para.space’) by itself is not useful yet. We need to store the simulation runs somehow, so we can collect them afterwards to calculate summary statistics and analyse the runs. How do we store the repeats seperately in a file? One approach would be to have a different file name for every repeat, but in my view, a cleaner approach is to store all simulation outputs and also store the complete parameter and input information in a file, so everything that is need is in one place. A nice way to do that in R is to create a ‘list’ object that stores all in a single object, which can be saved (and is automatically packed) and re-loaded as an R object. This is convenient as long as I only want to analyze the results in R, not export to other software. Here we do the following: Create a timer with the function ‘proc.time’ so that we know roughly how long the computations take. For each line ‘i’ in ‘para.space’: Initialize simpops.0. Run the simulation with ‘steps=para.space$time[i]’. Convert the resulting ‘simpop’ to a genind object ‘gi’ (smaller to store) Create a list ‘sim’ of all simulation parameters we want to store. Save the object ‘sim’ as an ‘RData’ file with a unique file name in the folder ‘output/simout’ in the project directory. Print a message after each run to report progress and computation time. Flush the output console to make sure it is current (only relevant for console-based versions of R). The list ‘sim’ will contain the following elements. Note: the name is repeated (e.g., ‘gi = gi’) to create a named list, i.e., to specify the names of the list elements. para.space: the row ‘para.space[i]’ with settings of the simulation run. para: a copy of the list ‘para’ that contains the other parameters that are the same for all simulation runs. landscape: the landscape. cost.mat: the matrix of pairwise cost distances gi: the genind object that contains the genotypes at the end of the simulation. First we make sure the folder simout exists within the output folder in the R project: if(!dir.exists(paste0(here::here(),&quot;/output&quot;))) dir.create(paste0(here::here(),&quot;/output&quot;)) if(!dir.exists(paste0(here::here(),&quot;/output/simout&quot;))) dir.create(paste0(here::here(),&quot;/output/simout&quot;)) ## c.eate a timer (just to know how long it will take roughly) timer0 &lt;- round(proc.time()[3],2) for (i in 1:nrow(para.space)) { ## i.itialize simpops.0 &lt;- PopGenReport::init.popgensim(para$n.pops, para$n.ind, para$sex.ratio, para$n.loci, para$n.allels, para$locs, para$n.cov ) # run for para.space$time[i] generations simpops &lt;- PopGenReport::run.popgensim(simpops.0, steps=para.space$time[i], cost.mat, n.offspring=para$n.offspring, n.ind=para$n.ind, para$mig.rate, para$disp.max, para$disp.rate, para$n.allels, para$mut.rate, n.cov=para$n.cov, rec=&quot;none&quot;) ## c.nvert to genind object (smaller) gi &lt;- PopGenReport::pops2genind(simpops) ## c.eate a list of all I want to collect sim &lt;- list(para.space=para.space[i,], para=para, landscape=landscape, cost.mat=cost.mat, gi=gi) # save everything in an output folder (with a consecutive number, with three leading zeros, so the file sorting is nicer) save(sim, file = paste0(here::here(),&quot;/output/simout/sim_time5-45_rep5_&quot;, sprintf(&quot;%03i&quot;,i) ,&quot;.RData&quot;)) cat(paste0(&quot;Finished run: &quot;, i,&quot; out of &quot;,nrow(para.space), &quot;. So far, it took: &quot;, round(proc.time()[3]-timer0,2),&quot; sec.\\n&quot;)) flush.console() } ## Finished run: 1 out of 15. So far, it took: 0.31 sec. ## Finished run: 2 out of 15. So far, it took: 0.65 sec. ## Finished run: 3 out of 15. So far, it took: 0.95 sec. ## Finished run: 4 out of 15. So far, it took: 1.26 sec. ## Finished run: 5 out of 15. So far, it took: 1.56 sec. ## Finished run: 6 out of 15. So far, it took: 2.88 sec. ## Finished run: 7 out of 15. So far, it took: 4.25 sec. ## Finished run: 8 out of 15. So far, it took: 5.58 sec. ## Finished run: 9 out of 15. So far, it took: 6.91 sec. ## Finished run: 10 out of 15. So far, it took: 8.25 sec. ## Finished run: 11 out of 15. So far, it took: 10.61 sec. ## Finished run: 12 out of 15. So far, it took: 13.34 sec. ## Finished run: 13 out of 15. So far, it took: 15.72 sec. ## Finished run: 14 out of 15. So far, it took: 18.07 sec. ## Finished run: 15 out of 15. So far, it took: 20.37 sec. d. Analyze and synthesize results If you check your output folder (simout) you should see 15 files. Note: File paths can be different when you execute a chunk in an R notebook compared to when you copy-paste the same line into the console! We avoid this problem by using the function ‘here’ from package ‘here’. head(dir(paste0(here::here(), &quot;/output/simout&quot;))) ## [1] &quot;sim_time5-45_rep5_001.RData&quot; &quot;sim_time5-45_rep5_002.RData&quot; ## [3] &quot;sim_time5-45_rep5_003.RData&quot; &quot;sim_time5-45_rep5_004.RData&quot; ## [5] &quot;sim_time5-45_rep5_005.RData&quot; &quot;sim_time5-45_rep5_006.RData&quot; Now we are at step D where we need to read in all our files one by one, calculate some summary statistics and plot our results. Again, this could be easy, but be aware if you have thousands of files it could take quite some time and memory. The most convenient way is to load everyting and store it in a list, so we can access all of our simulations from memory. I will show how to do this in the example below, but be aware in larger simulations (think millions of runs, or large sample sizes) this is not possible and we would load a single simulation, calculate a statistic, store only the result in a table and free the memory for the next simulation run. Let’s load our simulation runs. There is one caveat: when we load the object ‘sim’ from the ‘.RData file’, we can’t assign it a new object name. I.e., we can’t use ‘newName &lt;- load(“sim.Rdata”). Instead, we can only type ’load(“sim.Rdata”)’ and it will create or overwrite the object ‘sim’. Also, R takes the name from the object that was saved, not from the file name. Hence, once we load the object, any existing object of the same name will be overwritten. So if we want to keep, it we need to rename it before using ‘load’. Here we do the following: Create an empty table with three columns ‘rep’, ‘time’, ‘fst’ and other columns to collect summary results for the 15 simulation runs. Create a vector that contains all filenames. The function ‘list.files’ does just that. We specify with path=“./simout”’ that the files in folder ‘simout’ should be listed, and with ’pattern=“sim” we specify that we want all file names that contain the expression “sim” (we could also have used “time”, for example - any component of the file name that is shared by all the files we are interested in but no other files that might be in the same folder). Loop through the files. For each filename ‘i’: Load the file, which will create or overwrite the object ‘sim’. We need to supply not only the file name but the path, hence ‘paste0(“./simout/”,filenames[i])’ (see Week 8 video for more on file paths). Extract simulation parameters: Copy the ith row from ‘para.space’ (repeat number, timesteps) into the first two columns of the ith row of ‘res’. Extract the genind object ‘gi’ with the final genotypes. Calculate the mean of pairwise fst values and store in the third column of the ith row of ‘res’. Perform partial Mantel tests with function ‘wasserman’ and store the Mantel r statistics and the p-values in the corresponding columns. Note that this is somewhat tricky because the function ‘wasserman’ ranks the models and the better fitting model is listed in the first row, so that the order of the models can vary between runs. Therefore we extract the values based on model name. Perform ‘Sunder’ covariance analysis and extract which model is best supported (‘G’, ‘E’ or ‘GE’). res &lt;- data.frame(rep=NA, time=NA, fst=NA, r.Eucl=NA, p.Eucl=NA, r.cost=NA, p.cost=NA, MLPE=NA) #load all files in the folder filenames &lt;- list.files(path= paste0(here::here(), &quot;/output/simout&quot;), pattern=&quot;sim&quot;) for (i in 1:length(filenames)) { #creates a sim object load(paste0(here::here(), &quot;/output/simout/&quot;,filenames[i])) #now let us take what we need from the simulation res[i,1:2] &lt;- sim$para.space #calculate a summary statistic: mean of pairwise fst values ## h.re we only take the lower triangle of the matrix to avoid the diagonal values, # which are zero by definition (comparing each population to itself) gen.mat &lt;- PopGenReport::pairwise.fstb(sim$gi) res [i,3] &lt;- mean(gen.mat[lower.tri(gen.mat)]) #Distance matrices eucl.mat &lt;- dist(sim$para$locs) cost.mats = list(cost=sim$cost.mat) #partial Mantel tests wass &lt;- PopGenReport::wassermann(eucl.mat, cost.mats = list(cost=sim$cost.mat), gen.mat = gen.mat, plot=F)$mantel.tab res[i,4:5] &lt;- wass[wass$model == &quot;Gen ~Euclidean | cost&quot;, 2:3] res[i,6:7] &lt;- wass[wass$model == &quot;Gen ~cost | Euclidean&quot;, 2:3] #Sunder res[i,8] &lt;- getMLPE(gen=gen.mat, eucl=eucl.mat, cost=sim$cost.mat) } Look at the ‘res’ data frame and check the results. head(res) ## rep time fst r.Eucl p.Eucl r.cost p.cost MLPE ## 1 1 5 0.009270325 0.0725 0.324 -0.0206 0.506 G ## 2 2 5 0.009484194 -0.2763 0.941 0.5356 0.013 E ## 3 3 5 0.008522157 -0.1102 0.699 0.2604 0.149 E ## 4 4 5 0.009055060 -0.0172 0.527 0.056 0.415 G ## 5 5 5 0.009037522 0.1347 0.216 -0.0211 0.521 G ## 6 1 25 0.022749633 -0.3674 0.992 0.6932 0.003 E The next step would be to visualise the results (e.g. plot runs over times and color by rep). A quick way to do that is to use the function ‘ggplot’ from the ‘ggplot2’ package. Here we add a jitter to keep points from overlapping too much. ggplot2::ggplot(res, ggplot2::aes(x=time, y=fst)) + ggplot2::geom_point(position = ggplot2::position_jitter(w = 0.5)) Now it is again time for you to experiment. For example, why not set up a simulation that varies the number of loci. Or as you may have seen even after 100 generation there was no sign that the mean pairwise Fst value is levelling off. So how long do you have to run a simulation in terms of time to see this (be aware that simulation runs take longer if you increase the number of timesteps)? Questions: How would you set up a simulation experiment to compare type I error rates between partial Mantel test and Sunder? How about statistical power? Have fun and please give us feed back what you think about this Worked Example. Bernd Gruber, Erin Landguth, Helene Wagner. "],["r-exercise-week-8.html", "11.4 R Exercise Week 8", " 11.4 R Exercise Week 8 Task: Carry out a permutation test for the Mantel rank correlation to test for fine-scale spatial genetic structure in Pulsatilla vulgaris adults, using the pooled data from all seven patches. Use a one-sided alternative “greater”, as we expect the Mantel rank correlation to be positive. Hints: Exclude all pairs that involve individuals from different patches. Permute individuals only within patches, not between patches. Calculate the Mantel rank correlation for the observed data (M.rho.obs) For each of R = 499 permutations, calculate M.rho.sim Determine the approximate p-value of the one-sided test with alternative “greater” as the percentage of the 500 values (1 observed, 499 permuted) that are greater or equal to the observed one. Load packages: You may want to load the packages dplyr and gstudio. Alternatively, you can use :: to call functions from packages. Import data, extract adults. Use the code below to import the data into gstudio and extract adults (OffID == 0). library(dplyr) Pulsatilla.gstudio &lt;- gstudio::read_population(path=system.file(&quot;extdata&quot;, &quot;pulsatilla_genotypes.csv&quot;, package = &quot;LandGenCourse&quot;), type=&quot;column&quot;, locus.columns=c(6:19), phased=FALSE, sep=&quot;,&quot;, header=TRUE) Adults.gstudio &lt;- Pulsatilla.gstudio %&gt;% filter(OffID == 0) Sort individuals by patch. Create a new ID variable Names that combines the existing variables Population and ID (starting with population). Then use the function dplyr::arrange to sort Adults.gstudio by Names (check the help file for arrange). This is important here for two reasons: In order to efficiently permute individuals within patches, they need to be sorted by patch. In some cases, the function gstudio::genetic_distance will sort the distance matrix alphabetically. To avoid mismatches between the data and the resulting distance matrix, it is best to sort the data alphabetically already. Calculate Euclidean distances: Use the metric coordinates in variables “X” and “Y” to calculate Euclidean distances (see Week 5, section 4). Here it is important to store the distances Dgeo in the full matrix format, with as.matrix. Calculate genetic distances (Dps): Use the following code (as needed (adapt if needed) to calculate individual-level genetic distances (proportion of shared alleles) and store them in the full matrix format. We subtract values from 1 to obtain a distance measure. Note: the calculation in gstudio is based on Bray distance and the resulting values are proportional to those calculated by adegenet::propShared. I.e., the two measures have a correlation of 1 but the actual values differ. Dgen &lt;- 1 - as.matrix(gstudio::genetic_distance(Adults.gstudio, stratum=\"Names\", mode=\"dps\")) Plot distances, calculate M.rho: Create a plot of Dgen vs Dgeo, and calculate the Mantel rank correlation. Note: the function ‘cor’ with default settings does not allow missing values (NA). This can be changed e.g. with the argument use. Use as.dist to access only the values from the lower triangle of each matrix. The function plot will do. Inspect the plot. Where would you find the pairs of individuals within the same patch? Use the function cor with method = \"spearman\" to calculate the Mantel rank correlation. Allow for missing values with the argument use = \"complete.obs\". Limit to within-patch pairs: Restrict the analysis to pairs within the same patch. For this, we want to set all between-site comparisons in Dgeo to NA. Uncomment the code below to: Create a matrix that is TRUE if the two individuals are in the same patch, and FALSE if not (first line) Change FALSE to NA (second line) Multiply Dgeo by this matrix to set distances between patches to NA (third line). Adapt your code from above to plot the distances Dgen vs. Dgeo.within. Calculate the Mantel rank correlation between Dgen vs. Dgeo.within and store it as Cor.obs. #SamePatch &lt;- outer(Adults.gstudio$Population, Adults.gstudio$Population, FUN = &quot;==&quot;) #SamePatch[SamePatch == &quot;FALSE&quot;] &lt;- NA #Dgeo.within &lt;- SamePatch * Dgeo Note: check the help file or run the following examples to figure out what outer does: outer(c(1:5), c(1:5), FUN = \"*\") outer(c(1:5), c(1:5), FUN = \"-\") outer(LETTERS[1:5], c(1:5), FUN = \"paste0\") Unrestricted permutation test: Create a variable Order with row numbers (from 1 to the number of individuals in Adults.gstudio). Then, uncomment the code below (adapt as needed) to carry out a permutation test by permuting the order of individuals, R = 499 times. Notes to permute a distance matrix, we need to permute the rows and columns of the full distance matrix simultaneously with the same order: Dgen[a,a]. We only need to permute one of the two matrices (Dgen or Dgeo.within), but not both. The approximate p-value is calculated as the proportion of values (R simulated ones and 1 observed Cor.obs) that were as large, or larger, than Cor.obs. #R = 499 #Cor.perm.unrestricted &lt;- rep(NA, R) #for(r in 1:R) #{ # a &lt;- sample(Order) # Cor.perm.unrestricted[r] &lt;- cor(as.dist(Dgen[a,a]),as.dist(Dgeo.within), method=&quot;spearman&quot;, use=&quot;complete.obs&quot;) #} #approx.p.unrestricted &lt;- mean(c(Cor.obs, Cor.perm.unrestricted) &gt;= Cor.obs) #approx.p.unrestricted Restricted permutation test: Adapt the code to permute individuals only within patches. For this, split ‘Order’ by population, randomize row numbers within groups (list elements = populations) with sapply, and use unlist to convert to a vector again. Make sure to change object names from ‘unrestricted’ to ‘restricted’. Before the for loop, add this code: b &lt;- split(Order, Adults.gstudio$Population) Inside the for loop, replace the calculation of a : a &lt;- unlist(sapply(b, sample)) Compare results: Create side-by-side boxplots of the simulated Mantel rank correlation values for the unrestricted and the restricted permutation tests. Note: if none of the simulated values was larger than Cor.obs, the approx. p-value will be 1/(R+1). This indicates the resolution of the permutation test, i.e., the smallest possible p-value given R. Questions: Did the direction, size or statistical significance of the observed Mantel rank correlation (as a measure of fine-scale spatial genetic structure in P. vulgaris) change between the unrestricted and the restricted permutation test? Why, or why not? How did the distributions of the simulated values of the Mantel rank correlation differ between the unrestricted and the restricted test? Can you think of a reason for this? "],["bonus_8a.html", "11.5 Bonus: Efficient R", " 11.5 Bonus: Efficient R Helene Wagner 1. Overview of Bonus Material a. Goals This Bonus Material provides some introductory worked examples for: Navigating the file system Benchmarking file import and export functions Profiling an R script Creating and executing a Bash R script Parallelizing code b. Data set We will use the wolf SNP data (Schweizer et al., 2016) from the Week 11 worked example. The genetic data are individual-based, and are input as allele counts (i.e. 0/1/2) for each locus. We are using a randomly sampled subset of 10,000 single nucleotide polymorphism (SNP) markers from the full data set (which contains 42,587 SNPs). c. Required R packages library(LandGenCourse) #library(microbenchmark) #library(profvis) #library(here) #library(readr) #library(data.table) library(feather) #library(rio) #library(devtools) #library(parallel) #library(doParallel) #library(knitr) #library(compiler) 2. Navigating the file system This Bonus material assumes that you are running R in RStudio within a ‘R project’. Therefore, the default workspace when you enter commands in the console should be your R project folder. A discussed in the video, Part 2, the default location when executing code from an R Notebook is the folder where the notebook is stored. If you downloaded this Bonus material from ‘LandGenCourse’, it should be stored in a folder ‘downloads’ in your project folder. More about R projects: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects. a. Where am I? Let’s see where we are. getwd() ## [1] &quot;/Users/helene/Desktop/Github_Projects/LandGenCourse_book/vignettes&quot; here::here() ## [1] &quot;/Users/helene/Desktop/Github_Projects/LandGenCourse_book&quot; Sys.getenv(&quot;HOME&quot;) ## [1] &quot;/Users/helene&quot; R.home() ## [1] &quot;/Library/Frameworks/R.framework/Resources&quot; Question: Copy-paste the commands in the chunk above directly into the Console and run them there. Do you get the same paths? getwd(): If you execute the chunk, or knit the notebook, this will return the locatio of the R Notebook. If you copy-paste it into the console, it will return the project folder. here::here: This should return the project folder in both cases. Sys.getenv(\"HOME\"): This should return your home directory on the local machine. Note: “HOME” must be in all capitals. R.home: This shows the location where R is installed on your computer. b. Accessing a system file The example data set is available in the ‘extdata’ folder of the ‘LandGenCoures’ package. This is how we import the data in the Week 11 Worked Example: gen &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;wolf_geno_samp_10000.csv&quot;, package = &quot;LandGenCourse&quot;), row.names=1) dim(gen) ## [1] 94 10000 Let’s unpack this code. What does the function system.file do? Let’s compare it to R.home. R.home() ## [1] &quot;/Library/Frameworks/R.framework/Resources&quot; system.file() ## [1] &quot;/Library/Frameworks/R.framework/Resources/library/base&quot; So, system.file goes to the R home and, without additional arguments, locates the folder ‘library’ with the subfolder ‘base’. What happens when we add the arguments? system.file(package = &quot;LandGenCourse&quot;) ## [1] &quot;/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourse&quot; system.file(&quot;extdata&quot;, &quot;wolf_geno_samp_10000.csv&quot;, package = &quot;LandGenCourse&quot;) ## [1] &quot;/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourse/extdata/wolf_geno_samp_10000.csv&quot; Compare the paths to those from the previous chunk! When we specify the package argument, system.file modifies the path to the location where the package is stored. The remaining, unnamed arguments are interpreted as a path and added at the end: within package ‘LandGenCourse’, go to folder ‘extdata’ and locate file “wolf_geno_samp_10000.csv”. This is pretty cool! To be honest, I have no clue where such files are stored, and the absolute path would be different anyways on my Mac and on my Windows machine. c. File manipulation We can use R’s file manipulation functions to do things with the file before even importing the data. Let’s check the file size: myFile &lt;- system.file(&quot;extdata&quot;, &quot;wolf_geno_samp_10000.csv&quot;, package = &quot;LandGenCourse&quot;) file.size(myFile) ## [1] 2078748 cat(&quot;File size: &quot;, file.size(myFile) / 10^6, &quot; MB&quot;) ## File size: 2.078748 MB The number returned is in byte, hence we divide by 1 million (10^6) to get megabyte (MB). Other useful functions include (see help file: ‘?files’): file.create: creates file, truncates name if name already exists file.exist: checks whether a file with this name exists at location file.remove: attempts to delete file file.rename: attempts to rename file file.append: attempts to append one file to another file.copy: copies file ‘from’ ‘to’ (set ‘overwrite=TRUE’ to allow overwriting an existing file) download.file: download a file from the internet unzip: unzip a zip archive (there is also a function zip, also untar and tar for tar archives) dir: list content of folder (=directory) dir.exist: check whether a folder with this name exists at location dir.create: creates a folder (does not work when knitting R Notebook!) What other files are available? The function dir lists all files and subfolders in a specific folder. The question then is which folder to specify? Question: What do you think the following commands will return? dir() dir(here::here()) dir(system.file(package = “LandGenCourse”)) dir(system.file(“extdata”, package = “LandGenCourse”)) dir(myFile) Give it a try: dir() ## [1] &quot;radish_tutorial_fig1A.png&quot; &quot;radish_tutorial_fig1B.png&quot; ## [3] &quot;WE12_Fig1.PNG&quot; &quot;WE13_Fig1.png&quot; ## [5] &quot;WE9_Fig1.png&quot; &quot;WE9_Fig2.png&quot; ## [7] &quot;Week0_BasicR.Rmd&quot; &quot;Week0_Graphics.Rmd&quot; ## [9] &quot;Week1_vignette.Rmd&quot; &quot;Week10_bonus_vignette.Rmd&quot; ## [11] &quot;Week10_vignette.Rmd&quot; &quot;Week11_vignette.Rmd&quot; ## [13] &quot;Week12_vignette.Rmd&quot; &quot;Week13_vignette.Rmd&quot; ## [15] &quot;Week14_vignette.Rmd&quot; &quot;Week2_bonus_vignette.Rmd&quot; ## [17] &quot;Week2_vignette.Rmd&quot; &quot;Week3_vignette.Rmd&quot; ## [19] &quot;Week4_vignette.Rmd&quot; &quot;Week5_vignette.Rmd&quot; ## [21] &quot;Week6_vignette.Rmd&quot; &quot;Week7_bonus_vignette.Rmd&quot; ## [23] &quot;Week7_vignette.Rmd&quot; &quot;Week8_bonus_vignette.Rmd&quot; ## [25] &quot;Week8_vignette.Rmd&quot; &quot;Week9_vignette.Rmd&quot; dir(here::here()) ## [1] &quot;_book&quot; &quot;_bookdown_files&quot; ## [3] &quot;_bookdown.yml&quot; &quot;_output.yml&quot; ## [5] &quot;00-GitHub_files&quot; &quot;00-GitHub.md&quot; ## [7] &quot;00-GitHub.Rmd&quot; &quot;00-ReviewR_files&quot; ## [9] &quot;00-ReviewR.md&quot; &quot;00-ReviewR.Rmd&quot; ## [11] &quot;01-Week01_files&quot; &quot;01-Week01.md&quot; ## [13] &quot;01-Week01.Rmd&quot; &quot;02-Week02_files&quot; ## [15] &quot;02-Week02.md&quot; &quot;02-Week02.Rmd&quot; ## [17] &quot;03-Week03_files&quot; &quot;03-Week03.md&quot; ## [19] &quot;03-Week03.Rmd&quot; &quot;04-Week04_files&quot; ## [21] &quot;04-Week04.md&quot; &quot;04-Week04.Rmd&quot; ## [23] &quot;05-Week05_files&quot; &quot;05-Week05.md&quot; ## [25] &quot;05-Week05.Rmd&quot; &quot;06-Week06_files&quot; ## [27] &quot;06-Week06.md&quot; &quot;06-Week06.Rmd&quot; ## [29] &quot;07-Week07_files&quot; &quot;07-Week07.md&quot; ## [31] &quot;07-Week07.Rmd&quot; &quot;08-Week08_files&quot; ## [33] &quot;08-Week08.knit.md&quot; &quot;08-Week08.Rmd&quot; ## [35] &quot;09-Week09_files&quot; &quot;09-Week09.Rmd&quot; ## [37] &quot;10-Week10_files&quot; &quot;10-Week10.knit.md&quot; ## [39] &quot;10-Week10.Rmd&quot; &quot;11-Week11_files&quot; ## [41] &quot;11-Week11.knit.md&quot; &quot;11-Week11.Rmd&quot; ## [43] &quot;12-Week12_files&quot; &quot;12-Week12.Rmd&quot; ## [45] &quot;13-Week13_files&quot; &quot;13-Week13.Rmd&quot; ## [47] &quot;14-Week14_files&quot; &quot;14-Week14.knit.md&quot; ## [49] &quot;14-Week14.Rmd&quot; &quot;404.html&quot; ## [51] &quot;book.bib&quot; &quot;ChapterDepot.R&quot; ## [53] &quot;data&quot; &quot;downloads&quot; ## [55] &quot;header.html&quot; &quot;index.md&quot; ## [57] &quot;index.Rmd&quot; &quot;js&quot; ## [59] &quot;LandGenCourse_book.rds&quot; &quot;LandGenCourse_book.Rproj&quot; ## [61] &quot;LICENSE&quot; &quot;ListOfPackages.nb.html&quot; ## [63] &quot;ListOfPackages.Rmd&quot; &quot;my_plot.png&quot; ## [65] &quot;output&quot; &quot;README.md&quot; ## [67] &quot;render1b7c27bb24ce.rds&quot; &quot;render1e541582584a.rds&quot; ## [69] &quot;render23ec66f4ddd.rds&quot; &quot;render2fbc1ad7f46.rds&quot; ## [71] &quot;render2fd81ad75275.rds&quot; &quot;render33ed6170c2e4.rds&quot; ## [73] &quot;render363c1997640c.rds&quot; &quot;render3894c561f3.rds&quot; ## [75] &quot;render3b305196d38.rds&quot; &quot;render65e32e6edd81.rds&quot; ## [77] &quot;render65ea2d72427d.rds&quot; &quot;rendera1ac36509ec1.rds&quot; ## [79] &quot;renderb74a25ab009f.rds&quot; &quot;renderbedc508fe6b8.rds&quot; ## [81] &quot;renderf9011b67ac.rds&quot; &quot;rsconnect&quot; ## [83] &quot;search_index.json&quot; &quot;style.css&quot; ## [85] &quot;toc.css&quot; &quot;tutorials&quot; ## [87] &quot;vignettes&quot; &quot;Week1_vignette.R&quot; ## [89] &quot;Week2_bonus_vignette.R&quot; &quot;Week8_bonus_vignette.R&quot; ## [91] &quot;Workflow.nb.html&quot; &quot;Workflow.Rmd&quot; dir(system.file(package = &quot;LandGenCourse&quot;)) ## [1] &quot;data&quot; &quot;DESCRIPTION&quot; &quot;doc&quot; &quot;extdata&quot; &quot;help&quot; ## [6] &quot;html&quot; &quot;INDEX&quot; &quot;Meta&quot; &quot;NAMESPACE&quot; &quot;R&quot; ## [11] &quot;rstudio&quot; dir(system.file(&quot;extdata&quot;, package = &quot;LandGenCourse&quot;)) ## [1] &quot;BashExample.sh&quot; &quot;Colortable_LULC.csv&quot; ## [3] &quot;CSF_network.csv&quot; &quot;dataNm1.str&quot; ## [5] &quot;dataNm10.txt&quot; &quot;dataNm2.txt&quot; ## [7] &quot;dModels.rds&quot; &quot;EnvironmentalData_8pred.csv&quot; ## [9] &quot;ExcelTable.png&quot; &quot;Frogs_diversity_allpops.csv&quot; ## [11] &quot;Index_of_functions_formatted.pdf&quot; &quot;myNotebook.Rmd&quot; ## [13] &quot;panel.cor.r&quot; &quot;Patch_XY_Dianthus.csv&quot; ## [15] &quot;pulsatilla_genotypes.csv&quot; &quot;pulsatilla_momVariables.csv&quot; ## [17] &quot;pulsatilla_population.csv&quot; &quot;ralu_coords_allpops.csv&quot; ## [19] &quot;ralu_dc.csv&quot; &quot;RALU_Dps.csv&quot; ## [21] &quot;ralu_loci_allpops.csv&quot; &quot;RALU_Site.csv&quot; ## [23] &quot;ralu.loci.csv&quot; &quot;ralu.rasters.rds&quot; ## [25] &quot;RCommands.docx&quot; &quot;supplemental_R_functions.R&quot; ## [27] &quot;Testfile.nb.html&quot; &quot;Testfile.Rmd&quot; ## [29] &quot;WE12_Fig1.PNG&quot; &quot;WE13_Fig1.png&quot; ## [31] &quot;WE9_Fig1.png&quot; &quot;WE9_Fig2.png&quot; ## [33] &quot;Wetlands.csv&quot; &quot;wolf_geno_samp_10000.csv&quot; ## [35] &quot;WWP_environmental_data.txt&quot; &quot;WWP_phenotype_data.txt&quot; ## [37] &quot;WWP_SNP_genotypes.txt&quot; dir(myFile) ## character(0) dir(): the content of the last folder to which you navigated. dir(here::here()): the content of your project folder. dir(system.file(package = \"LandGenCourse\")): the content of the package. dir(system.file(\"extdata\", package = \"LandGenCourse\")): the content of the folder ‘extdata’ in the package. dir(myFile): nothing (this is a file, not a folder). 3. Benchmarking file import and export options See also Chapter 5 in “Efficient R Programming”: https://csgillespie.github.io/efficientR/input-output.html a. Benchmark methods for importing csv files The file ‘myFile’ has 2MB and thus a reasonable size to compare the speed of different import and export functions. Let’s benchmark the function read.csv used in Week 11. We use the function microbenchmark from the package microbenchmark to compare the speed of four different functions that can import a ‘csv’ file. Note: Here we execute each function only once to save time, typically you would set times = 10 or so. Also, read_csv will print a warning about a missing column name. This is because the first column here contains the row names and does not have a column name. We’ll ignore this here, as we can use the first column as an example to compare how character data are being imported. x = myFile microbenchmark::microbenchmark(times = 1, unit = &quot;ms&quot;, read.csv(x), readr::read_csv(x, show_col_types = FALSE), data.table::fread(x), rio::import(x)) ## Warning in microbenchmark::microbenchmark(times = 1, unit = &quot;ms&quot;, read.csv(x), ## : less accurate nanosecond times to avoid potential integer overflows ## Unit: milliseconds ## expr min lq mean ## read.csv(x) 2073.90472 2073.90472 2073.90472 ## readr::read_csv(x, show_col_types = FALSE) 1764.02828 1764.02828 1764.02828 ## data.table::fread(x) 132.43299 132.43299 132.43299 ## rio::import(x) 89.93768 89.93768 89.93768 ## median uq max neval ## 2073.90472 2073.90472 2073.90472 1 ## 1764.02828 1764.02828 1764.02828 1 ## 132.43299 132.43299 132.43299 1 ## 89.93768 89.93768 89.93768 1 Would it be faster if we first loaded the packages so that we could call the functions directly? Note: you can list several independent commands on the same line by separating the with a semi-colon ‘;’. Also, the chunk setting message=FALSE here suppresses the warning message from read_csv. library(readr); library(data.table); library(rio); library(microbenchmark) microbenchmark(times = 1, unit = &quot;ms&quot;, read.csv(x), read_csv(x, show_col_types = FALSE), fread(x), import(x)) ## Unit: milliseconds ## expr min lq mean ## read.csv(x) 2091.23009 2091.23009 2091.23009 ## read_csv(x, show_col_types = FALSE) 1594.94305 1594.94305 1594.94305 ## fread(x) 79.47608 79.47608 79.47608 ## import(x) 82.28983 82.28983 82.28983 ## median uq max neval ## 2091.23009 2091.23009 2091.23009 1 ## 1594.94305 1594.94305 1594.94305 1 ## 79.47608 79.47608 79.47608 1 ## 82.28983 82.28983 82.28983 1 Yes, the import was faster when the packages were already loaded. Overall fread and import were in the order of 50 times faster than read.csv and read_csv! The two had practically the same speed, which is little surprising: for ‘csv’ files, import uses the function fread. The beauty of import is that it can handle a wide range of file types (and the list keeps growing): csv, xls, xlsx, html, xml, json, feather, R, RData, rda, rds, psv, tsv, sas7bdat, xpt, sav, dta, xpt, por, rec, mtp, syd, dbf, arff, dif, fwf, csv.gz, CSVY, fst, mat, ods, yml, as well as Fortan files and clipboard imports (Mac and Windows). It recognizes the file type from the extension and uses an appropriate import function. Note: there is also a fuction Import in the car package that similarly aims to provide an easy way to import various file formats. However, car::Import can be very slow (slower than ‘read.csv’), whereas rio::import is fast. b. Check handling of character data The functions differ not only in their speed but also in how they handle text data (character or factor?), missing values etc. The first column in ‘myFile’ is an ID variable that should be used as row names. Let’s compare what the four methods did with this. The following code determines, for each import method, the class of the first column (IDs), and the class (or classes) of the resulting object. Note: here we use double square brackets to subset the first column. Strictly speaking, we interpret ‘gen’ as a list of vectors. With a data.frame, we could also access the first column by gen[,1]. However, this would not return what we want for ‘tbl’ of ‘data.table’ objects. Always double check. gen &lt;- read.csv(x); c(class(gen[[1]]), class(gen)) ## [1] &quot;character&quot; &quot;data.frame&quot; gen &lt;- read_csv(x, show_col_types = FALSE); c(class(gen[[1]]), class(gen)) ## [1] &quot;character&quot; &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; gen &lt;- fread(x); c(class(gen[[1]]), class(gen)) ## [1] &quot;character&quot; &quot;data.table&quot; &quot;data.frame&quot; gen &lt;- import(x); c(class(gen[[1]]), class(gen)) ## [1] &quot;character&quot; &quot;data.frame&quot; The function read.csv interprets any text as ‘factor’, the other functions use ‘character’ as default. Always double check! All of these functions have optional arguments for specifying how each column how it should be interpreted (see help files). With the functions fread and import, you can set the argument ‘stringsAsFactors = TRUE’ to import all text data as factors. c. Binary files Binary files are not readable by users (or other software) but provide an efficient way of storing data. Let’s compare file size and input/output speed between text files (csv) and different types of binary files (RData, rds, feather). We’ll also export the ‘csv’ file so that we have it in the same location. First we make sure an output folder exists in the R project: if(!dir.exists(paste0(here::here(),&quot;/output&quot;))) dir.create(paste0(here::here(),&quot;/output&quot;)) gen &lt;- import(myFile) export(gen, file.path(here::here(), &quot;output&quot;, &quot;gen.csv&quot;)) save(gen, file=file.path(here::here(), &quot;output&quot;, &quot;gen.RData&quot;)) saveRDS(gen, file=file.path(here::here(), &quot;output&quot;, &quot;gen.rds&quot;)) export(gen, file=file.path(here::here(), &quot;output&quot;, &quot;gen.feather&quot;)) c(csv=file.size(file.path(here::here(), &quot;output&quot;, &quot;gen.csv&quot;)), RData=file.size(file.path(here::here(), &quot;output&quot;, &quot;gen.RData&quot;)), rds=file.size(file.path(here::here(), &quot;output&quot;, &quot;gen.rds&quot;)), feather=file.size(file.path(here::here(), &quot;output&quot;, &quot;gen.feather&quot;)))/10^6 ## csv RData rds feather ## 2.002491 0.352069 0.352041 3.681058 The ‘csv’ file is 2 MB (first row). The R binary files ‘RData’ and ‘rds’ are much smaller! In contrast, the ‘feather’ file (last row) is twice as large here than the ‘csv’ file, and more than 10 times larger than ‘rds’! Let’s benchmark the import again. We can use the function import for all of them. This is so fast that we can actually do it 10 times. microbenchmark(times = 10, unit = &quot;ms&quot;, csv= rio::import(file.path(here::here(), &quot;output&quot;, &quot;gen.csv&quot;)), RData=rio::import(file.path(here::here(), &quot;output&quot;, &quot;gen.RData&quot;), trust=TRUE), rds=rio::import(file.path(here::here(), &quot;output&quot;, &quot;gen.rds&quot;), trust=TRUE), feather=rio::import(file.path(here::here(), &quot;output&quot;, &quot;gen.feather&quot;))) ## Unit: milliseconds ## expr min lq mean median uq max ## csv 104.12069 104.79666 106.58840 106.90076 107.22988 109.69181 ## RData 16.93124 17.30704 17.77163 17.58394 18.21482 19.20173 ## rds 16.73419 16.89770 17.45076 17.20237 18.14775 18.57882 ## feather 3137.09229 3199.62942 3558.84196 3501.99768 3879.64985 4221.74134 ## neval cld ## 10 a ## 10 a ## 10 a ## 10 b Look at the column ‘mean’. Importing any of the binary files was at least twice as fast as importing the ‘csv’ file with the underlying function fread (which was already 50 times faster than read.csv). Here’s my recommendation for saving R objects/data efficiently: If object is not in tabular form: rds (can store any R object) If storage space is most important: rds If portability with Python is important: feather If being able to read text file is important: csv Note: the developer of feather does not recommend using it for long-term data storage since its stability with future updates to R or Python can’t be guaranteed: https://github.com/wesm/feather/issues/183 Why ‘rds’ and not ‘RData’? In practice, the main advantage of ‘rds’ is convenience when importing data. with ‘readRDS’, you can directly assign the imported data to an object, and thus choose the object name. with ‘load’, you have to do this in two steps. When using ‘load’, the loaded object will inherit the name from the object that was stored. # Let&#39;s delete any copy of &#39;gen&#39; from the workspace: rm(gen) # Create object &#39;myData&#39; in a single step from &#39;rds&#39; file: myData &lt;- readRDS(file.path(here::here(), &quot;output&quot;, &quot;gen.rds&quot;)) # Two steps when importing &#39;RData&#39;: first, load the stored object: load(file.path(here::here(), &quot;output&quot;, &quot;gen.RData&quot;)) # then assign to the new object &#39;myData&#39;: myData &lt;- gen Note that when you use load, the object name is NOT taken from the file name! This means that you may not know what object you are loading, if the object and file names are different. Let’s test this. Here we save the object ‘gen’ in file ‘gen2.RData’, then load it. # Export &#39;gen&#39; to a file with a different name &#39;gen2.RData&#39;: save(gen, file=file.path(here::here(), &quot;output&quot;, &quot;gen2.RData&quot;)) rm(gen) # Load: load(file.path(here::here(), &quot;output&quot;, &quot;gen2.RData&quot;)) # What is the name of the loaded object? exists(&quot;gen&quot;) ## [1] TRUE exists(&quot;gen2&quot;) ## [1] FALSE We see that an object ‘gen’ exists (TRUE), but an object ‘gen2’ does not exist (FALSE). The name of the loaded object is thus ‘gen’. d. Should you save your R workspace? When you close RStudio, you may be asked whether you want to save your workspace. What happens when you do this, and should you do so? When you save your workspace, all objects from the workspace are saved in one binary file ‘.RData’ (unless you provide a name, like ‘myWorkspace.Rdata’). This may result in a large file! There are other downsides: you may accidentally overwrite an object. And your code will not be portable because it depends on a copy of the workspace. The general recommendation is to NOT save your workspace, but save your dataset and your R scripts (or Notebooks). This means that you can always recreate all the objects needed. Also, in the vein of reproducible research, do not save multiple copies of your data set. Instead: Save the raw data (with the original file name and extension, e.g. if you downloaded it from the internet - this will help identify the source) Save all the data manipulation steps (such as checking for errors, excluding rows, recoding variables, etc.) in an R Notebook and document them (what you are doing, why and when). Save the ‘clean’ dataset (result from data manipulation), preferably as a binary file (especially if it is large). Keep only one copy (you can always recreate it). Save your data analysis in another R Notebook (or multiple) that starts with importing the ‘clean’ dataset. If your code runs quickly, this is sufficient to recreate your results. If your code takes a lot of computation time, you may want to export the results (see above). Backup your data and scripts (R Notebooks)! Use version control for your script (R Notebooks)! See Chapter 0 video ‘Version Control 101’. The simplest is to include version numbers in the file name: ‘myNotebook_v7.Rmd’. A better way is to use e.g. GitHub. e. Compile your functions A recommended way of keeping your code tidy is to write functions for anything you will do more than once. Collect your functions in a separate file (R script), e.g. myFunctions.R. At the beginning of your R Notebook, source the R script with the functions with source(\"myFunctions.R\"). This will make your code much shorter and thus easier to read. If you need to change some code inside your function, you only have to change it in one place, hence there is less risk of mistakes. Use some kind of version control for your functions file so that you can go back to an older version, and you always know which is the current version. To further speed up your code, you can compile your function with ‘cmpfun’: myFunction &lt;- function() { sum(rnorm(1000))/1000 } myFunction.cmp &lt;- compiler::cmpfun(myFunction) microbenchmark::microbenchmark(myFunction(), myFunction.cmp()) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## myFunction() 29.315 30.2785 42.55103 30.9755 31.4675 1176.741 100 a ## myFunction.cmp() 29.479 30.2170 30.89145 30.6885 31.1805 46.043 100 a Question: Which of the following times are most different between the uncompiled and the compiled versions of this simple function? min: minimum time across ‘neval’ replicates Quartiles: ‘lq’ = lower quartile (25%), ‘median’, ‘uq’ = upper quartile (75%) mean: mean time across ‘neval’ replicates max: maximum time across ‘neval’ replicates In this case, compiling mainly reduced the duration of the longest 25% runs (with longer times than the 75% quartile), which brought down the mean processing time. 4. Profiling your code a. Named chunks An simple way to identify parts of your code that may be slow is to: Name each chunk in your R Notebook Knit the notebook Monitor the R markdown pane while the notebook is knitted: which chunks seem to take a lot of time? To name a chunk, click on the wheel symbol at the top right of the grey chunk area and enter a one-word name. Here’s an example of a named chunk: the name ‘myChunkName’ has been added in the curly brackets {r, myChunkName}. You can add a name manually in the same way. More generally, this is where chunk options are added in the R Notebook. Here’s a long list of chunk options: https://yihui.name/knitr/options/ b. Profiling some lines of code RStudio has a built-in menu for profiling. Check it out: Select the five lines of code below, from dat &lt;- data.frame( until lm(y ~ x, data=dat) In RStudio’s menu, click on ‘Profile’ &gt; ‘Profile Selected Line(s)’. You can achieve the same with a call to the funciton ‘profvis’ of the ‘profvis’ package: profvis::profvis({ dat &lt;- data.frame( x = rnorm(5e5), y = rnorm(5e5)) mean(dat$x) with(dat, cor(x, y)) lm(y ~ x, data=dat) }) The results will be opened in a ‘Profile’ tab. The upper part has two tabs (the lower part is less intuitive to interpret, we’ll ignore it here): Flame Graph: this plots horizontal bars indicating the amount of memory and time used by each line of code, in the original order of the code. Data: code is sorted by resource use, with the most ‘costly’ line qqnorm at the top of the list. You can click on the triangle before each line to see more detail. The results may depend on the speed of your computer. Check the sample interval at the bottom of the ‘Profile’ tab: time was estimated by observing every 10 milliseconds what the computer was doing. Some lines lines must have been too fast to be recorded. c. Converting an R Notebook to an R script Unfortunately, we can’t profile an entire R Notebook (as far as I know). However, we can extract the R code as a script file, then profile the script file. Copy an R Notebook file (Testfile.Rmd) to the downloads folder. This is just so that we have an example of an .Rmd file to extract the code from. file.copy(from=system.file(&quot;extdata&quot;, &quot;Testfile.Rmd&quot;, package = &quot;LandGenCourse&quot;), to=file.path(here::here(), &quot;downloads&quot;, &quot;Testfile.Rmd&quot;)) ## [1] FALSE Extract the R code from the R Notebook and save it in a script file Testfile.R in the downloads folder. You may adapt the infile and outfile to extract the code from any R Notebook saved in your project. Let’s open the two files and compare them. Note that file.show opens a simple text version, whereas file.edit shows the colored versions commonly displayed by the RStudio editor. Question: Compare the two files (they should be open, check the tabs of the source pane). How are the chunks from the Notebook file divided in the new script file? What happened to the titles and text? What about the header information? d. Profiling an R script We can now use the function source to read and execute the code in the R script. We use the function Rprof to start and stop the profiling. With the function summaryRprof, we get a summary by function. First, let’s check the total time spent executing the code, and the sample interval (in seconds). Note that profiling works in this way: as the code is executed, R checks at regular time intervals which function is being executed at that very moment. The summaryRprof function then tabulates the number of intervals, and thus the time used, by function. #summaryRprof(here::here(&quot;downloads/Rprof.out&quot;))[c(&quot;sampling.time&quot;, &quot;sample.interval&quot;)] The attribute $by.total only lists the top-level functions called (not any functions called internally by that top-level function). #summaryRprof(here::here(&quot;downloads/Rprof.out&quot;))$by.total Note: The functions are listed by total time, in decreasing order. The two functions eval and source, are related to evaluating (running) the code from the sourced R script file. total.time: total time spent executing the function (including functions called by it) total.pct: (ignore this) percent time spent executing the function (including functions called by it) self.time: total time spent executing the function itself (excluding functions called by it) self.pct: percent time spent executing the function itself (excluding functions called by it) If you need to see a summary with more detail, $by.self lists each function used, even internal functions. If you want to profile memory use rather than processing time, see here: https://developer.r-project.org/memory-profiling.html 5. Creating and executing a Bash R script a. Run R script directly in the Terminal Now that we have a stand-alone R script ‘myScript.R’, we can run it from the command line in the terminal (shell). After navigating to the correct folder, type: Rscript myScript.R This will source the file and execute the R code. Numerical output will be printed in the termminal. Here, a random number should be returned. Best include code in your R script to export any R objects, data tables or figures that you want to retain. These will be saved in the same folder as the R script (unless you specify file paths). b. Create a Bash R script If you want to run your code on a node or cluster, you may need to take this one step further and include the R code in a bash Rscript. In a bash script, you can add bash commands that govern resource use to submit a job to a node or cluster. Bash scripts are the way of giving instructions to the scheduler of the cluster (e.g. SLURM) for how to manage input and output files. To execute our R script ‘myScript.R’ as a Bash script, we need to add a few lines. The ‘shebang’ line #!/bin/bash that tells the computer that this is a Bash script, and where to find Bash. Note that here the hashtag symbol does NOT mean that the line is commented out (this line is Unix code, not R code). The line R --slave &lt;&lt; EOF that declares the rest of the file (until EOF) as R code. The end of file EOF marker. Let’s modify the previous code and write it into a Bash file. As an additional challenge, our code contains two figures, which won’t be written anywhere unless we change the code to write them into a file: We create a graphics file ‘my_plot.png’ that is 800 pixels wide and 400 pixels high. With par(mfrow=c(1,2)), we specify that the two plots should be plotted side-by-side. Then we create the plots. We close the graphics device (png file) with dev.off. Note: We use single quotes for the file name here, ‘my_plot.png’, as they are nested within a set of double quotes. R pretty much considers single and double quotes as synonyms, which allows us to nest them either way: ‘““’ or”’’“. myPath &lt;- file.path(here::here(), &quot;output/myBashScript.sh&quot;) fileConn &lt;- file(myPath) writeLines(c(&quot;#!/bin/bash&quot;, &quot;R --slave &lt;&lt; EOF&quot;, &quot;x &lt;- rnorm(100)&quot;, &quot;mean(x)&quot;, &quot;png(&#39;my_plot.png&#39;, height = 400, width = 800)&quot;, &quot;par(mfrow=c(1,2))&quot;, &quot;hist(x)&quot;, &quot;qqnorm(x)&quot;, &quot;dev.off()&quot;, &quot;EOF&quot;), fileConn) close(fileConn) file.show(myPath) c. Executing a Bash R script On Mac / Unix / Linux, this is straight-forward: Open terminal: From the RStudio menu, select ‘Tools’ &gt; ‘Terminal’ &gt; ‘New Terminal’. This will open an Terminal tab in RStudio. Alternatively, you could select ‘Tools’ &gt; ‘Shell’ to open a Shell in a new window outside RStudio. Check the prompt: it should start with the name of your computer, then a colon, then the name of your project folder, then your use name followed ‘$’. Navigate to Bash file: Enter ls to list the content of the project folder. Enter cd output to change directory to the subfolder ‘output’. Repeat ls to list the content of the ‘output’ folder. The Bash script ‘myBashScript.sh’ should be listed there. Execute Bash file: Enter chmod +x myBashScript.sh to change file permission for the script. Enter ./myBashScript.sh to execute the script. Find output: The output from mean(x) is printed in the terminal, it should look like this: [1] -0.07731751. This is followed ‘null device’ and the number 1, which tells us that a graphics device has been closed. Enter ls to list the content of the project folder. The graphics file ‘my_plot.png’ should now be listed. Use R again to open the graphics file (the code here first checks whether the file exists): myPNG &lt;- file.path(here::here(), &quot;output/my_plot.png&quot;) if(file.exists(myPNG)) { file.show(myPNG) } d. Moving to a node or cluster? The example bash file and advice in this section have been provided by Hossam Abdel Moniem, thanks! Here’s an annotated example of a bash file that contains instructions for submitting a job to a single node (a single machine with multiple/many cores). Note: A copy of the file ‘BashExample.sh’ should also have been copied into the downloads folder inside your project folder. writeLines(readLines(system.file(&quot;extdata&quot;, &quot;BashExample.sh&quot;, package = &quot;LandGenCourse&quot;))) ## #!/bin/bash ## ## #SBATCH --nodes=1 # Number of Nodes ## #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL) ## #SBATCH --mail-user=myEmail@gmail.com # Where to send mail ## #SBATCH --ntasks=1 # Run a single task ## #SBATCH --cpus-per-task=24 # Number of CPU cores per task ## #SBATCH --mem-per-cpu=8000 # allocated memory for the task ## #SBATCH -p nodename # name of cluster node ## #SBATCH --requeue # Allow the job to be requeued ## #SBATCH -e myJob.err # File to which STDERR will be written ## #SBATCH -o myJob.out # File to which STDOUT will be written ## #SBATCH -J myJob # Job name ## ## module load R/MS3.4.1 # call a preinstalled module(program) on the cluster ## ## Rscript connect_calc_25.R # Run the R script in bash ## Instead of including the R code directly in the bash file, the last line here executes an R script with the Rscript command. Notice the second-last line. Obviously, on the node, R and any relevant packages need to be pre-installed. Different users may need different configurations (different packages or versions) installed, hence each installation has a name, which needs to be specified in the bash script. Note: Make sure that all packages that you need (and their dependencies), as well as the package unixtools, have been installed on the node or cluster (i.e., they are part of the installation you will be using). Install unixtools with: install.packages(\"unixtools\",,\"http://rforge.net/\") Further reading: Bash script tutorial: https://ryanstutorials.net/bash-scripting-tutorial/ Scheduling a job with SLURM commands: https://www.rc.fas.harvard.edu/resources/documentation/convenient-slurm-commands/ e. Store your session info and package versions If you use any R packages, load them at the beginning of your script file with library. Make sure the packages are installed on the system where you will be running the Bash R script. A big issue with R is that package updates may make your code break. At least at the end of any project (such as the analyses for a manuscript), save your session information. Here we use the function session_info from the devtools package (preferred over the R base function sessionInfo). We store the information as an object, ‘Session’, of class ‘session_info’ that has two list elements: platform: Information about R version, your computer’s operating system, etc. packages: List of all packages, including their version, installation date and repository (CRAN, Github etc.). All packages that are currently loaded will be marked with an asterisk. Platform: Session &lt;- devtools::session_info() Session$platform ## setting value ## version R version 4.5.2 (2025-10-31) ## os macOS Sequoia 15.7.3 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Europe/Zurich ## date 2026-01-21 ## pandoc 3.4 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/ (via rmarkdown) ## quarto 1.6.42 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/quarto Packages: here we display only the first six lines, as the list may be long. head(Session$packages) ## package * version date (UTC) lib source ## abind 1.4-8 2024-09-12 [1] CRAN (R 4.5.0) ## ade4 1.7-23 2025-02-14 [1] CRAN (R 4.5.0) ## adegenet 2.1.11 2025-02-06 [1] CRAN (R 4.5.0) ## ape 5.8-1 2024-12-16 [1] CRAN (R 4.5.0) ## arrow 22.0.0.1 2025-12-23 [1] CRAN (R 4.5.2) ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.5.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library Exporting the session information is a bit tricky because ‘Session’ is not in tabular format, and what we want to export is the formatted output, not the object itself. Also, we will add a time stamp to the file name. This achieves two goals: avoid overwriting earlier files, and keep a record of the date of the session information. capture.output: captures the output of a function (here: devtools::session_info()) writeLines: writes the captured output into a file, here a text file. Sys.Date: returns the current date. Here we specify the format as “%Y-%m-%d”, i.e., ‘Year-month-day’. More advanced ways for handling the problem of package versions to make sure you can run your code in the future without compatibility issues include: Package ‘packrat’: bundle all your current package version. This can become quite large. Package ‘checkpoint’: access daily snapshots of CRAN for any given date. Note that checkpoint only covers packages from CRAN, not other repositories like GitHub. More on the topic: https://timogrossenbacher.ch/2017/07/a-truly-reproducible-r-workflow/ f. Potential Windows issues Check that Bash is installed and ready to use with RStudio: In the RStudio menu, select ‘Tools’ &gt; ‘Global options’ &gt; ‘Terminal’. Make sure some form of Bash (e.g., Git Bash) is listed under ‘Shell: New terminals open with:’. If this does not work, try installing ‘Git for Windows’, which will also install Bash: http://neondataskills.org/setup/setup-git-bash-R Here’s a detailed multi-part tutorial on running R from the command line, with R scripts and Bash R scripts, with some additional information on Windows: Introduction: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/01-introduction.Rmd Batch mode: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/02-batch-mode.Rmd Executing R scripts: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/03-rscript.Rmd Bash R scripts: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/04-shell-script.Rmd Redirection: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/05-redirection.Rmd 6. Parallelizing code Note that the following issues may create problems when developing Bash R scripts on Windows that you want to run e.g. on a Linux cluster or another Unix-type system; File paths are different, and system files are stored in a different place. End-of-line symbols are different a. Replace ‘lapply’ by ‘mclapply’ with package ‘parallel’ Note: while this will run on a Windows without causing an error, it will only be faster on Mac / Unix. With the package ‘parallel’, it is really easy to use all cores of your local machine (as long as you are on a Mac / Unix / Linux system). Let’s check the number of cores available: library(parallel) detectCores() ## [1] 10 Question: How many cores does your machine have? Note: Here we check whether the operating system is ‘Windows’, in which case we set nCores = 1. This means that we will use all cores on Mac or Linux, but only one core on Windows. This is to avoid problems on Windows machines. nCores &lt;- detectCores() if(Sys.info()[[&#39;sysname&#39;]]==&quot;Windows&quot;) nCores = 1 nCores ## [1] 10 Code your analysis with lapply (and related functions). Replace lapply by mclapply (and related functions). Use the argument mc.cores=detectCores() to automatically detect the number of cores in your machine. NOTE: April 2021: package build does not work with multiple cores, changing nCores to 1. x &lt;- gen[,-1] m1 &lt;- lapply(x, mean, na.rm=TRUE) #m2 &lt;- mclapply(x, mean, na.rm=TRUE, mc.cores=nCores) # Use this line when running the code yourself m2 &lt;- mclapply(x, mean, na.rm=TRUE, mc.cores=1) # Replace this line with the previous line Let’s benchmark four ways of calculating the mean of each column in our example data set ‘gen’ with 94 rows and 10,000 columns (SNPs): Method 1: The dedicated function colMeans from base R. Method 2: A for loop. Method 3: Vectorization with lapply Method 4: Multi-core with mclapply. method1 &lt;- function(x) {colMeans(x, na.rm=TRUE)} method2 &lt;- function(x) {for(i in 1:ncol(x)) mean(x[,i], na.rm=TRUE)} method3 &lt;- function(x) {lapply(x, mean, na.rm=TRUE)} #method4 &lt;- function(x) {mclapply(x, mean, na.rm=TRUE, mc.cores=nCores)} # Use this line when running the code yourself method4 &lt;- function(x) {mclapply(x, mean, na.rm=TRUE, mc.cores=1)} # Replace this line with the previous line microbenchmark::microbenchmark(times = 10, unit = &quot;ms&quot;, method1(x), method2(x), method3(x), method4(x)) ## Unit: milliseconds ## expr min lq mean median uq max neval ## method1(x) 26.86086 27.51916 28.33970 28.00536 29.14087 30.16805 10 ## method2(x) 153.91236 160.35403 185.18076 166.44413 217.43903 230.34165 10 ## method3(x) 40.91747 41.06015 54.35233 43.15879 49.61135 100.57525 10 ## method4(x) 40.76876 41.44592 43.36019 42.54568 43.36632 49.56289 10 ## cld ## a ## b ## c ## a c Question: Compare the mean Which method was the fastest? Can you explain this? Which method was slowest? Was mclapply faster than lapply in this example? Note: Obviously you might only expect to see a gain in speed if nCores &gt; 1. b. Replace ‘for’ by ‘foreach’ with package ‘doParallel’ On Windows, it is easier to use the package ‘doParallel’ with the function ‘foreach’. Here’s a detailed introduction: http://127.0.0.1:26758/help/library/doParallel/doc/gettingstartedParallel.pdf. Use makeCluster to specify the number of cores to be used. The default is half the number of cores. Check number of clusters by printing cl. Register the cluster with registerDoParallel. If you omit this step, the code will not use parallel computing. The following code is commented out to avoid problems when knitting the Notebook. You may uncomment and run it. library(doParallel) ## Loading required package: foreach ## Loading required package: iterators #cl &lt;- makeCluster(2) #cl #registerDoParallel(cl) Now we adapt the code in two steps: The code is a little different with foreach than with for, as we use a pipe-like syntax with %do%, which means, for each value of i, do the following. Thus, %do% is only a pipe operator, it does not result in parallelisation yet. To make this parallel, replace %do% by %dopar%. m1 &lt;- for(i in 1:ncol(x)) mean(x[,i], na.rm=TRUE) m2 &lt;- foreach(i = 1:ncol(x)) %do% (mean(x[,i], na.rm=TRUE)) #m3 &lt;- foreach(i = 1:ncol(x)) %dopar% (mean(x[,i], na.rm=TRUE)) Let’s benchmark this again. We’ll only do 3 replicates this time (uncomment before running this code). #method1 &lt;- function(x) {colMeans(x, na.rm=TRUE)} #method2 &lt;- function(x) {for(i in 1:ncol(x)) mean(x[,i], na.rm=TRUE)} #method3 &lt;- function(x) {lapply(x, mean, na.rm=TRUE)} #method4 &lt;- function(x) {mclapply(x, mean, na.rm=TRUE, mc.cores=nCores)} #method5 &lt;- function(x) {foreach(i = 1:ncol(x)) %do% (mean(x[,i], na.rm=TRUE))} #method6 &lt;- function(x) {foreach(i = 1:ncol(x)) %dopar% (mean(x[,i], na.rm=TRUE))} #microbenchmark::microbenchmark(times = 3, unit = &quot;ms&quot;,method1(x), method2(x), method3(x), method4(x), method5(x), method6(x)) Whether parallelisation is faster depends on the type of task and on your system. In this case, both versions that used parallelisation were actually slower than the sequential code (at least on my system). Of course, this will not always be the case, it may depend on: What type and size of computational task you are running. How many nodes you can use in parallel. The initial ‘cost’ of coordinating the task among cores may not be worth it if you only have two cores to use anyways, but with 20 cores, the gain will be higher. Whether all cores are in the same node, or whether you distribute the work among multiple nodes (this will increase the cost of coordination, but potentially also give you access to many more cores). "],["Week9.html", "12 Lab 9: Population Structure", " 12 Lab 9: Population Structure This week’s lab is the first one in the Advanced Topics section. We will explore different ways of assessing spatial genetic structure and delineating genetic populations. Worked Example Note: The worked examples in the Advanced Topics section require the R skills developed in the Basic Topics (Weeks 1 - 8). "],["WE_9.html", "12.1 Worked Example", " 12.1 Worked Example Sean Schoville 1. Overview of Worked Example a. Goals The goals of this lab are to: Assess how patterns of genetic variation can be used to delimit natural populations. Compare methods that assess population structure. Understand how population structure can be used to interpret biogeographic history. b. Data sets All files are distributed as system files with the ‘LandGenCourse’ package (folder ‘extdata’). Simulated data using the two-island model and admixture model. SNP data from Catchen et al. 2013 Catchen et al. 2013. The population structure and recent colonization history of Oregon threespine stickleback determined using restriction-site associated DNA-sequencing. Molecular Ecology 22:1365-294X. http://dx.doi.org/10.1111/mec.12330 c. Required R packages Note: the function ‘library’ will always load the package, even if it is already loaded, whereas ‘require’ will only load it if it is not yet loaded. Either will work. require(LandGenCourse) #require(LandGenCourseData) #require(fields) #require(RColorBrewer) #require(maps) #require(mapplots) #require(here) The package ‘LEA’ is available from the ‘Bioconductor’ repository. Also, package ‘fields’ was not installed with ‘LandGenCourse’ automatically due to compatibility issues. if(!requireNamespace(&quot;fields&quot;, quietly = TRUE)) install.packages(&quot;fields&quot;, repos=&#39;http://cran.us.r-project.org&#39;) if(!requireNamespace(&quot;LEA&quot;, quietly = TRUE)) { if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;LEA&quot;) } The data are in a data package: if(!requireNamespace(&quot;LandGenCourseData&quot;, quietly = TRUE)) devtools::install_github(&quot;hhwagner1/LandGenCourseData&quot;) 2. Simulated data: 2-island model We simulated data under a classic two-island model, using the coalescent simulation program ‘ms’, developed by Richard Hudson. The program simulates a coalescent tree under various demographic models, and uses those trees to create allelic data. For those interested in using ‘ms’, the source code is available here: http://home.uchicago.edu/rhudson1/source/mksamples.html We simulated 200 haploid individuals genotyped at 100 loci. The effective mutation rate was ‘μ = 0.5’. We sampled 2 islands with 100 individuals in each. The effective migration rate was ‘Nm 2’ (‘N’ is the effective size in each of the two island, ‘m’ is the bidirectional rate of gene flow). Our ms command was as follows: ms 200 100 -t .5 -I 2 100 100 -ma x 2 2 x &gt; dataNm2.txt These raw data need to be converted in a format amenable to statistical analyses in R. a. Import data file &lt;- scan(file = system.file(&quot;extdata&quot;, &quot;dataNm2.txt&quot;, package = &quot;LandGenCourse&quot;), what =&quot;character&quot;, sep=&quot;\\n&quot;, skip = 2) genotype &lt;- NULL for(locus in 1:100){ res.locus &lt;- file[4:203] file &lt;- file[-(1:203)] genotype &lt;- cbind(genotype, as.numeric(as.factor(res.locus)))} dim(genotype) ## [1] 200 100 Now we have a new data file, genotype, loaded in R. This file contains 200 rows and 100 columns. Each row corresponds to a simulated individual. The columns code for their multi-locus genotypes. b. Perform Principal Components Analysis (PCA) Our first objective is to use ordination (Principal components analysis, or PCA) to examine population structure for the Nm = 2 data set. The R command for PCA is fairly simple and fast: pc = prcomp(genotype, scale =T) In order to visualize how the first two eigenvectors capture genotype variation, we will color each population. par(mar=c(4,4,0.5,0.5)) plot(pc$x, pch = 19, cex = 2, col = rep(c(&quot;blue2&quot;, &quot;orange&quot;), each = 100)) Question 1: Is population structure (genetic differentiation) evident? How much of the genetic variance can be explained by our first two components? To answer the second part of this question, use: summary(pc) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.77699 1.65283 1.5587 1.53452 1.48944 1.48496 1.46718 ## Proportion of Variance 0.07712 0.02732 0.0243 0.02355 0.02218 0.02205 0.02153 ## Cumulative Proportion 0.07712 0.10443 0.1287 0.15228 0.17446 0.19651 0.21804 ## PC8 PC9 PC10 PC11 PC12 PC13 PC14 ## Standard deviation 1.43774 1.41957 1.40980 1.40041 1.38541 1.37335 1.35879 ## Proportion of Variance 0.02067 0.02015 0.01988 0.01961 0.01919 0.01886 0.01846 ## Cumulative Proportion 0.23871 0.25886 0.27874 0.29835 0.31754 0.33640 0.35487 ## PC15 PC16 PC17 PC18 PC19 PC20 PC21 ## Standard deviation 1.35395 1.31441 1.3078 1.30115 1.29493 1.26846 1.26525 ## Proportion of Variance 0.01833 0.01728 0.0171 0.01693 0.01677 0.01609 0.01601 ## Cumulative Proportion 0.37320 0.39048 0.4076 0.42451 0.44128 0.45737 0.47338 ## PC22 PC23 PC24 PC25 PC26 PC27 PC28 ## Standard deviation 1.23786 1.23212 1.22814 1.20358 1.18924 1.17923 1.17139 ## Proportion of Variance 0.01532 0.01518 0.01508 0.01449 0.01414 0.01391 0.01372 ## Cumulative Proportion 0.48870 0.50388 0.51896 0.53345 0.54759 0.56150 0.57522 ## PC29 PC30 PC31 PC32 PC33 PC34 PC35 ## Standard deviation 1.16216 1.12304 1.10610 1.10157 1.09177 1.07816 1.05735 ## Proportion of Variance 0.01351 0.01261 0.01223 0.01213 0.01192 0.01162 0.01118 ## Cumulative Proportion 0.58873 0.60134 0.61357 0.62571 0.63763 0.64925 0.66043 ## PC36 PC37 PC38 PC39 PC40 PC41 PC42 ## Standard deviation 1.04793 1.02672 1.01936 1.01364 1.00225 0.99349 0.98594 ## Proportion of Variance 0.01098 0.01054 0.01039 0.01027 0.01005 0.00987 0.00972 ## Cumulative Proportion 0.67141 0.68195 0.69235 0.70262 0.71267 0.72254 0.73226 ## PC43 PC44 PC45 PC46 PC47 PC48 PC49 ## Standard deviation 0.96673 0.96357 0.9485 0.94380 0.92308 0.91570 0.90220 ## Proportion of Variance 0.00935 0.00928 0.0090 0.00891 0.00852 0.00839 0.00814 ## Cumulative Proportion 0.74160 0.75089 0.7599 0.76879 0.77731 0.78570 0.79384 ## PC50 PC51 PC52 PC53 PC54 PC55 PC56 ## Standard deviation 0.89163 0.88277 0.86488 0.84941 0.83872 0.83328 0.82304 ## Proportion of Variance 0.00795 0.00779 0.00748 0.00721 0.00703 0.00694 0.00677 ## Cumulative Proportion 0.80179 0.80958 0.81706 0.82427 0.83131 0.83825 0.84503 ## PC57 PC58 PC59 PC60 PC61 PC62 PC63 ## Standard deviation 0.81748 0.80746 0.79085 0.77042 0.76650 0.76104 0.7553 ## Proportion of Variance 0.00668 0.00652 0.00625 0.00594 0.00588 0.00579 0.0057 ## Cumulative Proportion 0.85171 0.85823 0.86448 0.87042 0.87629 0.88209 0.8878 ## PC64 PC65 PC66 PC67 PC68 PC69 PC70 ## Standard deviation 0.72978 0.7279 0.72353 0.70575 0.69818 0.68517 0.68335 ## Proportion of Variance 0.00533 0.0053 0.00523 0.00498 0.00487 0.00469 0.00467 ## Cumulative Proportion 0.89312 0.8984 0.90365 0.90863 0.91351 0.91820 0.92287 ## PC71 PC72 PC73 PC74 PC75 PC76 PC77 ## Standard deviation 0.67501 0.65756 0.63896 0.62114 0.61388 0.60925 0.60217 ## Proportion of Variance 0.00456 0.00432 0.00408 0.00386 0.00377 0.00371 0.00363 ## Cumulative Proportion 0.92743 0.93175 0.93583 0.93969 0.94346 0.94717 0.95080 ## PC78 PC79 PC80 PC81 PC82 PC83 PC84 ## Standard deviation 0.58408 0.56742 0.55817 0.54962 0.53610 0.52625 0.52442 ## Proportion of Variance 0.00341 0.00322 0.00312 0.00302 0.00287 0.00277 0.00275 ## Cumulative Proportion 0.95421 0.95743 0.96054 0.96356 0.96644 0.96921 0.97196 ## PC85 PC86 PC87 PC88 PC89 PC90 PC91 ## Standard deviation 0.50528 0.49755 0.4798 0.46734 0.45256 0.44882 0.44121 ## Proportion of Variance 0.00255 0.00248 0.0023 0.00218 0.00205 0.00201 0.00195 ## Cumulative Proportion 0.97451 0.97699 0.9793 0.98147 0.98352 0.98554 0.98748 ## PC92 PC93 PC94 PC95 PC96 PC97 PC98 ## Standard deviation 0.41786 0.41163 0.39781 0.37758 0.36923 0.36602 0.34913 ## Proportion of Variance 0.00175 0.00169 0.00158 0.00143 0.00136 0.00134 0.00122 ## Cumulative Proportion 0.98923 0.99092 0.99251 0.99393 0.99529 0.99663 0.99785 ## PC99 PC100 ## Standard deviation 0.34128 0.31336 ## Proportion of Variance 0.00116 0.00098 ## Cumulative Proportion 0.99902 1.00000 Next we would like to see how population genetic structure relates to geographic space. To this aim, we could display PC maps. A PC map is a spatial interpolation of a particular component. Let’s map PC 1. c. Create synthetic spatial coordinates (X,Y) and map them par(mar=c(4,4,0.5,0.5)) coord &lt;- cbind(sort(c(rnorm(100, -2, 1), rnorm(100, 2, 1))), runif(200)) fit &lt;- fields::Krig(coord, pc$x[,1], m=1) fields::surface(fit) points(coord, pch=19) This map predicts the value of the first principal component at each location in our study area. We observe that the study area is partitioned into two zones that correspond to the 2 clusters visible from PC 1. We have superimposed individual sample sites to see our species distribution. To check that the PC map is consistent with having 2 islands, we can examine the PC assignment vs the sampling location. Remember that, in the data sets, the 100 first individuals were sampled from island 1 and the last 100 were sampled from island 2. We compare these assignments to the PCA classification as follows. table((pc$x[,1] &gt; 0) == (1:200 &lt;= 100)) ## ## FALSE TRUE ## 1 199 In this example, we found that only one individual was not assigned to its island of origin. Well, this individual might be a migrant from the last generation. These results indicated that a very simple method based on principal component analysis can correctly describe spatial population genetic structure. Question 2: Does PCA provide an accurate description of population genetic structure when the genetic differentiation between the 2 islands is less pronounced? To answer this question, re-run the first analytical steps up to PCA for data simulated with Nm = 10 (a higher value of gene flow), which is stored in datafile “dataNm10.txt”. This was generated with the following ms command: ms 200 100 -t .5 -I 2 100 100 -ma x 10 10 x &gt; dataNm10.txt You can import the file as: file &lt;- scan(file = system.file(&quot;extdata&quot;, &quot;dataNm10.txt&quot;, package = &quot;LandGenCourse&quot;), what =&quot;character&quot;, sep=&quot;\\n&quot;, skip = 2) 3. Simulated data: 2-island model with admixture 2-island model A 2-island model is a relatively simple scenario and unlikely to capture the variation we will see in empirical studies. Will our very basic assignment method based on PCA hold up to more complex scenarios? Let’s consider a scenario where the 2 populations had been evolving for a long time under an equilibrium island model, and then their environment suddenly changed. Our 2 populations had to move to track their shifting habitat, and after these movements they come into contact in an intermediate region. This contact event resulted in an admixed population with the density of mixed individuals greater in the center of the contact zone than at the ancestral origins at the edges of the landscape. Using R and our previous simulation, a multi-locus cline that resumes this scenario can be simulated has follows. The source population data are in the file “dataNm1.str”. First we define a function for the shape of a cline: # A function for the shape of a cline sigmoid &lt;- function(x){ 1/(1 + exp(-x))} p1 &lt;- sigmoid( 0.5 * coord[,1]) Our admixed genotypes are built from a 2 island model with low gene flow (Nm=1) genotype = read.table(file = system.file(&quot;extdata&quot;, &quot;dataNm1.str&quot;, package = &quot;LandGenCourse&quot;))[,-(1:2)] admixed.genotype &lt;- matrix(NA, ncol = 100, nrow = 200) for (i in 1:100){ for (j in 1:100) admixed.genotype[i,j] = sample( c(genotype[i, j],genotype[i+100, j]), 1, prob = c(p1[i], 1 - p1[i]) )} for (i in 101:200){ for (j in 1:100) admixed.genotype[i,j] = sample( c(genotype[i - 100, j],genotype[i, j]), 1, prob = c(p1[i], 1 - p1[i]) )} res &lt;- data.frame(coord, admixed.genotype) Now our data set is the R object ‘res’. The next exercise is to apply PCA to these data to evaluate how geographical genetic variation can be captured by this approach. In comparison with the previous example where we had two geographically discrete populations, we now have a “continuous” population in a landscape. Geographical genetic variation is thus expected to be more gradual than in the previous example. Generate and examine the PC 1 map. par(mar=c(4,4,0.5,0.5)) pcA = prcomp(admixed.genotype, scale =T) plot(pcA$x, pch = 19, cex = 2, col = rep(c(&quot;blue2&quot;,&quot;orange&quot;), each = 100)) Look at the PC Map par(mar=c(4,4,0.5,0.5)) fit &lt;- fields::Krig(res, pcA$x[,1], m=1) ## Warning: ## Grid searches over lambda (nugget and sill variances) with minima at the endpoints: ## (REML) Restricted maximum likelihood ## minimum at right endpoint lambda = 0.05291003 (eff. df= 190 ) fields::surface(fit) points(res) Question 3: How does admixture change our prediction of population structure (PCA plot)? Is genomic ancestry correlated with geographical location? To answer this latter part, check the R2 and significance of statistical association between PC1 component scores and geographical position (p1): summary(lm(pcA$x[,1]~ p1)) ## ## Call: ## lm(formula = pcA$x[, 1] ~ p1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.436 -0.748 0.053 0.721 2.510 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.7765 0.1716 -22.01 &lt;2e-16 *** ## p1 7.6076 0.3117 24.40 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.048 on 198 degrees of freedom ## Multiple R-squared: 0.7505, Adjusted R-squared: 0.7492 ## F-statistic: 595.6 on 1 and 198 DF, p-value: &lt; 2.2e-16 4. Empirical data: Threespine sticklebacks The Threespine stickleback (Gasterosteus aculeatus) is a fish that has emerged as a model of rapid and parallel adaptation. Catchen et al. (2013) were interested in how populations colonize freshwater habitats in the Pacific Northwest, USA. These sticklebacks have diversified into three life history forms, one exclusively dwelling in the ocean, another being adapted to freshwater habitats, and one unusual population that can utilize both habitats. It was unclear if this one particular population (Riverbend), from a stream in central Oregon, was introduced due to unintentional human transport, and if this could be an example of rapid adaptation to freshwater from oceanic populations. Single nucleotide polymorphism data were generated using genotyping-by-sequencing, for 9 populations occupying coastal areas and inland streams. Map In this tutorial, we will analyze the genetic data generated by Catchen et al. (2013) using a few exploratory methods to quantify and visualize genetic differentiation among the stickleback populations sampled. By the end of this tutorial, hopefully you will be able to make a convincing argument for the regional origin of the recently-introduced inland stickleback population. a. Import the data data &lt;- read.table(system.file(&quot;extdata&quot;, &quot;stickleback_data.txt&quot;, package = &quot;LandGenCourseData&quot;), sep=&quot;\\t&quot;, as.is=T, check.names=F) Create a list of population IDs: pops &lt;- unique(unlist(lapply(rownames(data), function(x){y&lt;-c();y&lt;-c(y,unlist(strsplit(x,&quot;_&quot;)[[1]][1]))}))) To understand the experimental design a bit better, let’s look at the sample size at each site. sample_sites &lt;- rep(NA,nrow(data)) for (i in 1:nrow(data)){ sample_sites[i] &lt;- strsplit(rownames(data),&quot;_&quot;)[[i]][1]} N &lt;- unlist(lapply(pops,function(x){length(which(sample_sites==x))})) names(N) &lt;- pops N ## cr cs pcr pl sj stl wc rb ms ## 23 97 67 20 86 50 22 138 68 b. Examine population structure with PCA Let’s start examining population structure, first using PCA. We’ll look at the amount of variation explained in the first few components, and then we’ll plot individuals for four components, coloring them by population. par(mar=c(4,4,2,0.5)) pcaS &lt;- prcomp(data,center=T) plot(pcaS$sdev^2 / sum(pcaS$sdev^2), xlab=&quot;PC&quot;, ylab=&quot;Fraction Variation Explained&quot;, main=&quot;Scree plot&quot;) Get % variance explained for first few PCs: perc &lt;- round(100*(pcaS$sdev^2 / sum(pcaS$sdev^2))[1:10],2) names(perc) &lt;- apply(array(seq(1,10,1)), 1, function(x){paste0(&quot;PC&quot;, x)}) perc ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 ## 37.47 3.78 2.55 0.88 0.76 0.44 0.44 0.39 0.37 0.32 Use the RColorBrewer package to select a color palette: colors &lt;- RColorBrewer::brewer.pal(9, &quot;Paired&quot;) Plot first three PCs with colored symbols: par(mfrow=c(2,2), mar=c(4,4,0.5,0.5)) plot(pcaS$x[,1:2], col=colors[factor(sample_sites)], pch=16,cex=1.2) legend(&quot;bottomleft&quot;, legend=levels(factor(sample_sites)), col=colors, pch=16, ncol=3, cex=0.8) plot(pcaS$x[,2:3], col=colors[factor(sample_sites)], pch=16, cex=1.2) plot(pcaS$x[,3:4], col=colors[factor(sample_sites)], pch=16, cex=1.2) Question 4: Do you see evidence of population structure? Is the number of components (here 4) a good representation of the number of populations? c. Clustering with SNMF (similar to ‘STRUCTURE’) Now we are going to use a clustering method to examine population structure. There are many approaches, with various assumptions, and it is important to consider the underlying biology of your research organism (and your dataset size) before choosing an appropriate method. Here, we will use sparse negative matrix factorization (SNMF) because it is fast to compute for large datasets and it approximates the results of the well-known STRUCTURE algorithm. Notably, it relaxes population genetic assumptions such as Hardy-Weinberg proportions, so it may not converge on the same results as other programs. We can use SNMF to estimate the number of genetic clusters (K) among our sampled populations. However, this may take a long time. If you want to run the analysis, un-comment the lines by removing the ‘#’ symbol at the beginning of each line. We use SNMF’s cross-entropy criterion to infer the best estimate of K. The lower the cross-entropy, the better our model accounts for population structure. Sometimes cross-entropy continues to decline, so we might choose K where cross entropy first decreases the most. #snmf2 &lt;- LEA::snmf(paste0(here::here(), &quot;/data/stickleback.geno&quot;), # K=1:8, ploidy=2, entropy=T, alpha=100, project=&quot;new&quot;) #snmf2 &lt;- LEA::snmf(&quot;stickleback.geno&quot;, K=1:8, ploidy=2, entropy=T, # alpha=100, project=&quot;new&quot;) #par(mfrow=c(1,1)) #plot(snmf2, col=&quot;blue4&quot;, cex=1.4, pch=19) INSERT FIGURE WITH RESULT? The number of clusters is hard to determine, but four seems to be important and is similar to the results revealed by PCA. I will proceed assuming K=4. We will rerun SNMF using this setting. K=4 snmf = LEA::snmf(system.file(&quot;extdata&quot;, &quot;stickleback.geno&quot;, package = &quot;LandGenCourseData&quot;), K = K, alpha = 100, project = &quot;new&quot;) ## The project is saved into : ## y/LandGenCourseData/extdata/stickleback.snmfProject ## ## To load the project, use: ## project = load.snmfProject(&quot;y/LandGenCourseData/extdata/stickleback.snmfProject&quot;) ## ## To remove the project, use: ## remove.snmfProject(&quot;y/LandGenCourseData/extdata/stickleback.snmfProject&quot;) ## ## [1] &quot;*************************************&quot; ## [1] &quot;* sNMF K = 4 repetition 1 *&quot; ## [1] &quot;*************************************&quot; ## summary of the options: ## ## -n (number of individuals) 571 ## -L (number of loci) 10000 ## -K (number of ancestral pops) 4 ## -x (input file) /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourseData/extdata/stickleback.geno ## -q (individual admixture file) /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.Q ## -g (ancestral frequencies file) /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.G ## -i (number max of iterations) 200 ## -a (regularization parameter) 100 ## -s (seed random init) 4607182419202904667 ## -e (tolerance error) 1E-05 ## -p (number of processes) 1 ## - diploid ## ## Read genotype file /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourseData/extdata/stickleback.geno: OK. ## ## ## Main algorithm: ## [ ] ## [================================] ## Number of iterations: 86 ## ## Least-square error: 722698.114031 ## Write individual ancestry coefficient file /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.Q: OK. ## Write ancestral allele frequency coefficient file /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourseData/extdata/stickleback.snmf/K4/run1/stickleback_r1.4.G: OK. ## ## The project is saved into : ## y/LandGenCourseData/extdata/stickleback.snmfProject ## ## To load the project, use: ## project = load.snmfProject(&quot;y/LandGenCourseData/extdata/stickleback.snmfProject&quot;) ## ## To remove the project, use: ## remove.snmfProject(&quot;y/LandGenCourseData/extdata/stickleback.snmfProject&quot;) d. Plot ancestry proportions Create matrix of ancestry proportions: qmatrix = LEA::Q(snmf, K = K) Plot results with a barplot similar to that used to represent STRUCTURE results par(mar=c(4,4,0.5,0.5)) barplot(t(qmatrix), col=RColorBrewer::brewer.pal(9,&quot;Paired&quot;), border=NA, space=0, xlab=&quot;Individuals&quot;, ylab=&quot;Admixture coefficients&quot;) #Add population labels to the axis: for (i in 1:length(pops)){ axis(1, at=median(which(sample_sites==pops[i])), labels=pops[i])} e. Visualize admixture proportions on a map Import geographical coordinates for the populations: sites &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;stickleback_coordinates.csv&quot;, package = &quot;LandGenCourseData&quot;), as.is=T, check.names=F, h=T) Calculate population average ancestry proportions and create an array with population coordinates: #initialize array for ancestry proportions: qpop &lt;- matrix(NA,nrow=length(pops),ncol=K) #intialize array for coordinates: coord.pop &lt;- matrix(NA,nrow=length(pops),ncol=2) index=0 for (i in 1:length(pops)){ if (i==1){ ## i.put pop ancestry proportions for each K cluster: qpop[i,] &lt;- apply(qmatrix[1:N[i],], 2, mean) #input pop coordinates: coord.pop[i,1] &lt;- sites[which(sites[,1]==names(N[i])),6] #input pop coordinates: coord.pop[i,2] &lt;- sites[which(sites[,1]==names(N[i])),5] index = index + N[i] } else { qpop[i,] &lt;- apply(qmatrix[(index+1):(index+N[i]),], 2, mean) coord.pop[i,1] &lt;- sites[which(sites[,1]==names(N[i])),6] coord.pop[i,2] &lt;- sites[which(sites[,1]==names(N[i])),5] index = index + N[i] } } Create map with pie charts depicting ancestry proportions: par(mar=c(4,4,0.5,0.5)) plot(coord.pop, xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, type = &quot;n&quot;) maps::map(database=&#39;state&#39;,add = T, col = &quot;grey90&quot;, fill = TRUE) for (i in 1:length(pops)){ mapplots::add.pie(z=qpop[i,], x=coord.pop[i,1], y=coord.pop[i,2], labels=&quot;&quot;, col=RColorBrewer::brewer.pal(K,&quot;Paired&quot;),radius=0.1) } Question 5: How does the pattern of clustering vary in space? Is there evidence of population admixture? How do you interpret this pattern biologically? Question 6: Based on the analyses done in this tutorial, where do you hypothesize the “Riverbend” (“rb”) population originated from? What evidence supports your rationale? "],["Week11.html", "13 Lab 11: Detecting Adaptation", " 13 Lab 11: Detecting Adaptation This week’s lab explores different methods for testing genotype-environment associations. Worked Example "],["WE_11.html", "13.1 Worked Example", " 13.1 Worked Example Brenna R. Forester 1. Overview of Worked Example a. Goals This worked example will illustrate the use of two types of genotype-environment association analyses, one univariate and one multivariate. Specifically, you will learn: Strategies for screening and preparing predictor variables for different GEA analyses; How to run and interpret a Latent Factor Mixed Model (LFMM); One option for post-processing LFMM results using the genomic inflation factor (GIF) and false discovery rate (FDR) to correct for multiple tests; How to run and interpret Redundancy Analysis for GEA. b. Data set We will reanalyze genomic data from 94 North American gray wolves (Canis lupus) sampled across Canada and Alaska (Schweizer et al., 2016). We are interested in understanding how wolves may be locally adapted to environmental conditions across their North American range. The genetic data are individual-based, and are input as allele counts (i.e. 0/1/2) for each locus. In the interest of computational efficiency, we will use a randomly sampled subset of 10,000 single nucleotide polymorphism (SNP) markers from the full data set (which contains 42,587 SNPs). In addition, we have eight environmental predictors that are ecologically relevant and are not highly correlated (|r| &lt; 0.7). This is a reduced set of predictors from the 12 originally included by Schweizer et al. (2016). c. Required R libraries You may need to install packages qvalue from BioClim and lfmm from GitHub: if(!requireNamespace(&quot;qvalue&quot;, quietly = TRUE)) { if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(version = &quot;3.14&quot;) BiocManager::install(&quot;qvalue&quot;) } if(!requireNamespace(&quot;lfmm&quot;, quietly = TRUE)) { remotes::install_github(&quot;bcm-uga/lfmm&quot;) } library(LandGenCourse) library(vegan) # Used to run PCA &amp; RDA library(lfmm) # Used to run LFMM library(qvalue) # Used to post-process LFMM output 2. Import and prepare the data a. Import the genetic data I downloaded these data from the Schweizer et al. (2016) Dryad repository and converted them from .tped to .raw format using plink (Purcell et al., 2007). Then, using the R package adegenet (Jombart 2008), I read in the .raw data and extracted the matrix of 94 individuals x 42,587 SNPs. Finally, I randomly sampled 10,000 columns (SNPs) from the full data set, which is what we will analyze in this worked example. The full data in .raw format are available in the Supplemental Information for Forester et al. (2018). If you want to analyze the full data set, use the read.PLINK call from adegenet to read the data into R. gen &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;wolf_geno_samp_10000.csv&quot;, package = &quot;LandGenCourse&quot;), row.names=1) dim(gen) ## [1] 94 10000 We have 94 individuals (rows) genotyped at 10,000 SNPs (columns). Both LFMM and RDA require complete data frames (i.e., no missing genetic data). For this example, we’ll use a simple approach to imputing missing genotype values: we will impute using the most common genotype at each SNP across all individuals. sum(is.na(gen)) ## 2.,987 NAs in the matrix (~3% missing data) ## [1] 27987 gen.imp &lt;- apply(gen, 2, function(x) replace(x, is.na(x), as.numeric(names(which.max(table(x)))))) sum(is.na(gen.imp)) # No NAs ## [1] 0 We could also use this imputation approach within ecotypes (rather than across all individuals). Other promising imputation methods for species lacking a reference genome include: using ancestry values from snmf in the LEA package (Frichot &amp; Francois 2015), and the program LinkImpute (Money et al., 2015). b. Import the environmental data The original data set comes with 12 predictors, but many of them are highly correlated, which can cause problems for regression-based methods like LFMM and RDA. I conducted variable reduction using the |0.7| “rule of thumb” (Dormann et al., 2013) and an ecological interpretation of the relevance of possible predictors. Can you double check the correlations among the variables? For more information on the rationale for variable reduction, see the full RDA vignette. env &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;EnvironmentalData_8pred.csv&quot;, package = &quot;LandGenCourse&quot;)) str(env) # Look at the structure of the data frame ## &#39;data.frame&#39;: 94 obs. of 12 variables: ## $ individual : chr &quot;11226.CEL&quot; &quot;11228.CEL&quot; &quot;11232_CLU_NQUE-I&quot; &quot;11234_CLU_NQUE-I&quot; ... ## $ ecotype : chr &quot;Pop_2_BorealForest&quot; &quot;Pop_6_AtlanticForest&quot; &quot;Pop_6_AtlanticForest&quot; &quot;Pop_6_AtlanticForest&quot; ... ## $ long : num -94.5 -88.1 -72 -72 -114.8 ... ## $ lat : num 49.8 49.1 58.8 58.6 60.6 ... ## $ ann_mean_temp : int 23 15 -69 -66 -34 -37 -35 -47 -46 -20 ... ## $ mean_diurnal_range: int 97 115 75 76 102 114 112 99 109 115 ... ## $ temp_seasonality : int 13047 11408 11831 11867 14259 14806 15176 15844 15312 14099 ... ## $ ann_precip : int 610 784 483 477 335 319 372 339 284 375 ... ## $ precip_seasonality: int 47 29 39 40 39 34 43 48 36 46 ... ## $ ndvi : int 7403 8222 6450 6141 7595 7518 7472 8157 7348 6616 ... ## $ elev : int 349 234 264 200 194 219 280 154 190 222 ... ## $ percent_tree_cover: int 36 51 5 9 26 48 47 61 61 47 ... env$individual &lt;- as.character(env$individual) # Make individual names characters (not factors) # Confirm that genotypes and environmental data are in the same order identical(rownames(gen.imp), env[,1]) ## [1] TRUE Now we’ll subset just the environmental predictors &amp; shorten their names: pred &lt;- env[,5:12] colnames(pred) &lt;- c(&quot;AMT&quot;,&quot;MDR&quot;,&quot;sdT&quot;,&quot;AP&quot;,&quot;cvP&quot;,&quot;NDVI&quot;,&quot;Elev&quot;,&quot;Tree&quot;) For the univariate LFMM test, we could run a separate set of tests for each of these eight predictors; this would be a test for each of the 10,000 SNPs with each predictor = 80,000 tests (!). Instead, for LFMM, we’ll perform a PCA on the environmental predictors and use the first principal component (PC) as a synthetic predictor. This will reduce our ability to interpret the output, since the PC predictor will be a linear combination of the original eight variables, but it will reduce the number of corrections needed for multiple tests. Your decision of how to handle multiple predictors for a univarate GEA test will depend on the study goals and characteristics of the data set. There are many ways to run PCA in R; we’ll use the rda function in vegan (Oksanen et al., 2016). We’ll center and scale the predictors (scale=T), since they’re in different units. We’ll then determine the proportion of the environmental variance explained by each PC axis &amp; investigate how the original predictors correlate with the first PC axis. pred.pca &lt;- rda(pred, scale=T) summary(pred.pca)$cont ## $importance ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Eigenvalue 3.224 1.8354 1.1258 0.70269 0.48967 0.34983 0.21138 ## Proportion Explained 0.403 0.2294 0.1407 0.08784 0.06121 0.04373 0.02642 ## Cumulative Proportion 0.403 0.6324 0.7731 0.86096 0.92217 0.96590 0.99232 ## PC8 ## Eigenvalue 0.061457 ## Proportion Explained 0.007682 ## Cumulative Proportion 1.000000 screeplot(pred.pca, main = &quot;Screeplot: Eigenvalues of Wolf Predictor Variables&quot;) ## c.rrelations between the PC axis and predictors: round(scores(pred.pca, choices=1:8, display=&quot;species&quot;, scaling=0), digits=3) ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## AMT -0.519 0.029 -0.040 0.198 -0.091 0.297 0.436 -0.634 ## MDR -0.315 -0.531 0.258 -0.051 -0.190 0.259 0.332 0.579 ## sdT 0.193 -0.586 -0.225 -0.203 0.340 -0.464 0.372 -0.244 ## AP -0.353 0.468 -0.071 0.212 0.228 -0.516 0.387 0.372 ## cvP 0.319 -0.219 0.070 0.888 -0.199 -0.130 0.033 -0.007 ## NDVI -0.404 -0.253 -0.220 0.281 0.618 0.161 -0.481 0.077 ## Elev -0.150 -0.054 0.876 -0.029 0.174 -0.301 -0.178 -0.231 ## Tree -0.426 -0.200 -0.233 -0.064 -0.580 -0.480 -0.385 -0.071 ## attr(,&quot;const&quot;) ## [1] 5.222678 40% of the variance in the predictors is explained by the first PC axis, and 23% by the second axis. We could follow up with an LFMM model using the second axis as a predictor, if we wanted. The strongest correlations with PC1 are annual mean temperature (AMT), tree cover (Tree), NDVI, and annual precipitation (AP). We’ll store our synthetic PC axis predictor as pred.PC1 for use in LFMM. pred.PC1 &lt;- scores(pred.pca, choices=1, display=&quot;sites&quot;, scaling=0) 3. Latent Factor Mixed Models (LFMM): a univariate GEA LFMM is a univariate test, which means that it builds a model for each SNP and each predictor variable. In this case, we will run 10,000 SNPs x 1 synthetic predictor = 10,000 separate LFMM tests. a. Determine K LFMM requires an estimate of the number of populations in the data (K). To determine the most likely value of K, we’ll use PCA, noting that there are many different approaches for determining “K” from genetic data (more below). It is also reasonable to run LFMM with different values of K, if there is uncertainty. We’ll use a broken stick criterion to determine K. The broken stick stopping rule states that principal components should be retained as long as observed eigenvalues are higher than corresponding random broken stick components. For example, if we reran the environmental PCA screeplot from above with a broken stick criterion: screeplot(pred.pca, main = &quot;Screeplot of Wolf Predictor Variables with Broken Stick&quot;, bstick=TRUE, type=&quot;barplot&quot;) You can see that PC1 and PC2 explain more than the random broken stick components, while PC3 + do not. If this were genomic data, and we were determining a value of K using this approach, we’d set K = 3. Now let’s run a PCA with the genommic data and plot the eigenvalues with the broken stick criterion: gen.pca &lt;- rda(gen.imp, scale=T) screeplot(gen.pca, main = &quot;Screeplot of Genetic Data with Broken Stick&quot;, bstick=TRUE, type=&quot;barplot&quot;) For the genomic data, we can see that none of the PCs have eigenvalues greater than random (greater than the broken stick values in red). This effectively means that K=1 for the wolf data set, based on a PCA assessment. [Note that we see a similar result using the full genomic data set of 42K SNPs]. This does not mean there isn’t genetic structure in the data; it just means the structure isn’t particularly strong and/or easily partitioned into discrete groupings. In the original paper, the authors determined K using Structure. They found support for multiple K values between 3 and 7. The wolf data show an isolation by distance signature, which can confound methods used to identify K and contribute to uncertainty. How do you suspect that differences in K (say K=1 vs. K=3) for LFMM might influence the GEA results? For now, we’ll use K=3 following the original manuscript. It would be interesting to compare K=1 to these results, if you have time. Note that with K=1, LFMM is essentially running a simple linear regression (i.e., no latent factors). K &lt;- 3 b. Run LFMM LFMM is a regression model that includes unobserved variables (latent factors, set with “K”) that correct the model for confounding effects, such as population structure. The latent factors are estimated simultaneously with the environmental and response variables, which can help improve power when environment and demography are correlated. The previous version of LFMM (v1.5, implemented in the LEA package) uses an MCMC (Markov chain Monte Carlo) algorithm to identify GEAs while correcting for confounding. MCMC made it (very!) time-intensive for large data sets. LFMM v.2 computes LFMMs for GEA using a least-squares estimation method that is substantially faster than v1.5 (Caye et al., 2019). There are two penalty approaches: ridge and lasso. We’ll use ridge today (see ?lfmm_ridge); see Jumentier et al. (2020) for more information on the ridge vs. lasso penalties. wolf.lfmm &lt;- lfmm_ridge(Y=gen.imp, X=pred.PC1, K=K) ## c.ange K as you see fit That was fast! As you’ll see, post-processing these results is what takes up most of our time and effort… c. Identify LFMM candidates using False Discovery Rate Next, we post-process the model output. We will move fast here; I strongly recommend reading “Controlling false discoveries in genome scans for selection” (Francois et al., 2016) if you will be running these post-processing steps on your own data! Decisions made here can dramatically impact the candidate markers you identify. The steps for post-processing are: Look at the genomic inflation factor (GIF), which gives us a sense for how well the model has accounted for confounding factors in the data. Plot the p-values to see how application of the GIF influences the p-value distribution. Modify the GIF (if needed) and re-plot the p-values to identify the best possible fit to the “ideal” p-value distribution. Apply a False Discovery Rate control method to the p-values by converting to q-values. Identify candidates as those below a given FDR threshold. The False Discovery Rate is the expected proportion of false positives among the list of positive tests (see Storey and Tibshirani, 2003). An essential point to understand here is that the FDR is predicated on the “ideal” p-value distribution (flat with a peak at 0, see Francois et al. 2016). For example, an FDR threshold of 0.10 applied to a dataset with an ideal p-value distribution would produce 10% false positives (aka false discoveries) among the set of positive tests. However, if the p-value distribution deviates from the ideal, this same FDR threshold of 0.10 would produce more or fewer false discoveries among the set of postive tests, depending on the skew in the p-value distribution. So remember: having an actual FDR that is in accordance with the nominal threshold you set is completely dependent on the p-values used. The lfmm package has a nice built-in function to calculate test statistics for the predictor(s), see ?lfmm_test: wolf.pv &lt;- lfmm_test(Y=gen.imp, X=pred.PC1, lfmm=wolf.lfmm, calibrate=&quot;gif&quot;) names(wolf.pv) # this object includes raw z-scores and p-values, as well as GIF-calibrated scores and p-values ## [1] &quot;B&quot; &quot;epsilon.sigma2&quot; &quot;B.sigma2&quot; ## [4] &quot;score&quot; &quot;pvalue&quot; &quot;gif&quot; ## [7] &quot;calibrated.score2&quot; &quot;calibrated.pvalue&quot; Let’s look at the genomic inflation factor (GIF): wolf.pv$gif ## PC1 ## 2.772992 An appropriately calibrated set of tests will have a GIF of around 1. The elevated GIF for our tests indicates that the results may be overly liberal in identifying candidate SNPs. If the GIF is less than one, the test may be too conservative. NOTE: Changing the value of K influences the GIF, so additional tests using the “best” value of K +/- 1 may be needed in some cases. See Francois et al. (2016) for more details. Let’s look at how application of the GIF to the p-values impacts the p-value distribution: hist(wolf.pv$pvalue[,1], main=&quot;Unadjusted p-values&quot;) hist(wolf.pv$calibrated.pvalue[,1], main=&quot;GIF-adjusted p-values&quot;) We want to see a relatively flat histogram (most loci not under selection) with a peak near zero, indicating candidate adaptive markers. We see a very large peak with the unadjusted p-values, and a much smaller peak with the GIF-adjusted p-values (note differences in the scale of the y-axis). Note that you can choose a different GIF and readjust the p-values to get a “better” histogram distribution (that is, a distribution that conforms best with what is expected under a well-calibrated set of models, see Francois et al, 2016). This process is subjective and can be difficult with empirical data sets, especially those with an IBD signature, such as these wolf data. Remember, you can also change the value of K in your lfmm models and see how this impacts the GIF. Below I’ll show you how to manually adjust the GIF correction factor: # Let&#39;s change the GIF and readjust the p-values: zscore &lt;- wolf.pv$score[,1] # zscores for first predictor, we only have one in our case... (gif &lt;- wolf.pv$gif[1]) ## d.fault GIF for this predictor ## PC1 ## 2.772992 new.gif1 &lt;- 2.0 ## c.oose your new GIF # Manual adjustment of the p-values: adj.pv1 &lt;- pchisq(zscore^2/new.gif1, df=1, lower = FALSE) Plot the p-value histograms: hist(wolf.pv$pvalue[,1], main=&quot;Unadjusted p-values&quot;) hist(wolf.pv$calibrated.pvalue[,1], main=&quot;GIF-adjusted p-values (GIF=2.8)&quot;) hist(adj.pv1, main=&quot;REadjusted p-values (GIF=2.0)&quot;) For now, we’ll stick with the default GIF calculated by the lfmm package, though it looks like the application of the GIF may be a bit conservative (e.g. it is compressing the peak at 0). Finally, we convert the adjusted p-values to q-values. q-values provide a measure of each SNP’s significance, automatically taking into account the fact that thousands are simultaneously being tested. We can then use an FDR threshold to control the number of false positive detections (given that our p-value distribution is “well-behaved”). wolf.qv &lt;- qvalue(wolf.pv$calibrated.pvalue)$qvalues length(which(wolf.qv &lt; 0.1)) ## h.w many SNPs have an FDR &lt; 10%? ## [1] 12 (wolf.FDR.1 &lt;- colnames(gen.imp)[which(wolf.qv &lt; 0.1)]) ## i.entify which SNPs these are ## [1] &quot;chr6.36999927&quot; &quot;chr20.36354466&quot; &quot;chr17.54224832&quot; &quot;chr12.58928631&quot; ## [5] &quot;chr12.55629962&quot; &quot;chr8.41109194&quot; &quot;chr19.17640345&quot; &quot;chr8.63918041&quot; ## [9] &quot;chr20.10469494&quot; &quot;chr22.64010061&quot; &quot;chr5.25188969&quot; &quot;chr9.36496633&quot; Using K=3, the default GIF correction, and an FDR threshold of 0.10, we only detect 12 candidate SNPs under selection in response to our PC1 environmental predictor. What changes could we make to have a less conservative test? 4. Redundancy Analysis (RDA): a multivariate GEA a. Run RDA RDA is a multivariate ordination technique that can analyze many loci and environmental predictors simultaneously. For this reason, we can input all of the SNPs and environmental predictors at once, with no need to correct for multiple tests. RDA determines how groups of loci covary in response to the multivariate environment, and can better detect processes that result in weak, multilocus molecular signatures relative to univariate tests (Rellstab et al., 2015; Forester et al., 2018). RDA can be used on both individual and population-based sampling designs. The distinction between the two may not be straightforward in all cases. A simple guideline would be to use an individual-based framework when you have individual coordinates for most of your samples, and the resolution of your environmental data would allow for a sampling of environmental conditions across the site/study area. For population-level data, you would input the genetic data as allele frequencies within demes. The code to run the RDA is simple. However, I highly recommend reading Borcard et al. (2011) for details on the implementation and interpretation of RDA models and the objects created by vegan. RDA runs relatively quickly on most data sets, though you may need a high memory node on a cluster for very large data sets (i.e., millions of SNPs). wolf.rda &lt;- rda(gen.imp ~ ., data=pred, scale=T) wolf.rda ## ## Call: rda(formula = gen.imp ~ AMT + MDR + sdT + AP + cvP + NDVI + Elev + ## Tree, data = pred, scale = T) ## ## Inertia Proportion Rank ## Total 8307.000 1.000 ## Constrained 1113.488 0.134 8 ## Unconstrained 7193.512 0.866 85 ## ## Inertia is correlations ## ## Eigenvalues for constrained axes: ## RDA1 RDA2 RDA3 RDA4 RDA5 RDA6 RDA7 RDA8 ## 281.19 216.87 179.53 110.84 89.78 87.15 75.93 72.20 ## ## Eigenvalues for unconstrained axes: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## 261.51 211.68 197.60 171.80 127.67 121.61 115.54 110.04 ## (Showing 8 of 85 unconstrained eigenvalues) First, note that we will have as many constrained (“RDA”) axes as we have predictors in the model. All residual variance is then modeled by PCA (the unconstrained “PC” axes). The proportion of the variance explained by the environmental predictors is given under the “Proportion” column for “Constrained”; this is equivalent to the R2 of a multiple regression. Just like in multiple regression, this R2 will be biased and should be adjusted based on the number of predictors. We can calculate the adjusted R2 using: RsquareAdj(wolf.rda) ## $r.squared ## [1] 0.1340421 ## ## $adj.r.squared ## [1] 0.05254019 Our constrained ordination explains about 5% of the variation; this low explanatory power is not surprising given that we expect that most of the SNPs in our dataset will not show a relationship with the environmental predictors (e.g., most SNPs will be neutral). The eigenvalues for the constrained axes reflect the variance explained by each canonical axis: summary(wolf.rda)$concont ## $importance ## Importance of components: ## RDA1 RDA2 RDA3 RDA4 RDA5 RDA6 ## Eigenvalue 281.1906 216.8651 179.5282 110.84337 89.78143 87.14904 ## Proportion Explained 0.2525 0.1948 0.1612 0.09955 0.08063 0.07827 ## Cumulative Proportion 0.2525 0.4473 0.6085 0.70807 0.78870 0.86697 ## RDA7 RDA8 ## Eigenvalue 75.92523 72.20485 ## Proportion Explained 0.06819 0.06485 ## Cumulative Proportion 0.93515 1.00000 We can visualize this information using a screeplot of the canonical eigenvalues by calling screeplot: screeplot(wolf.rda) Here, we can see that the first three constrained axes explain most of the variance. The screeplot provides an informal (and quick) way to determine how many constrained axes to include when we search for candidate SNPs (below). We could start by investigating RDA axes that explain the most variance (excluding those after the “drop off” point in the screeplot.) You can run a formal test of statistical significance of each constrained axis using: anova.cca(wolf.rda, by=\"axis\"). We can assess both the full model and each constrained axis using F-statistics (Legendre et al, 2010). The null hypothesis is that no linear relationship exists between the SNP data and the environmental predictors. See ?anova.cca for more details and options. The screeplot and a formal test (by axis) are both reasonable approaches for determining which RDA axes to assess for candidate SNPs. The permutation process to test the signficiance of each axis takes a while (up to a few hours on large data sets), so we’ll just use the screeplot for a first assessment. If we did run the formal test, we would find that the first three constrained axes are significant (p = 0.001); constrained axis 4 has a p-value of 0.080, while axes 5-8 have p-values &gt; 0.850. This corresponds with our evaluation of the screeplot, above. Finally, vegan has a simple function for checking Variance Inflation Factors for the predictor variables used in the model. This helps identify redundant predictors: vif.cca(wolf.rda) ## AMT MDR sdT AP cvP NDVI Elev Tree ## 7.854243 6.495892 2.775059 4.051610 1.318631 2.285632 2.028377 2.260139 All values are below 10, and most are below 5, which indicates that multicollinearity among these predictors shouldn’t be a problem (Zuur et al., 2010). Let’s make a quick plot of the RDA output using the default plotting in vegan: plot(wolf.rda, scaling=3) ## d.fault is axes 1 and 2 Here, the SNPs are in red (in the center of each plot), and the individuals are the black circles. The blue vectors are the environmental predictors. The relative arrangement of these items in the ordination space reflects their relationship with the ordination axes, which are linear combinations of the predictor variables. See the full RDA vignette for details on how to make more informative (and prettier!) RDA plots for this data set. For example, we could more clearly visualize the identified candidate loci in the ordination space and see how they are linked to the environmental predictors. We could also use RDA to investigate how wolf ecotypes (based on individual genotypes) are distributed in relation to the environmental predictors (Forester et al., 2018, Figures 9 &amp; 10). b. Identify RDA candidates We’ll use the loadings of the SNPs (their location) in the ordination space to determine which SNPs are candidates for local adaptation. The SNP loadings are stored as species in the RDA object. We’ll extract the SNP loadings from the first three constrained axes, based on our assessment of the screeplot above. load.rda &lt;- scores(wolf.rda, 1:3)$species If we look at histograms of the loadings on each RDA axis, we can see their (relatively normal) distribution. SNPs loading at the center of the distribution are not showing a relationship with the environmental predictors; those loading in the tails are, and are more likely to be under selection as a function of those predictors (or some other predictor correlated with them). hist(load.rda[,1], main=&quot;Loadings on RDA1&quot;) hist(load.rda[,2], main=&quot;Loadings on RDA2&quot;) hist(load.rda[,3], main=&quot;Loadings on RDA3&quot;) I’ve written a simple function to identify SNPs that load in the tails of these distributions. We’ll start with a 3 standard deviation cutoff (two-tailed p-value = 0.0027). As with all cutoffs, this can be modified to reflect the goals of the analysis and our tolerance for true positives vs. false positives. For example, if you needed to be very conservative and only identify those loci under very strong selection (i.e., minimize false positive rates), you could increase the number of standard deviations to 3.5 (two-tailed p-value = 0.0005). This would also increase the false negative rate. If you were less concerned with false positives, and more concerned with identifying as many potential candidate loci as possible (including those that may be under weaker selection), you might choose a 2.5 standard deviation cutoff (two-tailed p-value = 0.012). I define the function here as outliers, where x is the vector of loadings and z is the number of standard deviations to use: outliers &lt;- function(x,z){ lims &lt;- mean(x) + c(-1, 1) * z * sd(x) ## f.nd loadings +/- z SD from mean loading x[x &lt; lims[1] | x &gt; lims[2]] # locus names in these tails } Now let’s apply it to the first three constrained axes: cand1 &lt;- outliers(load.rda[,1], 3) ## 3. cand2 &lt;- outliers(load.rda[,2], 3) ## 6. cand3 &lt;- outliers(load.rda[,3], 3) ## 3. wolf.rda.cand &lt;- c(names(cand1), names(cand2), names(cand3)) ## j.st the names of the candidates length(wolf.rda.cand[duplicated(wolf.rda.cand)]) ## 7.duplicate detections (detected on multiple RDA axes) ## [1] 7 wolf.rda.cand &lt;- wolf.rda.cand[!duplicated(wolf.rda.cand)] ## 1.4 unique candidates Let’s see where these candidate SNPs are in the ordination space. We’ll zoom the plot in to just the SNPs, and color code candidate SNPs in red: # Set up the color scheme for plotting: bgcol &lt;- ifelse(colnames(gen.imp) %in% wolf.rda.cand, &#39;gray32&#39;, &#39;#00000000&#39;) snpcol &lt;- ifelse(colnames(gen.imp) %in% wolf.rda.cand, &#39;red&#39;, &#39;#00000000&#39;) ## a.es 1 &amp; 2 - zooming in to just the SNPs here... plot(wolf.rda, type=&quot;n&quot;, scaling=3, xlim=c(-1,1), ylim=c(-1,1), main=&quot;Wolf RDA, axes 1 and 2&quot;) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=&quot;gray32&quot;, bg=&#39;#f1eef6&#39;, scaling=3) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=bgcol, bg=snpcol, scaling=3) text(wolf.rda, scaling=3, display=&quot;bp&quot;, col=&quot;#0868ac&quot;, cex=1) ## a.es 2 &amp; 3 plot(wolf.rda, type=&quot;n&quot;, scaling=3, xlim=c(-1,1), ylim=c(-1,1), choices=c(2,3), main=&quot;Wolf RDA, axes 2 and 3&quot;) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=&quot;gray32&quot;, bg=&#39;#f1eef6&#39;, scaling=3, choices=c(2,3)) points(wolf.rda, display=&quot;species&quot;, pch=21, cex=1, col=bgcol, bg=snpcol, scaling=3, choices=c(2,3)) text(wolf.rda, scaling=3, display=&quot;bp&quot;, col=&quot;#0868ac&quot;, cex=1, choices=c(2,3)) Which environmental predictor(s) seem to be most strongly related to candidate SNPs in the RDA1/RDA2 plot? How about the RDA2/RDA3 plot? Why do we see candidate SNPs loading in the center of the ordination space in both plots? Let’s see which environmental predictors are most strongly correlated with the first three RDA axes: intersetcor(wolf.rda)[,1:3] ## RDA1 RDA2 RDA3 ## AMT -0.8692103 0.10797423 0.19767975 ## MDR -0.4791404 0.77837759 -0.09163633 ## sdT 0.2971194 0.47503944 -0.03234453 ## AP -0.6902664 -0.62757476 0.09806042 ## cvP 0.3155829 0.02749876 -0.64084267 ## NDVI -0.6531881 0.24697034 0.07226876 ## Elev -0.3473015 0.24167932 -0.33073363 ## Tree -0.5699345 0.21080592 0.03478928 Generally, candidate SNPs on axis 1 represent multilocus sets of SNP genotypes associated most strongly with annual mean temperature and annual precipitation; SNPs on axis 2 represent genotypes associated with mean diurnal range; and SNPs on axis 3 represent genotypes associated with precipitation seasonality. See the full RDA vignette for additional investigation of candidate SNPs. 5. Compare LFMM and RDA candidates Let’s see what kind of overlap we have in our candidates from the two methods. Remember that we had only 12 candidates for LFMM and 134 candidates for RDA. intersect(wolf.FDR.1, wolf.rda.cand) ## f.und by both LFMM and RDA ## [1] &quot;chr6.36999927&quot; &quot;chr20.36354466&quot; &quot;chr17.54224832&quot; &quot;chr12.58928631&quot; ## [5] &quot;chr12.55629962&quot; &quot;chr8.41109194&quot; &quot;chr19.17640345&quot; &quot;chr8.63918041&quot; ## [9] &quot;chr20.10469494&quot; &quot;chr22.64010061&quot; &quot;chr9.36496633&quot; setdiff(wolf.FDR.1, wolf.rda.cand) # unique to LFMM ## [1] &quot;chr5.25188969&quot; We see a lot of overlap, even though the tests differ (e.g., correction for population structure, univariate vs. multivariate tests, differences in predictor variables, differences in post-processing). An important note: A common approach in studies that use GEAs and populations differentiation methods is to use many tests and then look for the overlap across the detections. This can be helpful for some questions (e.g. where minimizing false positive detections is of greatest importance), but can be overly conservative in many cases and bias detections against weaker signatures of selection. See our paper for an investigation of different approaches for combining detections &amp; how it impacts true and false positive rates: Forester et al. (2018); and see the (excellent!) FDR paper for more options based on combining z-scores across different tests: Francois et al. (2016). 6. A quick note on controlling for population structure in RDA We are still testing approaches to controlling for population structure in RDA. We’ve generally found that using ancestry values (e.g. from snmf or Admixture) or MEMs (Moran’s Eigenvector Maps, aka spatial eigenvectors) is overly conservative. So far, the best approach (e.g. balancing true positives and false negatives) is to compute a PCA of the genetic data and select relevant PCs (e.g. using the broken stick criteron as we did above), then use retained PCs in a partial RDA: ?rda ## h.lp for running RDA # pseudo code for simple RDA: foo &lt;- rda(genomic.data ~ predictor1 + predictor2, data=dat, scale=T) # pseudo code for partial RDA (correcting for population structure with PCs): bar &lt;- rda(genomic.data ~ predictor1 + predictor2 + Condition(PC1), data=dat, scale=T) You will want to check for correlations between retained PCs and your environmental predictors. If environment and population structure are not correlated, you can retain all predictors and PCs and proceed with your partial RDA. If environment and population structure are highly correlated, you’ll have to decide whether to prune correlated environmental variables or PCs. Pruning environmental variables will likely reduce false positives but will also increase false negatives. Pruning PCs (instead of environmental predictors) will likely increase false positives, but will also reduce false negatives. These trade-offs exist with all approaches for controlling population structure and other confounding factors when environment is correlated with population structure. 7. References Borcard D, Gillet F, Legendre P (2011) Numerical Ecology with R. Springer, New York. Caye K, Jumentier B, Lepeule J, Francois O (2019) LFMM 2: Fast and Accurate Inference of Gene-Environment Associations in Genome-Wide Studies. Molecular Biology and Evolution 36: 852-860. Dormann CF, Elith J, Bacher S, et al. (2013) Collinearity: a review of methods to deal with it and a simulation study evaluating their performance. Ecography, 36: 27-46. Forester BR, Lasky JR, Wagner HH, Urban DL (2018) Comparing methods for detecting multilocus adaptation with multivariate genotype-environment associations. Molecular Ecology. Forester BR (2018) Vignette: Detectingn multilocus adaptation using Redundancy Analysis (RDA). Population Genetics in R: popgen.nescent.org. Francois O, Martins H, Caye, K, Schoville S (2016) Controlling false discoveries in genome scans for selection. Molecular Ecology, 25: 454-469. Frichot E, Francois O (2015) LEA: An R package for landscape and ecological association studies. Methods in Ecology and Evolution, 6: 925-929. Jombart, T (2008) adegenet: a R package for the multivariate analysis of genetic markers. Bioinformatics, 24: 1403-1405. Jumentier B, Caye K, Heude B, Lepeule J, Francois O (2020) Sparse latent factor regression models for genome-wide and epigenome-wide association studies. bioRxiv, 2020.02.07.938381. Legendre P, Oksanen J, ter Braak CJ (2010) Testing the significance of canonical axes in redundancy analysis. Methods in Ecology and Evolution, 2: 269-277. Money D, Migicovsky Z, Gardner K, Myles S (2017) LinkImputeR: user-guided genotype calling and imputation for non-model organisms. BMC Genomics, 18: 1-12. Oksanen J, Blanchet FG, Kindt R, et al. (2016) vegan: Community Ecology Package. R package version 2.3-5. Rellstab C, Gugerli F, Eckert AJ, Hancock AM, Holderegger R (2015) A practical guide to environmental association analysis in landscape genomics. Molecular Ecology, 24: 4348-4370. Schweizer RM, vonHoldt BM, Harrigan R, et al. (2016) Genetic subdivision and candidate genes under selection in North American grey wolves. Molecular Ecology, 25: 380-402. Storey JD, Tibshirani R (2003) Statistical significance for genome-wide experiments. Proceedings of the National Academy of Sciences, 100: 9440-9445. Zuur AF, Ieno EN, Elphick CS (2010) A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution, 1: 3-14. "],["Week12.html", "14 Lab 12: Model Selection", " 14 Lab 12: Model Selection This week’s lab shows how to create and analyzeg landscape genetic hypotheses using maximum likelihood population effect models and information theory. Worked Example "],["WE_12.html", "14.1 Worked Example", " 14.1 Worked Example Caren Goldberg and Helene Wagner 1. Overview of Worked Example a. Goals and Background Goals: The goal of this worked example is for you to become familiar with creating and analyzing landscape genetic hypotheses using maximum likelihood population effect models (MLPE; Van Strien et al. 2012) and information theory. With this approach, we focus on evaluating evidence for the influence of the (intervening) landscape on link-based metrics (i.e. genetic distance). Incorporating node-based effects (i.e. pond size, etc.) will be covered in the gravity modeling lab. We will re-analyze a dataset from Goldberg and Waits (2010) using a maximum likelihood population effect model (MLPE). Here’s a map of the study area. The lines represent those pair-wise links between sampling locations that we will be considering here (graph model: Delaunay triangulation). Map of study area b. Data set In previous labs, you have created genetic distance matrices, so here we are going to start with those data already complete, as are the landscape data. For MLPE, two vectors representing the nodes at the ends of each link must be created. These are already included in the data set (pop1 and pop2). Each row in the file ‘CSF_network.csv’ represents a link between two pairs of sampling locations. The columns contain the following variables: Id: Link ID. Name: Link name (combines the two site names). logDc.km: Genetic distance per geographic distance, log transformed (response variable). LENGTH: Euclidean distance. slope: Average slope along the link. solarinso: Average solar insolation along the link. soils: Dominant soil type along the link (categorical). ag: Proportion of agriculture along the link. grass: Proportion of grassland along the link. shrub: Proportion of shrubland along the link. hi: Proportion of high density forest along the link. lo: Proportion of low density forest along the link. dev: Proportion of development (buildings) along the link. forest: Proportion of forest (the sum of hi and lo) along the link. pop1: ‘From’ population. pop2: ‘To’ population. c. Required packages Install corMLPE if it is not yet installed. if(!requireNamespace(&quot;corMLPE&quot;, quietly = TRUE)) remotes::install_github(&quot;nspope/corMLPE&quot;) library(LandGenCourse) library(ggplot2) #library(nlme) #library(usdm) #library(corMLPE) d. Import data First, read in the dataset. It is named CSF (Columbia spotted frog) to differentiate it from the RALU dataset you have used in previous labs (same species, very different landscapes). CSFdata &lt;- read.csv(system.file(&quot;extdata&quot;, &quot;CSF_network.csv&quot;, package = &quot;LandGenCourse&quot;)) head(CSFdata) ## Id Name logDc.km LENGTH slope solarinso soils ag ## 1 1 CeGr to FeCr -5.664895 5449.643 11.328851 16332.75 thirteen 0.00000000 ## 2 2 CeGr to FlCr -5.786712 8352.643 12.822890 16142.01 one 0.01351351 ## 3 43 CeGr to Krum -5.466544 7232.601 15.852762 16208.67 one 0.00000000 ## 4 3 FlCr to Krum -5.473660 6196.521 14.156965 15871.14 twelve 0.00000000 ## 5 44 Krum to P37 -5.088242 4470.826 15.381000 15883.31 thirteen 0.00000000 ## 6 46 CeGr to LaTr -5.997768 8766.482 9.709769 16298.75 one5 0.00000000 ## grass shrub hi lo dev forest pop1 pop2 ## 1 0.00000000 0.55319149 0.2553191 0.1914894 0.00000000 0.4468085 1 2 ## 2 0.01351351 0.10810811 0.6216216 0.2432432 0.00000000 0.8648649 1 3 ## 3 0.00000000 0.19696970 0.6212121 0.1818182 0.00000000 0.8030303 4 1 ## 4 0.00000000 0.05084746 0.5932203 0.3559322 0.00000000 0.9491525 3 4 ## 5 0.05000000 0.00000000 0.3250000 0.6250000 0.00000000 0.9500000 12 4 ## 6 0.20000000 0.21333333 0.2666667 0.3066667 0.01333333 0.5733333 5 1 The variable solarinso is on a very different scale, which can create problems with model fitting with some methods. To avoid this, we scale it. Note: we could use the scale function without specifying the arguments ‘center’ or ‘scale’, as both are ‘TRUE’ by default. The argument ‘center=TRUE’ centers the variable by subtracting the mean, the ‘scale=TRUE’ argument rescales the variables to unit variance by dividing each value by the standard deviation. The resulting variable has a mean of zero and a standard deviation and variance of one. CSFdata$solarinsoz &lt;- scale(CSFdata$solarinso, center=TRUE, scale=TRUE) Take a look at the column names and make sure you and R agree on what everything is called, and on the data types (e.g., factor vs. character). str(CSFdata) ## &#39;data.frame&#39;: 48 obs. of 17 variables: ## $ Id : int 1 2 43 3 44 46 4 6 7 31 ... ## $ Name : chr &quot;CeGr to FeCr&quot; &quot;CeGr to FlCr&quot; &quot;CeGr to Krum&quot; &quot;FlCr to Krum&quot; ... ## $ logDc.km : num -5.66 -5.79 -5.47 -5.47 -5.09 ... ## $ LENGTH : num 5450 8353 7233 6197 4471 ... ## $ slope : num 11.3 12.8 15.9 14.2 15.4 ... ## $ solarinso : num 16333 16142 16209 15871 15883 ... ## $ soils : chr &quot;thirteen&quot; &quot;one&quot; &quot;one&quot; &quot;twelve&quot; ... ## $ ag : num 0 0.0135 0 0 0 ... ## $ grass : num 0 0.0135 0 0 0.05 ... ## $ shrub : num 0.5532 0.1081 0.197 0.0508 0 ... ## $ hi : num 0.255 0.622 0.621 0.593 0.325 ... ## $ lo : num 0.191 0.243 0.182 0.356 0.625 ... ## $ dev : num 0 0 0 0 0 ... ## $ forest : num 0.447 0.865 0.803 0.949 0.95 ... ## $ pop1 : int 1 1 4 3 12 5 2 1 2 4 ... ## $ pop2 : int 2 3 1 4 4 1 5 6 6 7 ... ## $ solarinsoz: num [1:48, 1] 0.796 -0.28 0.096 -1.808 -1.74 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 16192 ## ..- attr(*, &quot;scaled:scale&quot;)= num 177 Name and soils are interpreted as factors, Id, pop1 and pop2 are vectors of integers, and everything else is a numeric vector, i.e., continuous variable. The response Y in our MLPE models will be logDc.km, the genetic distance per geographic distance, log transformed to meet normality assumptions. This was used in the paper as a way to include IBD as the base assumption without increasing k. The landscape variables were then also estimated so that they would not necessarily increase with increasing distance between sites (e.g., proportion of forest between sites). This is a pruned network, where the links are only those neighbors in a Delauney trangulation. &lt;Capture.PNG&gt; For this exercise, you will test 5 alternative hypotheses. At the end, you will be invited to create your own hypotheses from these variables and test support for them. 2. Fitting candidate models a. Alternative hypotheses In information theory, we evaluate evidence for a candidate set of hypotheses that we develop from the literature and theory. Although it is mathematically possible (and often practiced) to fit all possible combinations of input variables, that approach is not consistent with the methodology (see Burnham and Anderson 2002 Chapter 8, the reading for this unit, for more detail). Here, your “a priori”” set of hypotheses (as in the Week 12 Conceptual Exercise) is as follows: Full model: solarinso, forest, ag, shrub, dev Landcover model: ag, shrub, forest, and dev Human footprint model: ag, dev Energy conservation model: slope, shrub, dev Historical model: soils, slope, solarinso b. Check for multicollinearity Before we add predictor variables to a model, we should make sure that these variables do not have issues with multicollinearity, i.e., that they are not highly correlated among themselves. To this end, we calculate variance inflation factors, VIF. Make a dataframe of just the variables to test (note, you cannot use factors here): CSF.df &lt;- with(CSFdata, data.frame(solarinsoz, forest, dev, shrub, ag)) usdm::vif(CSF.df) ## Variables VIF ## 1 solarinsoz 1.479273 ## 2 forest 3.127104 ## 3 dev 1.800218 ## 4 shrub 1.523341 ## 5 ag 2.532198 If you get an error that there is no package called ‘usdm’, use install.packages(\"usdm\"). VIF values less than 10, or 3, or 4, depending on who you ask, are considered to not be collinear enough to affect model outcomes. So we are good to go here. What would happen if we added grass to this? usdm::vif(cbind(CSF.df, grass=CSFdata$grass)) ## Variables VIF ## 1 solarinsoz 1.541954 ## 2 forest 97.167730 ## 3 dev 55.768172 ## 4 shrub 11.657666 ## 5 ag 68.852254 ## 6 grass 45.970721 Question 1: Why would adding in the last land cover cause a high amount of collinearity? Consider how these data were calculated. The variables ‘forest’, ‘dev’, ‘shrub’, ‘ag’ and ‘grass’ together cover almost all land cover types in the study area. Each is measured as percent area, and together, they make up 100% or close to 100% of the land cover types along each link. Thus if we know e.g. that all ‘forest’, ‘dev’, ‘shrub’ and ‘ag’ together cover 80% along a link, then it is a good bet that ‘grass’ will cover the remaining 20%. With such sets of compositional data, we need to omit one variable to avoid multi-collinearity (where a variable is a linear combination of other variables), even though the pairwise correlations may be low. For example, there the variable ‘grass’ showed relatively low correlations with the other variables: cor(CSF.df, CSFdata$grass) ## [,1] ## solarinsoz 0.10394523 ## forest -0.41341968 ## dev -0.23617929 ## shrub -0.29807027 ## ag 0.01060842 c. Fit all five models The R package corMLPE makes it easy to fit an MLPE model to our dataset. We use the function gls from package nlme to fit a “generalized least squares” model to the data. This is similar to a spatial regression model we’ve seen in Week 7. The main difference is how we define the correlation structure of the errors: instead of using a function of the spatial coordinates, we use a function of the population indicators: correlation=corMLPE(form=~pop1+pop2). Note: for linear mixed models, see Week 6 videos. For gls models, see Week 7 videos. mod1 &lt;- nlme::gls(logDc.km ~ solarinsoz + forest + ag + shrub + dev, correlation=corMLPE::corMLPE(form=~pop1+pop2), data=CSFdata, method=&quot;REML&quot;) This is the closest to a full model in our dataset (although our models are not completely nested), so we will take a look at the residuals. plot(mod1, abline=c(0,0)) Residuals are centered around zero and do not show large groupings or patterns, although there is some increase in variation at larger numbers (heteroscedasticity: the plot thickens). Here, this means that the model is having a more difficult time predicting larger genetic distances per km. We will keep that in mind as we move on. Next we will check to make sure the residuals are normally distributed. This is an important assumption for models fitted with (Restricted) Maximum Likelihood. We’ll check this with two plots: Histogram (left): we want to see a symmetric, bell-shaped distribution. Normal probability plot (right): we want to see the points fall on a straight line, without systematic deviations. par(mfrow=c(1,2)) hist(residuals(mod1)) qqnorm(residuals(mod1)) par(mfrow=c(1,1)) These residuals look okay so we will move on (but if this were a publication we would do a closer examination of the shape of the data at the ends of the distribution). Recall that mod1 had the following formula: logDc.km ~ solarinsoz + forest + ag + shrub + dev. Here we’ll use the function update to change the list of predictors in the model (i.e., the right-hand side of the formula, starting with the tilde symbol ~). mod2 &lt;- update(mod1, ~ forest + ag + shrub + dev) mod3 &lt;- update(mod1, ~ ag + dev) mod4 &lt;- update(mod1, ~ slope + shrub + dev) mod5 &lt;- update(mod1, ~ soils + slope + solarinsoz) d. Compare evidence for models Our goal here is to compare five models that all have the same random effects (the population effects modeled by the correlation structure) but differ in the fixed effects. Recall from the Week 6 videos: Use REML to test random effects, and to get AIC to compare models with different random effects. Use ML to test fixed effects, and to get AIC to compare models with different fixed effects. Use REML for parameter estimates of fixed effects. We can use the function update again, this time to change the fitting method from method=\"REML\" to method=\"ML\". mod1noREML &lt;- update(mod1, method=&quot;ML&quot;) mod2noREML &lt;- update(mod2, method=&quot;ML&quot;) mod3noREML &lt;- update(mod3, method=&quot;ML&quot;) mod4noREML &lt;- update(mod4, method=&quot;ML&quot;) mod5noREML &lt;- update(mod5, method=&quot;ML&quot;) In information theory, we use information criteria (AIC, BIC) to evaluate the relative distance of our hypothesis from truth. For more information, see Burnham and Anderson (2002). Note: package MuMIn will do this for you, but for this exercise it is useful to see all the pieces of what goes into your evaluation. Models &lt;- list(Full=mod1noREML, Landcover=mod2noREML, HumanFootprint=mod3noREML, EnergyConservation=mod4noREML, Historical=mod5noREML) CSF.IC &lt;- data.frame(AIC = sapply(Models, AIC), BIC = sapply(Models, BIC)) CSF.IC ## AIC BIC ## Full 115.9620 130.9316 ## Landcover 113.9757 127.0742 ## HumanFootprint 113.4053 122.7613 ## EnergyConservation 114.4910 125.7182 ## Historical 122.4089 137.3785 We now have some results, great! Now we will work with these a bit. First, because we do not have an infinite number of samples, we will convert AIC to AICc (which adds a small-sample correction to AIC). First, find the k parameters used in the model and add them to the table. CSF.IC &lt;- data.frame(CSF.IC, k = sapply(Models, function(ls) attr(logLik(ls), &quot;df&quot;))) CSF.IC ## AIC BIC k ## Full 115.9620 130.9316 8 ## Landcover 113.9757 127.0742 7 ## HumanFootprint 113.4053 122.7613 5 ## EnergyConservation 114.4910 125.7182 6 ## Historical 122.4089 137.3785 8 Question 2: How does k relate to the number of parameters in each model? Disclaimer: More research is needed to establish how to best account for sample size in MLPE. Here we use N = number of pairs. More generally, more research is needed to establish whether delta values and evidence weights calculated from MLPE are indeed valid. Calculate AICc and add it to the dataframe (experimental, see disclaimer above): N = nrow(CSFdata) # Number of unique pairs CSF.IC$AICc &lt;- CSF.IC$AIC + 2*CSF.IC$k*(CSF.IC$k+1)/(N-CSF.IC$k-1) CSF.IC ## AIC BIC k AICc ## Full 115.9620 130.9316 8 119.6543 ## Landcover 113.9757 127.0742 7 116.7757 ## HumanFootprint 113.4053 122.7613 5 114.8339 ## EnergyConservation 114.4910 125.7182 6 116.5398 ## Historical 122.4089 137.3785 8 126.1012 e. Calculate evidence weights Next we calculate evidence weights for each model based on AICc and BIC. If all assumptions are met, these can be interpreted as the probability that the model is the closest to truth in the candidate model set. Calculate model weights for AICc (experimental, see disclaimer above): AICcmin &lt;- min(CSF.IC$AICc) RL &lt;- exp(-0.5*(CSF.IC$AICc - AICcmin)) sumRL &lt;- sum(RL) CSF.IC$AICcmin &lt;- RL/sumRL Calculate model weights for BIC (experimental, see disclaimer above): BICmin &lt;- min(CSF.IC$BIC) RL.B &lt;- exp(-0.5*(CSF.IC$BIC - BICmin)) sumRL.B &lt;- sum(RL.B) CSF.IC$BICew &lt;- RL.B/sumRL.B round(CSF.IC,3) ## AIC BIC k AICc AICcmin BICew ## Full 115.962 130.932 8 119.654 0.047 0.012 ## Landcover 113.976 127.074 7 116.776 0.200 0.085 ## HumanFootprint 113.405 122.761 5 114.834 0.527 0.735 ## EnergyConservation 114.491 125.718 6 116.540 0.224 0.167 ## Historical 122.409 137.378 8 126.101 0.002 0.000 Question 3: What did using AICc (rather than AIC) do to inference from these results? In the multi-model inference approach, we can look across all the models in the dataset and their evidence weights to better understand the system. Here we see that models 1 and 5 have very little evidence of being close to the truth, that models 2 and 4 have more, and that model 3 has the most evidence of being the closest to truth, although how much more evidence there is for model 3 over 2 and 4 depends on which criterion you are looking at. BIC imposes a larger penalty for additional parameters, so it has a larger distinction between these. Let’s review what these models are: Full model: solarinso, forest, ag, shrub, dev Landcover model: ag, shrub, forest, and dev Human footprint model: ag, dev Energy conservation model: slope, shrub, dev Historical model: soils, slope, solarinso f. Confidence intervals for predictors The summary function for the model fitted with “ML” provides a model summary that includes a table with t-tests for each fixed effect in a Frequentist approach (see Week 6 Worked Example). Here we stay within the information-theoretic approach and use a different approach than significance testing to identify the most important predictors. According to Row et al. (2017), we can use the confidence interval from the variables in the models to identify those that are contributing to landscape resistance. Looking at models 2 and 3, we have some evidence that shrub and forest cover matter, but it’s not as strong as the evidence for ag and development. Let’s look at the parameter estimates to understand more. Note that for parameter estimation we use the REML estimates. ModelsREML &lt;- list(Full=mod1, Landcover=mod2, HumanFootprint=mod3, EnergyConservation=mod4, Historical=mod5) Now we’ll estimate the 95% confidence interval for these estimates: confint(ModelsREML$Landcover, level = 0.95, method = &quot;Wald&quot;) ## 2.5 % 97.5 % ## (Intercept) -5.412493 -3.9372220 ## forest -1.234534 0.8980824 ## ag -2.767849 -0.2789160 ## shrub -4.335188 0.4391631 ## dev -1.077595 1.4292393 confint(ModelsREML$HumanFootprint, level = 0.95, method = &quot;Wald&quot;) ## 2.5 % 97.5 % ## (Intercept) -5.3193590 -4.5159303 ## ag -2.0215023 -0.1997056 ## dev -0.6843463 1.5082812 confint(ModelsREML$EnergyConservation, level = 0.95, method = &quot;Wald&quot;) ## 2.5 % 97.5 % ## (Intercept) -6.83962997 -5.3057702 ## slope 0.02028322 0.1725869 ## shrub -3.83624990 1.0650554 ## dev 0.22109749 2.6485991 In essence, we use the confidence intervals to assess whether we can differentiate the effect of each variable from zero (i.e. statistical significance). The confidence intervals that overlap 0 do not have as much evidence of being important to gene flow, based on simulations in Row et al. (2017). So in this case, there is strong evidence that agriculture influences the rate of gene flow. Note that because the metric is genetic distance, negative values indicate more gene flow. Question 4: What evidence is there that shrub and forest matter to gene flow from these analyses? What about slope? Would you include them in a conclusion about landscape resistance in this system? Question 5: What evidence is there that development influences gene flow from these analyses? Would you include it in a conclusion about landscape resistance in this system? Question 6: We excluded grass from our models because of multicollinearity. How could we investigate the importance of this variable? 3. Your turn to test additional hypotheses! Create your own (small) set of hypotheses to rank for evidence using the dataset provided. Modify the code above to complete the following steps: Define your hypotheses. Calculate the VIF table for full model. Make a residual plot for full model. Create table of AICc and BIC weights. Create confidence intervals from models with high evidence weights. What can you infer from your analysis about what influences gene flow of this species? 4. Nested model (NMLPE) for hierarchical sampling designs Jaffe et al. (2019) proposed nested MLPE (NMLPE) to account for an additional level of hierarchical sampling. In their example, they had sampled multiple sites (clusters), and within each site, multiple colonies (populations). When they fitted a MLPE model that did not account for the hierarchical level of sites, there was unaccounted autocorrelation in the residuals. They tested for this by sorting the pairwise distances by site and then calculating an autocorrelation function (ACF) as a function of lag distance along rows (i.e., the lag represented how many rows apart to values were in the dataset). The following pseudo-code is based on code in the supplementary material of Jaffe et al. (2019). Sort data frame with pairwise data by site, then by pop (pseudo-code). Note: this is only needed for the ACF analysis. ## d. &lt;- df[order(df$site, df$pop1, df$pop2),] MLPE model (pseudo-code): # m1 &lt;- gls(Genetic.distance ~ Predictor.distance, # correlation = corMLPE(form = ~ pop1+pop2), data = df) ## a.f(resid(m1, type=&#39;normalized&#39;)) ## Autocorrelation function The package corMLPE contains another function, corNMLPE2, where we can specify cluster as an additional hierarchical level of non-independence in the data. The ACF of this model did not show any autocorrelation anymore. NMLPE model (pseudo-code) # m2 &lt;- gls(Genetic.distance ~ Predictor.distance, # correlation = corMLPE(form = ~ pop1+pop2, clusters = site), data = df) ## a.f(resid(m2, type=&#39;normalized&#39;)) ## Autocorrelation function Note that this is a method to account for hierarchical sampling designs. It does not explicitly account for spatial autocorrelation - indeed, no spatial information is included, unless Euclidean distance is included as a predictor in the model. 5. References Burnham KP and DR Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information Theoretic Approach. Chapter 8, p. 437-454. Springer-Verlag, New York. Goldberg CS and LP Waits (2010). Comparative landscape genetics of two pond-breeding amphibian species in a highly modified agricultural landscape. Molecular Ecology 19: 3650-3663. Jaffé, R, Veiga, JC, Pope, NS, et al. (2019). Landscape genomics to the rescue of a tropical bee threatened by habitat loss and climate change. Evol Appl. 12: 1164– 1177. https://doi-org.myaccess.library.utoronto.ca/10.1111/eva.12794 Row JR, ST Knick, SJ Oyler-McCance, SC Lougheed and BC Fedy (2017). Developing approaches for linear mixed modeling in landscape genetics through landscape-directed dispersal simulations. Ecology &amp; Evolution 7: 3751–3761. Van Strien MJ, D Keller and R Holderegger (2012). A new analytical approach to landscape genetic modelling: least‐cost transect analysis and linear mixed models. Molecular Ecology 21: 4010-4023. "]]
