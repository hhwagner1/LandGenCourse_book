# Lab 8: Simulation Experiments {#Week8}

In this week's computer lab, we will perform a landscape genetic simulation experiment. Along the way, we will learn how to design simulation experiments to test landscape genetic methods, and how to make our code more efficient (see also Bonus Vignette).  

- [View Course Video](#video_8)
- [Interactive Tutorial 8](#tutorial_8)
- [Worked Example](#WE_8)
- [R Exercise Week 8](#r-exercise-week-8)
- [Bonus Vignette](#bonus-8a): efficient R; parallel computing in R

Note: Weeks 1 - 8 (Basic Topics) form a streamlined program to aimed at building your R skills. These skills are required for the Advanced Topics. 


## View Course Video{#video_8}

### 1. Embedded Video {-}

- External link: [Week 8 video (Part 1)](https://play.library.utoronto.ca/watch/beb55da6c7244b4839e27769846a454f);  [Week 8 video (Part 2)](https://play.library.utoronto.ca/watch/58b786b35b1c564ad4fb1c723d8279db)
- Transcript: [Download transcript](https://github.com/hhwagner1/DGS_LG_Labs/raw/master/transcripts/Week8_script_v7.pdf)


### Video, Part 1 {-}

<iframe width="560" height="315" src="https://play.library.utoronto.ca/embed/beb55da6c7244b4839e27769846a454f" frameborder="0" allowfullscreen> iframe not supported </iframe>

### Video, Part 2 {-}

<iframe width="560" height="315" src="https://play.library.utoronto.ca/embed/58b786b35b1c564ad4fb1c723d8279db" frameborder="0" allowfullscreen> iframe not supported </iframe>


### Preview Slides {-}

[Download slides](https://raw.githubusercontent.com/hhwagner1/DGS_LG_Labs/master/docs/Video_slides/Week8_Slides.pdf)



```
## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.
## ℹ Please use the `linewidth` argument instead.
## This warning is displayed once per session.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.
```

<img src="08-Week08_files/figure-html/unnamed-chunk-1-1.png" alt="" width="768" />

## Interactive Tutorial 8{#tutorial_8}

### 1. List of R commands covered this week {-}

<table class="table table-bordered table-striped table-condensed table-responsive table" style="margin-left: auto; margin-right: auto; width: auto !important; margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"> Function </th>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"> Package </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> expand.grid </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> seq </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> plot(asp=1) </td>
   <td style="text-align:left;"> graphics </td>
  </tr>
  <tr>
   <td style="text-align:left;"> coord_fixed </td>
   <td style="text-align:left;"> ggplot2 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rnorm </td>
   <td style="text-align:left;"> stats </td>
  </tr>
  <tr>
   <td style="text-align:left;"> runif </td>
   <td style="text-align:left;"> stats </td>
  </tr>
  <tr>
   <td style="text-align:left;"> LETTERS </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> paste </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> paste0 </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> sprintf </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> grep </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> substr </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> strsplit </td>
   <td style="text-align:left;"> base </td>
  </tr>
  <tr>
   <td style="text-align:left;"> gsub </td>
   <td style="text-align:left;"> base </td>
  </tr>
</tbody>
</table>

### 2. General Instructions {-}

#### a) How to access tutorials {-}

Through RStudio Add-in:

- Install course Addins in RStudio: `library(LandGenCourse)`
- In RStudio, click on `Addins` (top menu bar)
- Follow instructions in the Console:
  - type: `require(swirl)` 
  - type: `swirl()` 
  - follow prompts 
  - select course ("Landscape_Genetics_R_Course") and tutorial (Weeks 1 - 8)


#### b) How to complete tutorial {-}

Follow prompts in the RStudio Console.

To stop and resume a tutorial: 

 - to stop and exit swirl, type: `bye()` 
 - to resume where you stopped, type: `swirl()` 

To restart tutorial from beginning: 

 - type:` swirl()` 
 - use a different name 
   (simply add a number, like this: 'MyName2')

#### c) How to submit answers (participating institutions only) {-}

The last prompt will ask whether you would like to submit the log of your tutorial session to Google Forms so that your instructor may evaluate your progress. **This feature is only available for students from participating institutions.**

If you choose 'yes', a form will open in your web browser. Complete and submit the form. 
    
You can submit multiple attempts and the best attempt will be graded. You will receive full marks as long as you answered all questions (i.e. did not use 'skip'). If you used 'skip' because you could not answer a question, please contact your instructor for advice.






## Worked Example{#WE_8}

Bernd Gruber, Erin Landguth & Helene Wagner

















### 1. Overview of Worked Example {-}

#### a. Goals {-} 

This worked example shows:

- Simulate a metapopulation on a resistance landscape
- Evaluate the power of a partial Mantel test
- Compare partial Mantel test to 'Sunder'
- Run many simulations and synthesize results

#### b. Data set {-}

We will simulate data using the 'landgenreport' function of the package 'PopGenReport'. See: www.popgenreport.org

#### c. Required R packages {-}


``` r
library(LandGenCourse)
library(PopGenReport )   #load the package
library(secr)            #to create a random habitat
#library(gdistance)
#library(mmod)
library(raster)
#library(tibble)
#library(here)
#library(ggplot2)
#library(MLPE)
```

Package `secr` not automatically installed with 'LandGenCourse':


``` r
if(!require(secr)) install.packages("secr", repos='http://cran.us.r-project.org')
#library(secr)
```

Install `gstudio` if missing:


``` r
if(!requireNamespace("popgraph", quietly = TRUE))
{
  install.packages(c("RgoogleMaps", "geosphere", "proto", "sampling", 
                      "seqinr", "spacetime", "spdep"), dependencies=TRUE)
  remotes::install_github("dyerlab/popgraph")
}
if(!requireNamespace("gstudio", quietly = TRUE)) remotes::install_github("dyerlab/gstudio")
```

```
## Registered S3 method overwritten by 'gstudio':
##   method      from    
##   print.locus genetics
```


The following 'setup chunk' is used to set the root address of file paths to the root of the project folder.


``` r
knitr::opts_knit$set(root.dir = normalizePath("..")) 
```

### 2. Initialize a landscape {-}

#### a. Create a random landscape {-} 

We will use the 'randomHabitat' function from the 'secr' package to create a random habitat map and assign resistance values to habitat and non-habitat. There are many alternative ways to define your map, e.g. simply load a png file or any other file format using the 'raster' function from package 'raster' (?raster::raster, see the examples in there). If your map is categorical, assign resistance values to the different values in the raster as shown below for the missing values. If your map is already a resistance surface, skip this step.

Here we use the function 'set.seed' at the beginning of the simulation to make sure we get the same sequence of random numbers everytime we run this code. This makes the exact results reproducible even though we use a random process.

The function 'make.grid' here creates a landscape of nx=50 times xy=50 gridpoints spaced 1 unit (meter) apart. This is returned as a data frame 'tempgrid' with two columns that represent 'x' and 'y' grid coordinates.


``` r
nx=50
ny=50
set.seed(555) #(to make sure we have the same example running)
#tempmask<-secr::make.mask(nx=nx,ny=ny,spacing=1)
tempgrid<-secr::make.grid(nx=nx,ny=ny,spacing=1)
```

In the function 'randomHabitat', the argument 'A' specifies the expected proportion of habitat, and 'p' controls the level of fragmentation, i.e., degree of spatial aggregation (sorry this is naming may be a bit confusing, but that's what it says in the help file: ?randomHabitat). The function simulates a map with these parameters and returns a data frame with only those points from 'tempgrid' that are habitat. It expects an input object of class 'mask' (an object type specific to the 'secr' package), hence we pass 'as.mask(tempgrid)'. 

Finally, we create a `raster` object (package `raster``) called `r` with the simulated habitat.


``` r
tmp <- secr::randomHabitat(secr::as.mask(tempgrid), p = 0.5, A = 0.5)
r <- as.data.frame(tempgrid) 
r$resistance <- 10
r$resistance[as.numeric(row.names(tmp))] <- 1
r <- raster::rasterFromXYZ(r)
```

Let's verify that we have two values in the raster: 1 for habitat, 10 for non-habitat


``` r
table(values(r), exclude="")
```

```
## 
##    1   10 
## 1309 1191
```

Plot the habitat map


``` r
plot(r)
```

<img src="08-Week08_files/figure-html/unnamed-chunk-17-1.png" alt="" width="672" />

We have thus created a numeric raster with a resistance surface where habitat cells (grey) have a resistance value of 1 and non-habitat cells (green) have a resistance value of 10.

#### b. Add populations to the landscape (using minimal distance) {-}

We create a function that allows us to set up 'n' subpopulations in the habitat only (grid cells with value = 1). The sub-populations should be at least 'minDist' units apart, given any resistance surface 'landscape'. We also include an option to plot a raster map with the sampled locations of the populations. 

We define a few variables within the function that help keep track. Note that we keep track of the cells by their raster cell number (which goes from 1:ncells). Here's what the code does:

- Extract all cells that are habitat and store cell number in **HabitatCells**.
- Randomly sample one habitat cell and store its cell number in **Selected**.
- Store cell numbers of all remaining habitat cells in **Remaining**.
- Create a 'while' loop that continues until one of two things happens:
    - Sample size 'n' is reached.
    - There are no cells left in Remaining.
- Inside the loop:
    - Randomly sample one habitat cell and store its number in **Candidate**.
    - Remove the Candidate from Remaining (we don't want to consider it twice).
    - Calculate the **Distance** between Candidate and all populations in Selected. The function 'xyFromCell' gets the cell coordinates for each cell number, and the function 'pointDistance' calculates the distance between two sets of coordinates, here the coordinates for Candidate and for all cells in Selected. The argument 'lonlat=FALSE' tells 'pointDistance' that the coordinates are Euclidean.
    - If the minimum of Distance is larger than 'minDist', add a population. This is done by appending the value in Candidate to the vector Selected.
    - Repeat.
- If requested, the raster map is plotted, cell coordinates for all populations (Selected) are extracted and added to the map as points with point symbol pch=16 (filled circle).


``` r
createpops <- function(n=10, minDist=5, landscape=r, habitat=1, plot=TRUE)
{ 
  HabitatCells <- c(1:length(values(landscape)))[values(landscape)==habitat]
  Selected <- sample(HabitatCells, 1)
  Remaining <- HabitatCells[!is.element(HabitatCells, Selected)]
  while (length(Selected) < n & length(Remaining) > 0)
  {
    Candidate <- sample(Remaining, 1)
    Remaining <- Remaining[!is.element(Remaining, Candidate)]
    Distances <- raster::pointDistance(raster::xyFromCell(landscape, Candidate), 
                               raster::xyFromCell(landscape, Selected), 
                               lonlat=FALSE)
    if(min(Distances) > minDist)
    {
      Selected <- append(Selected, Candidate)
    }
  }
  if(plot==TRUE) 
  {
    plot(landscape)  
    points(xyFromCell(landscape, Selected), pch=16)
  }
  return(Selected)
}
```

Test the function above:


``` r
createpops(n=8, minDist = 3, landscape = r, plot = TRUE)
```

<img src="08-Week08_files/figure-html/unnamed-chunk-19-1.png" alt="" width="672" />

```
## [1] 1328  476  395 2497  440  683  601 1189
```

#### c. Initialise a metapopulation {-} 

We use the function 'init.popgensim' from package 'PopGenReport' to initialise a metapopulation based on the grid cells that we just selected. To do this we need to initialise a number of parameters (the locations of the subpopulations, the number of individuals per subpopulation, the number of loci and alleles per loci. For a full list check '?init.popgensim').

To store all the parameters we create a list called para where we store all of them

### 3. Define simulation parameters {-}

#### a. Define your metapopulation {-}

Define metapopulation: 


``` r
para<- list()
#Define populations (dynamics)
para$n.pops=8
para$n.ind=100

para$sex.ratio <- 0.5
#age distribution....

para$n.cov <- 3 
#number of covariates (before the loci in the data.frame, do not change this!!)
```

Define population dynamics:


``` r
#reproduction
para$n.offspring = 2

#migration
para$mig.rate <- 0.1 

#dispersal: exponential dispersal with maximal distance in map units
para$disp.max=50   #average  dispersal of an individual in meters
para$disp.rate = 0.05 #proportion of dispersing individuals

#Define genetics
para$n.allels <- 10
para$n.loci <- 20
para$mut.rate <- 0.001
```

Define cost distance method:


``` r
par(mar=c(1,1,1,1))
para$method <- "leastcost" #rSPDdistance, commute
para$NN <- 8  #number of neighbours for the cost distance method

# Initialize simulation of populations from scratch

 landscape<- r  #<-raster(system.file("external/rlogo.grd", package="raster"))

# Define x and y locations
 
 para$cells <- createpops(n=para$n.pops, minDist = 3, 
                         landscape = landscape, plot = FALSE)
 para$locs <- raster::xyFromCell(landscape, para$cells)
 
 #give the population some names 
 rownames(para$locs) <- LETTERS[1:para$n.pops]
  
  
# Create a matrix of pairwise cost distances...  
 
  cost.mat <- PopGenReport::costdistances(landscape, para$locs, 
                                          para$method, para$NN)
  
# ... and a matrix of pairwise Euclidean distances
  
  eucl.mat <- as.matrix(dist(para$locs))  #needed for the analysis later

# Plot your landscape with the populations....
  plot(landscape)
  points(para$locs[,1], para$locs[,2], pch=16, cex=2, col="orange")
  text(para$locs[,1],para$locs[,2], row.names(para$locs), cex=1.5)
```

<img src="08-Week08_files/figure-html/unnamed-chunk-22-1.png" alt="" width="672" />

``` r
# Check the parameter list
  
  para
```

```
## $n.pops
## [1] 8
## 
## $n.ind
## [1] 100
## 
## $sex.ratio
## [1] 0.5
## 
## $n.cov
## [1] 3
## 
## $n.offspring
## [1] 2
## 
## $mig.rate
## [1] 0.1
## 
## $disp.max
## [1] 50
## 
## $disp.rate
## [1] 0.05
## 
## $n.allels
## [1] 10
## 
## $n.loci
## [1] 20
## 
## $mut.rate
## [1] 0.001
## 
## $method
## [1] "leastcost"
## 
## $NN
## [1] 8
## 
## $cells
## [1]  450  893 2358  388 2425  294 1393  679
## 
## $locs
##    x  y
## A 49 41
## B 42 32
## C  7  2
## D 37 42
## E 24  1
## F 43 44
## G 42 22
## H 28 36
```

#### b. Initialise your population on the landscape {-}  

Now finally we can initialise our population using the init function. We'll call it 'simpops.0' to indicate that this is the initial generation.


``` r
simpops.0 <- PopGenReport::init.popgensim(para$n.pops, para$n.ind, 
                           para$sex.ratio, para$n.loci, 
                           para$n.allels, para$locs, para$n.cov )  
```

You may want to check the simpops object, which is simply a list of our subpopulation and each individual is coded in a single run in one of the subpopulations.


``` r
names(simpops.0)  #the names of the subpopulations
```

```
## [1] "A" "B" "C" "D" "E" "F" "G" "H"
```

``` r
head(simpops.0$A[,1:6]) ## a.list of the first 6 individuals and columns of population A
```

```
##   pop    sex age locus1A locus1B locus2A
## 1   1 female  NA       6      10       2
## 2   1 female  NA       4       3       6
## 3   1 female  NA       2      10       9
## 4   1 female  NA       7       9       5
## 5   1 female  NA       3       1       9
## 6   1 female  NA       6       7       3
```

We can also analyse our simpop  object. (e.g. calculate the pairwise Fst value between all the populations).

To be able to do that we first need to convert it into a genind object (because many functions need this type of object as input).


``` r
gsp <- PopGenReport::pops2genind(simpops.0, locs =para$locs)
gsp #check the genind object
```

```
## /// GENIND OBJECT /////////
## 
##  // 800 individuals; 20 loci; 200 alleles; size: 738.5 Kb
## 
##  // Basic content
##    @tab:  800 x 200 matrix of allele counts
##    @loc.n.all: number of alleles per locus (range: 10-10)
##    @loc.fac: locus factor for the 200 columns of @tab
##    @all.names: list of allele names for each locus
##    @ploidy: ploidy of each individual  (range: 2-2)
##    @type:  codom
##    @call: df2genind(X = res, sep = "/", ind.names = rownames(res), pop = combine$pop)
## 
##  // Optional content
##    @pop: population of each individual (group size range: 100-100)
##    @other: a list containing: xy
```

``` r
summary(gsp)  #some summary statistics
```

```
## 
## // Number of individuals: 800
## // Group sizes: 100 100 100 100 100 100 100 100
## // Number of alleles per locus: 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
## // Number of alleles per group: 200 200 200 200 200 200 200 200
## // Percentage of missing data: 0 %
## // Observed heterozygosity: 0.9 0.89 0.89 0.89 0.9 0.89 0.9 0.92 0.9 0.9 0.91 0.9 0.9 0.88 0.9 0.9 0.91 0.9 0.89 0.88
## // Expected heterozygosity: 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9
```

``` r
round(mmod::pairwise_Gst_Nei(gsp),5)
```

```
##          A        B        C        D        E        F        G
## B -0.00012                                                      
## C  0.00006  0.00001                                             
## D -0.00019 -0.00060  0.00022                                    
## E  0.00010 -0.00030 -0.00014 -0.00058                           
## F  0.00016 -0.00031  0.00002 -0.00019 -0.00009                  
## G -0.00020 -0.00046  0.00022 -0.00023 -0.00027 -0.00013         
## H -0.00045 -0.00013 -0.00009 -0.00008 -0.00007 -0.00033 -0.00020
```

Is there an effect of the landscape on the population structure (there should not be after initialisation)?

The function 'pairwise.fstb' is around 150 times faster than mmod::pairwise_Gst_Nei, but slightly different.


``` r
gen.mat <- PopGenReport::pairwise.fstb(gsp)  
round(gen.mat ,5)
```

```
##         A       B       C       D       E       F       G       H
## A 0.00000 0.00239 0.00256 0.00231 0.00260 0.00266 0.00231 0.00205
## B 0.00239 0.00000 0.00252 0.00191 0.00220 0.00220 0.00205 0.00238
## C 0.00256 0.00252 0.00000 0.00273 0.00237 0.00253 0.00273 0.00241
## D 0.00231 0.00191 0.00273 0.00000 0.00193 0.00232 0.00228 0.00243
## E 0.00260 0.00220 0.00237 0.00193 0.00000 0.00241 0.00224 0.00244
## F 0.00266 0.00220 0.00253 0.00232 0.00241 0.00000 0.00237 0.00218
## G 0.00231 0.00205 0.00273 0.00228 0.00224 0.00237 0.00000 0.00230
## H 0.00205 0.00238 0.00241 0.00243 0.00244 0.00218 0.00230 0.00000
```

Now we perform a two partial Mantel tests, one for the effect of the cost distance partialling out the effect of Euclidean distance (Gen ~cost | Euclidean), and one the other way round. The method 'wassermann' from the 'PopGenReport' package returns a data frame with two rows (one for each test) and three columns (model, r = Mantel r statistic, p = p-value), following this method:

- Wassermann, T.N., Cushman, S. A., Schwartz, M. K. and Wallin, D. O. (2010). Spatial scaling and multi-model inference in landscape genetics: Martes americana in northern Idaho. Landscape Ecology, 25(10), 1601-1612.


``` r
PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), 
                           gen.mat = gen.mat, plot=F)$mantel.tab
```

```
##                   model       r     p
## 1 Gen ~cost | Euclidean  0.2056 0.157
## 2 Gen ~Euclidean | cost -0.0571 0.603
```

Check the pairwise Fst values, why are they so low? Hints:

- How were genotypes assigned to the initial generation
- How many generations have we simulated thus far?
- At this point in the simulation, do you expect to see an effet of IBD, IBR, or neither?

### 4. Run simulations and analyze results {-}

#### a. Run your simulation over multiple time steps (years) {-} 

Now we can run our simulation by simply passing our object 'simpops' to the function 'run.popgensim', with some additional parameters that are needed for the simulation. We specify the number of generations the simulation should run with the steps parameter. (Check ?run.popgensim for a description of all parameters).

Important to understand is the idea of the cost.mat (which is the cost matrix that is used for the distance between subpopulation). The n.alleles, n.ind cannot be different from the initialisation.


``` r
simpops <- PopGenReport::run.popgensim(simpops.0, steps=3, cost.mat, 
                         n.offspring=para$n.offspring, n.ind=para$n.ind,
                         para$mig.rate, para$disp.max, para$disp.rate, 
                         para$n.allels, para$mut.rate,
                         n.cov=para$n.cov, rec="none")
```

In essence we were running a metapopulation with 100 individuals per subpopulation on our resistance landscape for 3 generations. The question is now was that enough time to create an effect on population structure?

#### b. Analyse your simulated population with a partial Mantel test {-}

Let's check the pairwise Fst values and then do a landscape genetic analysis using partial Mantel tests.

Convert to genind to calculate pairwise Fst.


``` r
gsp <- PopGenReport::pops2genind(simpops, para$locs, para$n.cov)
```

Calculate your genetic distance matrix e.g. fst or D.


``` r
gen.mat <- PopGenReport::pairwise.fstb(gsp)   
round(gen.mat ,3)
```

```
##       A     B     C     D     E     F     G     H
## A 0.000 0.006 0.007 0.006 0.007 0.008 0.008 0.007
## B 0.006 0.000 0.008 0.007 0.006 0.008 0.008 0.008
## C 0.007 0.008 0.000 0.007 0.007 0.008 0.007 0.008
## D 0.006 0.007 0.007 0.000 0.007 0.007 0.007 0.007
## E 0.007 0.006 0.007 0.007 0.000 0.008 0.007 0.008
## F 0.008 0.008 0.008 0.007 0.008 0.000 0.008 0.007
## G 0.008 0.008 0.007 0.007 0.007 0.008 0.000 0.008
## H 0.007 0.008 0.008 0.007 0.008 0.007 0.008 0.000
```

Partial Mantel test:


``` r
PopGenReport::wassermann(eucl.mat = eucl.mat, cost.mats = list(cost=cost.mat), 
             gen.mat = gen.mat, plot=F)$mantel.tab
```

```
##                   model       r     p
## 2 Gen ~Euclidean | cost  0.1857 0.161
## 1 Gen ~cost | Euclidean -0.1086  0.74
```
We can extract a specific value from this result, e.g., the p-value of the test "Gen ~cost | Euclidean". (Note that every time we call the function 'wassermann', a permutation test is performed (default: 'nperm = 999'), and the p-value may thus vary somewhat).


``` r
res <- PopGenReport::wassermann(eucl.mat = eucl.mat, 
                                cost.mats = list(cost=cost.mat), 
                                gen.mat = gen.mat, plot=F)$mantel.tab
res[res$model == "Gen ~cost | Euclidean", "p"]
```

```
## [1] "0.717"
```

#### c. Optional: Analyze your simulated populations with MLPE {-}

We will cover this method, and model selection, in more detail in Week 12. 

Install package `corMLPE` if needed:


``` r
if(!requireNamespace("corMLPE", quietly = TRUE)) 
  remotes::install_github("nspope/corMLPE")
```

To run the model, we need to extract the vector of pairwise distances from the three distance matrices:


``` r
eucl.vect <- as.vector(as.dist(eucl.mat))
cost.vect <- as.vector(as.dist(cost.mat))
gen.vect <- as.vector(as.dist(gen.mat))
```

Define the two vectors of population effects: 


``` r
Pop <- matrix(names(as.dist(eucl.mat))[[1]], nrow(eucl.mat), ncol(eucl.mat), byrow=F)
pop1 <-Pop[lower.tri(Pop)]
pop2 <-t(Pop)[lower.tri(Pop)]
```

Assemble the link-based dataset:


``` r
Link.data <- data.frame(gen.vect, eucl.vect, cost.vect, pop1, pop2)
```

Fit and compare three models:

- **G**: Geographic distance model (IBD)
- **E**: Ecological distance model (IBR)
- **GE**: Both

We will compare the three models by `AICc`, the small-sample version of AIC (see Week 12). The lower the value of AICc, the better is the model fit. The method includes a correction for the number of predictors in the model, so that we can make a fair comparison between GE (two predictors) to G and E (one predictor each). Note that because we are comparing models with the same random effects (population effects `pop1` and `pop2`, but with different fixed effects, we fit the models with maximum likelihood, `ML` (see Week 6). 


``` r
GE <- nlme::gls(gen.vect ~ eucl.vect + cost.vect, 
                  correlation=corMLPE::corMLPE(form=~pop1+pop2), 
                  data=Link.data, method="ML")
G <- update(GE,  ~ eucl.vect)
E <- update(GE,  ~ cost.vect)
MuMIn::AICc(GE, G, E)
```

```
##    df      AICc
## GE  5 -329.2276
## G   4 -331.9044
## E   4 -330.7569
```

Which is the best model? 


``` r
tmp <- MuMIn::AICc(GE, G, E)
row.names(tmp)[tmp$AICc == min(tmp$AICc)]
```

```
## [1] "G"
```

Let's combine all of this into our own function to extract the vectors, define the population effects, run MLPE, and extract the name of the best fitting model


``` r
getMLPE <- function(gen=gen.mat, eucl=eucl.mat, cost=cost.mat)
{
  Pop <- matrix(names(as.dist(eucl))[[1]], nrow(eucl), ncol(eucl), byrow=F)
  Link.data <- data.frame(
    eucl.vect = as.vector(as.dist(eucl)),
    cost.vect = as.vector(as.dist(cost)),
    gen.vect = as.vector(as.dist(gen)),
    pop1 = Pop[lower.tri(Pop)],
    pop2 = t(Pop)[lower.tri(Pop)]
  )
  GE <- nlme::gls(gen.vect ~ eucl.vect + cost.vect, 
                  correlation=corMLPE::corMLPE(form=~pop1+pop2), 
                  data=Link.data, method="ML")
  G <- update(GE,  ~ eucl.vect)
  E <- update(GE,  ~ cost.vect)
  tmp <- MuMIn::AICc(GE, G, E)
  return(row.names(tmp)[tmp$AICc == min(tmp$AICc)])
}
```

Let's test it:


``` r
getMLPE()
```

```
## [1] "G"
```

### 5. Run simulator using a previously defined parameter set {-}

Once the simulator works and you are certain that you understand how the simulator needs to be set up for a single run, in almost all studies on simulations you want to be able to re-run the simulator in an automatized way. There are several reasons why you want to do that.

1. You want to perform a sensitivity analysis on a single parameter, which means, try to find how much does the output (e.g. pairwise Fst between subpopulations) change when you vary an input parameter (e.g. number of loci). 
2. You want to explore the "complete" parameter space, which means, instead of changing  values of a single input parameter you want to change all parameters (within certain levels) and run their combinations. 
3. Another reason is that you want to create a simulated test data set that forms the backbone of your future studies.

So we would like to do the following. 

- a) Specify and record all the parameter combinations that you would like to run.  
- b) Run the simulator with every combination 
- c) [Optional] save your complete simulation run (recommended, but sometimes prohibitive due to needed resources) or only a calculated summary.
- d) Read in your simulations, analyse them and synthesize your results via additional statistics, tests, plots.
- e) Publish an enormously important paper....


Admittedly there are several different approaches and as it seems every modeller has a slightly different way to achieve these steps. One approach is to create a parameter file that records all the parameter setting for each run. Another approach is to create so called scripts for every single run. The advantage here is that scripts can be easily distributed across different cores and machines and therefore this approach is easy to parallelise your runs, which sometimes is necessary. Finally the approach I will present here (also because of practical reasons) is to create an R data.frame that stores all the parameter settings and we run all combinations in serial mode instead of in parallel. 

Okay before we start we need to think about what kind of parameters we want to explore. I would like to do the following runs:

- Run our simulations as above (same parameter settings) for varying time steps (say between 5 to 45 years in steps of 20). We'll keep the number of levels and the maximum number of steps low in this example to limit computation time. Feel free to expand! 
- As output I would still like to record the Fst value, but also the full genetic data set and the parameters used to run the simulation.
- In addition I want to repeat each run 5 times (most often you would do more repetitions) to check how much general variation there is between runs with exactly the same parameter combination. 

#### a. Specify and record the parameter combinations {-}

Let's define the varying numbers of time steps we would like to run the simulations. Here we define a sequence from 5 to 45 in steps of 20, which results in a series c(5, 25, 45). We will interpret these values as numeric, therefore we don't convert to 'factor'. 


``` r
timesteps <- seq(from=5 , to=45, by=20)
```

We also specify the number of repeats (replicate simulation runs). We want to do five replicate simulation runs per for each level of 'time', and we will label replicates from 1 through 5. These are essentially labels and we'll save them as a factor:


``` r
repeats <- factor(1:5)
```

Now we would like to have a data frame that stores all possible combinations for those two parameters. As simple way to do that in R, is to use the 'expand.grid' function.


``` r
para.space <- expand.grid(rep=repeats, time=timesteps)
tibble::as_tibble(para.space)
```

```
## # A tibble: 15 × 2
##    rep    time
##    <fct> <dbl>
##  1 1         5
##  2 2         5
##  3 3         5
##  4 4         5
##  5 5         5
##  6 1        25
##  7 2        25
##  8 3        25
##  9 4        25
## 10 5        25
## 11 1        45
## 12 2        45
## 13 3        45
## 14 4        45
## 15 5        45
```

As you can see this results in 15 combinations (3 time steps x 5 repeats). The beauty of this approach is that it is very flexible and adaptable to runs over other parameter combinations, as you can provide more than two parameter variables to 'expand.grid'.

#### b. Run the simulator over every parameter combination {-}

Remember our parameters are all defined in the 'para' object (a list) and we want to keep them constant, except for running the simulation for different number of years. This means that we only need to modify the argument 'steps'.

Summarizing the code from above, a single run of our simulator runs via:


``` r
#initialize
simpops.0 <- PopGenReport::init.popgensim(para$n.pops, para$n.ind,
                           para$sex.ratio, para$n.loci, 
                           para$n.allels, para$locs, para$n.cov )  
#run for 20 generations
simpops <- PopGenReport::run.popgensim(simpops.0, steps=20, cost.mat, 
                         n.offspring=para$n.offspring, n.ind=para$n.ind,
                         para$mig.rate, para$disp.max, para$disp.rate, 
                         para$n.allels, para$mut.rate,
                         n.cov=para$n.cov, rec="none")
```

We adapt this code as follows:

- Create a 'for' loop that cycles through every row 'i' in 'para.space'
- For each value of 'i':
    - Initialize population 'simpops.0.'
    - Run the simulation with argument 'steps = para.space$time[i]'.

We are not running the code just yet, hence it is commented-out with '#'.


``` r
#for (i in 1:nrow(para.space))
#{
#  #initialize
#  simpops.0 <- PopGenReport::init.popgensim(para$n.pops, para$n.ind, 
#                           para$sex.ratio, para$n.loci, para$n.allels, 
#                           para$locs, para$n.cov )
#  
#  #run for para.space$time[i] generations
#  simpops <- PopGenReport::run.popgensim(simpops.0, 
#                           steps=para.space$time[i], cost.mat,
#                           n.offspring=para$n.offspring, n.ind=para$n.ind,
#                           para$mig.rate, para$disp.max, para$disp.rate, 
#                           para$n.allels, para$mut.rate,
#                           n.cov=para$n.cov, rec="none")
#}
```

Have a close look at the change.

Question: what changes between replicate runs, and what not? Consider the following aspects:

- Landscape
- Population locations
- Pairwise distances (cost, Euclidean)
- Initial populations with initial genotypes
- Migration and gene flow

#### c. Save your complete simulation run (input and output) {-}

Simply running the simulation 15 times (number of rows in 'para.space') by itself is not useful yet. We need to store the simulation runs somehow, so we can collect them afterwards to calculate summary statistics and analyse the runs. 

How do we store the repeats seperately in a file? One approach would be to have a different file name for every repeat, but in my view, a cleaner approach is to store all simulation outputs and also store the complete parameter and input information in a file, so everything that is need is in one place. A nice way to do that in R is to create a 'list' object that stores all in a single object, which can be saved (and is automatically packed) and re-loaded as an R object. This is convenient as long as I only want to analyze the results in R, not export to other software. 

Here we do the following:

- Create a timer with the function 'proc.time' so that we know roughly how long the computations take.
- For each line 'i' in 'para.space':
    - Initialize simpops.0.
    - Run the simulation with 'steps=para.space$time[i]'.
    - Convert the resulting 'simpop' to a genind object 'gi' (smaller to store)
    - Create a list 'sim' of all simulation parameters we want to store. 
    - Save the object 'sim' as an 'RData' file with a unique file name in the folder 'output/simout' in the project directory.
    - Print a message after each run to report progress and computation time.
    - Flush the output console to make sure it is current (only relevant for console-based versions of R).

The list 'sim' will contain the following elements. Note: the name is repeated (e.g., 'gi = gi') to create a named list, i.e., to specify the names of the list elements.

- **para.space**: the row 'para.space[i]' with settings of the simulation run.
- **para**: a copy of the list 'para' that contains the other parameters that are the same for all simulation runs.
- **landscape**: the landscape.
- **cost.mat**: the matrix of pairwise cost distances
- **gi**: the genind object that contains the genotypes at the end of the simulation.

First we make sure the folder `simout` exists within the `output` folder in the R project:


``` r
if(!dir.exists(paste0(here::here(),"/output"))) 
  dir.create(paste0(here::here(),"/output"))
if(!dir.exists(paste0(here::here(),"/output/simout")))
  dir.create(paste0(here::here(),"/output/simout"))
```



``` r
## c.eate a timer (just to know how long it will take roughly)
timer0 <- round(proc.time()[3],2)

for (i in 1:nrow(para.space))
{
  ## i.itialize
  simpops.0 <- PopGenReport::init.popgensim(para$n.pops, para$n.ind, 
                           para$sex.ratio, para$n.loci, para$n.allels, 
                           para$locs, para$n.cov )  
  
  # run for para.space$time[i] generations
  simpops <- PopGenReport::run.popgensim(simpops.0, 
                           steps=para.space$time[i], cost.mat, 
                           n.offspring=para$n.offspring, n.ind=para$n.ind,
                           para$mig.rate, para$disp.max, para$disp.rate, 
                           para$n.allels, para$mut.rate,
                           n.cov=para$n.cov, rec="none")
  
  ## c.nvert to genind object (smaller)
  gi <- PopGenReport::pops2genind(simpops)
  
  ## c.eate a list of all I want to collect
  sim <- list(para.space=para.space[i,], para=para, 
              landscape=landscape, cost.mat=cost.mat, gi=gi)
  
  # save everything in an output folder (with a consecutive number, with three leading zeros, so the file sorting is nicer)

  save(sim, file = paste0(here::here(),"/output/simout/sim_time5-45_rep5_",
                          sprintf("%03i",i) ,".RData"))
  
  cat(paste0("Finished run: ", i," out of ",nrow(para.space),
            ". So far, it took: ", round(proc.time()[3]-timer0,2)," sec.\n"))
  flush.console()
}
```

```
## Finished run: 1 out of 15. So far, it took: 0.31 sec.
## Finished run: 2 out of 15. So far, it took: 0.65 sec.
## Finished run: 3 out of 15. So far, it took: 0.95 sec.
## Finished run: 4 out of 15. So far, it took: 1.26 sec.
## Finished run: 5 out of 15. So far, it took: 1.56 sec.
## Finished run: 6 out of 15. So far, it took: 2.88 sec.
## Finished run: 7 out of 15. So far, it took: 4.25 sec.
## Finished run: 8 out of 15. So far, it took: 5.58 sec.
## Finished run: 9 out of 15. So far, it took: 6.91 sec.
## Finished run: 10 out of 15. So far, it took: 8.25 sec.
## Finished run: 11 out of 15. So far, it took: 10.61 sec.
## Finished run: 12 out of 15. So far, it took: 13.34 sec.
## Finished run: 13 out of 15. So far, it took: 15.72 sec.
## Finished run: 14 out of 15. So far, it took: 18.07 sec.
## Finished run: 15 out of 15. So far, it took: 20.37 sec.
```

#### d. Analyze and synthesize results {-} 

If you check your output folder (simout) you should see 15 files.

Note: File paths can be different when you execute a chunk in an R notebook compared to when you copy-paste the same line into the console! We avoid this problem by using the function 'here' from package 'here'. 


``` r
head(dir(paste0(here::here(), "/output/simout")))
```

```
## [1] "sim_time5-45_rep5_001.RData" "sim_time5-45_rep5_002.RData"
## [3] "sim_time5-45_rep5_003.RData" "sim_time5-45_rep5_004.RData"
## [5] "sim_time5-45_rep5_005.RData" "sim_time5-45_rep5_006.RData"
```

Now we are at step D where we need to read in all our files one by one, calculate some summary statistics and plot our results. 

Again, this could be easy, but be aware if you have thousands of files it could take quite some time and memory. The most convenient way is to load everyting and store it in a list, so we can access all of our simulations from memory. I will show how to do this in the example below, but be aware in larger simulations (think millions of runs, or large sample sizes) this is not possible and we would load a single simulation, calculate a statistic, store only the result in a table and free the memory for the next simulation run.

Let's load our simulation runs. There is one caveat: when we load the object 'sim' from the '.RData file', we can't assign it a new object name. I.e., we can't use 'newName <- load("sim.Rdata"). Instead, we can only type 'load("sim.Rdata")' and it will create or overwrite the object 'sim'. Also, R takes the name from the object that was saved, not from the file name. Hence, once we load the object, any existing object of the same name will be overwritten. So if we want to keep, it we need to rename it before using 'load'.

Here we do the following:

- Create an empty table with three columns 'rep', 'time', 'fst' and other columns to collect summary results for the 15 simulation runs. 
- Create a vector that contains all filenames. The function 'list.files' does just that. We specify with path="./simout"' that the files in folder 'simout' should be listed, and with 'pattern="sim" we specify that we want all file names that contain the expression "sim" (we could also have used "time", for example - any component of the file name that is shared by all the files we are interested in but no other files that might be in the same folder).
- Loop through the files. For each filename 'i':
    - Load the file, which will create or overwrite the object 'sim'. We need to supply not only the file name but the path, hence 'paste0("./simout/",filenames[i])' (see Week 8 video for more on file paths).
    - Extract simulation parameters: Copy the ith row from 'para.space' (repeat number, timesteps) into the first two columns of the ith row of 'res'.
    - Extract the genind object 'gi' with the final genotypes.
    - Calculate the mean of pairwise fst values and store in the third column of the ith row of 'res'.
    - Perform partial Mantel tests with function 'wasserman' and store the Mantel r statistics and the p-values in the corresponding columns. Note that this is somewhat tricky because the function 'wasserman' ranks the models and the better fitting model is listed in the first row, so that the order of the models can vary between runs. Therefore we extract the values based on model name.
    - Perform 'Sunder' covariance analysis and extract which model is best supported ('G', 'E' or 'GE').


``` r
res <- data.frame(rep=NA, time=NA, fst=NA, r.Eucl=NA, p.Eucl=NA, 
                  r.cost=NA, p.cost=NA, MLPE=NA)

#load all files in the folder
filenames <- list.files(path= paste0(here::here(), "/output/simout"), pattern="sim")

for (i in 1:length(filenames))
{
  #creates a sim object
  load(paste0(here::here(), "/output/simout/",filenames[i]))

  #now let us take what we need from the simulation
  res[i,1:2] <- sim$para.space
  
  #calculate a summary statistic: mean of pairwise fst values
  ## h.re we only take the lower triangle of the matrix to avoid the diagonal values,
  # which are zero by definition (comparing each population to itself)
  gen.mat <- PopGenReport::pairwise.fstb(sim$gi)  
  res [i,3] <- mean(gen.mat[lower.tri(gen.mat)])
  
  #Distance matrices
  eucl.mat <- dist(sim$para$locs)
  cost.mats = list(cost=sim$cost.mat)
  
  #partial Mantel tests
  wass <- PopGenReport::wassermann(eucl.mat, 
                                cost.mats = list(cost=sim$cost.mat), 
                                gen.mat = gen.mat, plot=F)$mantel.tab
  res[i,4:5] <- wass[wass$model == "Gen ~Euclidean | cost", 2:3]
  res[i,6:7] <- wass[wass$model == "Gen ~cost | Euclidean", 2:3]
  
  #Sunder
  res[i,8] <- getMLPE(gen=gen.mat, eucl=eucl.mat, cost=sim$cost.mat)
}
```

Look at the 'res' data frame and check the results. 


``` r
head(res)
```

```
##   rep time         fst  r.Eucl p.Eucl  r.cost p.cost MLPE
## 1   1    5 0.009270325  0.0725  0.324 -0.0206  0.506    G
## 2   2    5 0.009484194 -0.2763  0.941  0.5356  0.013    E
## 3   3    5 0.008522157 -0.1102  0.699  0.2604  0.149    E
## 4   4    5 0.009055060 -0.0172  0.527   0.056  0.415    G
## 5   5    5 0.009037522  0.1347  0.216 -0.0211  0.521    G
## 6   1   25 0.022749633 -0.3674  0.992  0.6932  0.003    E
```

The next step would be to visualise the results (e.g. plot runs over times and color by rep). A quick way to do that is to use the function 'ggplot' from the 'ggplot2' package. Here we add a jitter to keep points from overlapping too much.


``` r
ggplot2::ggplot(res, ggplot2::aes(x=time, y=fst)) + 
         ggplot2::geom_point(position = ggplot2::position_jitter(w = 0.5))
```

<img src="08-Week08_files/figure-html/unnamed-chunk-51-1.png" alt="" width="672" />

Now it is again time for you to experiment. For example, why not set up a simulation that varies the number of loci. Or as you may have seen even after 100 generation there was no sign that the mean pairwise Fst value is levelling off. So how long do you have to run a simulation in terms of time to see this (be aware that simulation runs take longer if you increase the number of timesteps)?

Questions:

- How would you set up a simulation experiment to compare type I error rates between partial Mantel test and Sunder?
- How about statistical power? 

Have fun and please give us feed back what you think about this Worked Example. [Bernd Gruber](mailto:bernd.gruber@canberra.edu.au), [Erin Landguth](erin.landguth@gmail.com), [Helene Wagner](mailto:helene.wagner@utoronto.ca).


## R Exercise Week 8

**Task:** Carry out a permutation test for the Mantel rank correlation to test for fine-scale spatial genetic structure in *Pulsatilla vulgaris* adults, using the pooled data from all seven patches. Use a one-sided alternative "greater", as we expect the Mantel rank correlation to be positive.

**Hints:**

  - Exclude all pairs that involve individuals from different patches.
  - Permute individuals only within patches, not between patches. 
  - Calculate the Mantel rank correlation for the observed data (M.rho.obs)
  - For each of R = 499 permutations, calculate M.rho.sim
  - Determine the approximate p-value of the one-sided test with alternative "greater" as the percentage of the 500 values (1 observed, 499 permuted) that are greater or equal to the observed one. 

a) **Load packages**: You may want to load the packages `dplyr` and `gstudio`. Alternatively, you can use `::` to call functions from packages.


b) **Import data, extract adults**. Use the code below to import the data into `gstudio` and extract adults (`OffID == 0`). 


``` r
library(dplyr)
Pulsatilla.gstudio <- gstudio::read_population(path=system.file("extdata",
                            "pulsatilla_genotypes.csv", 
                            package = "LandGenCourse"), 
                    type="column", locus.columns=c(6:19), 
                    phased=FALSE, sep=",", header=TRUE)
Adults.gstudio <- Pulsatilla.gstudio %>% filter(OffID == 0)
```

c) **Sort individuals by patch**. Create a new ID variable `Names` that combines the existing variables `Population` and `ID` (starting with population). Then use the function  `dplyr::arrange` to sort `Adults.gstudio` by `Names` (check the help file for `arrange`). This is important here for two reasons:

    - In order to efficiently permute individuals within patches, they need to be sorted by patch.
    - In some cases, the function `gstudio::genetic_distance` will sort the distance matrix alphabetically. To avoid mismatches between the data and the resulting distance matrix, it is best to sort the data alphabetically already. 

d) **Calculate Euclidean distances**: Use the metric coordinates in variables "X" and "Y" to calculate Euclidean distances (see Week 5, section 4). Here it is important to store the distances `Dgeo` in the full matrix format, with `as.matrix`. 

e) **Calculate genetic distances (Dps)**: Use the following code (as needed (adapt if needed) to calculate individual-level genetic distances (proportion of shared alleles) and store them in the full matrix format. We subtract values from 1 to obtain a distance measure. Note: the calculation in `gstudio` is based on Bray distance and the resulting values are proportional to those calculated by `adegenet::propShared`. I.e., the two measures have a correlation of 1 but the actual values differ.

    `Dgen <- 1 - as.matrix(gstudio::genetic_distance(Adults.gstudio, stratum="Names", mode="dps"))`


f) **Plot distances, calculate M.rho**: Create a plot of Dgen vs Dgeo, and calculate the Mantel rank correlation. Note: the function 'cor' with default settings does not allow missing values (NA). This can be changed e.g. with the argument `use`.

    - Use `as.dist` to access only the values from the lower triangle of each matrix. 
    - The function `plot` will do. Inspect the plot. Where would you find the pairs of individuals within the same patch? 
    - Use the function `cor` with `method = "spearman"` to calculate the Mantel rank correlation. Allow for missing values with the argument `use = "complete.obs"`.

g) **Limit to within-patch pairs**: Restrict the analysis to pairs within the same patch.
For this, we want to set all between-site comparisons in `Dgeo` to `NA`. Uncomment the code below to:

    - Create a matrix that is TRUE if the two individuals are in the same patch, and FALSE if not (first line)
    - Change FALSE to NA (second line)
    - Multiply Dgeo by this matrix to set distances between patches to NA (third line).
    - Adapt your code from above to plot the distances Dgen vs. Dgeo.within.
    - Calculate the Mantel rank correlation between Dgen vs. Dgeo.within and store it as `Cor.obs`. 


``` r
#SamePatch <- outer(Adults.gstudio$Population, Adults.gstudio$Population, FUN = "==")
#SamePatch[SamePatch == "FALSE"] <- NA
#Dgeo.within <- SamePatch * Dgeo
```

  Note: check the help file or run the following examples to figure out what `outer` does: 
    `outer(c(1:5), c(1:5), FUN = "*")`
    `outer(c(1:5), c(1:5), FUN = "-")`
    `outer(LETTERS[1:5], c(1:5), FUN = "paste0")`
    
h) **Unrestricted permutation test**: Create a variable `Order` with row numbers (from 1 to the number of individuals in `Adults.gstudio`). Then, uncomment the code below (adapt as needed) to carry out a permutation test by permuting the order of individuals, R = 499 times. Notes to permute a distance matrix, we need to permute the rows and columns of the full distance matrix simultaneously with the same order: `Dgen[a,a]`. We only need to permute one of the two matrices (Dgen or Dgeo.within), but not both. The approximate p-value is calculated as the proportion of values (R simulated ones and 1 observed `Cor.obs`) that were as large, or larger, than `Cor.obs`. 


``` r
#R = 499
#Cor.perm.unrestricted <- rep(NA, R)
#for(r in 1:R)
#{
#  a <- sample(Order)
#  Cor.perm.unrestricted[r] <- cor(as.dist(Dgen[a,a]),as.dist(Dgeo.within), method="spearman", use="complete.obs")
#}
#approx.p.unrestricted <- mean(c(Cor.obs, Cor.perm.unrestricted) >= Cor.obs)
#approx.p.unrestricted
```

i) **Restricted permutation test**: Adapt the code to permute individuals only within patches. For this, split 'Order' by population, randomize row numbers within groups (list elements = populations) with `sapply`, and use `unlist` to convert to a vector again. Make sure to change object names from 'unrestricted' to 'restricted'.

    - Before the `for` loop, add this code: `b <- split(Order, Adults.gstudio$Population)`
    - Inside the `for` loop, replace the calculation of a : `a <- unlist(sapply(b, sample))`

k) **Compare results**: Create side-by-side boxplots of the simulated Mantel rank correlation values for the unrestricted and the restricted permutation tests. Note: if none of the simulated values was larger than Cor.obs, the approx. p-value will be 1/(R+1). This indicates the resolution of the permutation test, i.e., the smallest possible p-value given R.

**Questions:** 

- Did the direction, size or statistical significance of the observed Mantel rank correlation (as a measure of fine-scale spatial genetic structure in *P. vulgaris*) change between the unrestricted and the restricted permutation test? Why, or why not?
- How did the distributions of the simulated values of the Mantel rank correlation differ between the unrestricted and the restricted test? Can you think of a reason for this?





## Bonus: Efficient R {#bonus_8a}

Helene Wagner


















#### 1. Overview of Bonus Material {-}

##### a. Goals {-} 

This Bonus Material provides some introductory worked examples for:

- Navigating the file system
- Benchmarking file import and export functions
- Profiling an R script
- Creating and executing a Bash R script
- Parallelizing code

##### b. Data set {-}

We will use the wolf SNP data (Schweizer et al., 2016) from the Week 11 worked example. The genetic data are individual-based, and are input as allele counts (i.e. 0/1/2) for each locus. We are using a randomly sampled subset of 10,000 single nucleotide polymorphism (SNP) markers from the full data set (which contains 42,587 SNPs).

##### c. Required R packages {-}


``` r
library(LandGenCourse)
#library(microbenchmark)
#library(profvis)
#library(here)
#library(readr)
#library(data.table)
library(feather)
#library(rio)
#library(devtools)
#library(parallel)
#library(doParallel)
#library(knitr)
#library(compiler)
```

#### 2. Navigating the file system {-}

This Bonus material assumes that you are running R in RStudio within a 'R project'. Therefore, the default workspace when you enter commands in the console should be your R project folder. A discussed in the video, Part 2, the default location when executing code from an R Notebook is the folder where the notebook is stored. If you downloaded this Bonus material from 'LandGenCourse', it should be stored in a folder 'downloads' in your project folder.

More about R projects: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects.

##### a. Where am I? {-} 

Let's see where we are. 


``` r
getwd()
```

```
## [1] "/Users/helene/Desktop/Github_Projects/LandGenCourse_book/vignettes"
```

``` r
here::here()
```

```
## [1] "/Users/helene/Desktop/Github_Projects/LandGenCourse_book"
```

``` r
Sys.getenv("HOME")
```

```
## [1] "/Users/helene"
```

``` r
R.home()
```

```
## [1] "/Library/Frameworks/R.framework/Resources"
```

**Question:** Copy-paste the commands in the chunk above directly into the Console and run them there. Do you get the same paths?

- `getwd()`: If you execute the chunk, or knit the notebook, this will return the locatio of the R Notebook. If you copy-paste it into the console, it will return the project folder. 
- `here::here`: This should return the project folder in both cases. 
- `Sys.getenv("HOME")`: This should return your home directory on the local machine. Note: "HOME" must be in all capitals.
- `R.home`: This shows the location where R is installed on your computer.

##### b. Accessing a system file {-}

The example data set is available in the 'extdata' folder of the 'LandGenCoures' package. This is how we import the data in the Week 11 Worked Example:


``` r
gen <- read.csv(system.file("extdata", "wolf_geno_samp_10000.csv", 
                            package = "LandGenCourse"), row.names=1)
dim(gen)
```

```
## [1]    94 10000
```

Let's unpack this code. What does the function `system.file` do? Let's compare it to `R.home`.


``` r
R.home()
```

```
## [1] "/Library/Frameworks/R.framework/Resources"
```

``` r
system.file()
```

```
## [1] "/Library/Frameworks/R.framework/Resources/library/base"
```

So, `system.file` goes to the R home and, without additional arguments, locates the folder 'library' with the subfolder 'base'. 

What happens when we add the arguments? 

``` r
system.file(package = "LandGenCourse")
```

```
## [1] "/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourse"
```

``` r
system.file("extdata", "wolf_geno_samp_10000.csv", 
                            package = "LandGenCourse")
```

```
## [1] "/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/LandGenCourse/extdata/wolf_geno_samp_10000.csv"
```

Compare the paths to those from the previous chunk!

- When we specify the package argument, `system.file` modifies the path to the location where the package is stored.
- The remaining, unnamed arguments are interpreted as a path and added at the end: within package 'LandGenCourse', go to folder 'extdata' and locate file "wolf_geno_samp_10000.csv".

This is pretty cool! To be honest, I have no clue where such files are stored, and the absolute path would be different anyways on my Mac and on my Windows machine.

##### c. File manipulation {-}

We can use R's file manipulation functions to do things with the file before even importing the data. Let's check the file size:


``` r
myFile <- system.file("extdata", "wolf_geno_samp_10000.csv", 
                            package = "LandGenCourse")
file.size(myFile)
```

```
## [1] 2078748
```

``` r
cat("File size: ", file.size(myFile) / 10^6, " MB")
```

```
## File size:  2.078748  MB
```

The number returned is in byte, hence we divide by 1 million (10^6) to get megabyte (MB).

Other useful functions include (see help file: '?files'):

- `file.create`: creates file, truncates name if name already exists
- `file.exist`: checks whether a file with this name exists at location
- `file.remove`: attempts to delete file 
- `file.rename`: attempts to rename file
- `file.append`: attempts to append one file to another
- `file.copy`: copies file 'from' 'to' (set 'overwrite=TRUE' to allow overwriting an existing file)
- `download.file`: download a file from the internet
- `unzip`: unzip a zip archive (there is also a function `zip`, also `untar` and `tar` for tar archives)
- `dir`: list content of folder (=directory)
- `dir.exist`: check whether a folder with this name exists at location
- `dir.create`: creates a folder (does not work when knitting R Notebook!)

What other files are available? The function `dir` lists all files and subfolders in a specific folder. The question then is which folder to specify?

**Question:** What do you think the following commands will return?

- dir()
- dir(here::here())
- dir(system.file(package = "LandGenCourse"))
- dir(system.file("extdata", package = "LandGenCourse"))
- dir(myFile)

Give it a try:


``` r
dir()
```

```
##  [1] "radish_tutorial_fig1A.png" "radish_tutorial_fig1B.png"
##  [3] "WE12_Fig1.PNG"             "WE13_Fig1.png"            
##  [5] "WE9_Fig1.png"              "WE9_Fig2.png"             
##  [7] "Week0_BasicR.Rmd"          "Week0_Graphics.Rmd"       
##  [9] "Week1_vignette.Rmd"        "Week10_bonus_vignette.Rmd"
## [11] "Week10_vignette.Rmd"       "Week11_vignette.Rmd"      
## [13] "Week12_vignette.Rmd"       "Week13_vignette.Rmd"      
## [15] "Week14_vignette.Rmd"       "Week2_bonus_vignette.Rmd" 
## [17] "Week2_vignette.Rmd"        "Week3_vignette.Rmd"       
## [19] "Week4_vignette.Rmd"        "Week5_vignette.Rmd"       
## [21] "Week6_vignette.Rmd"        "Week7_bonus_vignette.Rmd" 
## [23] "Week7_vignette.Rmd"        "Week8_bonus_vignette.Rmd" 
## [25] "Week8_vignette.Rmd"        "Week9_vignette.Rmd"
```

``` r
dir(here::here())
```

```
##  [1] "_book"                    "_bookdown_files"         
##  [3] "_bookdown.yml"            "_output.yml"             
##  [5] "00-GitHub_files"          "00-GitHub.md"            
##  [7] "00-GitHub.Rmd"            "00-ReviewR_files"        
##  [9] "00-ReviewR.md"            "00-ReviewR.Rmd"          
## [11] "01-Week01_files"          "01-Week01.md"            
## [13] "01-Week01.Rmd"            "02-Week02_files"         
## [15] "02-Week02.md"             "02-Week02.Rmd"           
## [17] "03-Week03_files"          "03-Week03.md"            
## [19] "03-Week03.Rmd"            "04-Week04_files"         
## [21] "04-Week04.md"             "04-Week04.Rmd"           
## [23] "05-Week05_files"          "05-Week05.md"            
## [25] "05-Week05.Rmd"            "06-Week06_files"         
## [27] "06-Week06.md"             "06-Week06.Rmd"           
## [29] "07-Week07_files"          "07-Week07.md"            
## [31] "07-Week07.Rmd"            "08-Week08_files"         
## [33] "08-Week08.knit.md"        "08-Week08.Rmd"           
## [35] "09-Week09_files"          "09-Week09.Rmd"           
## [37] "10-Week10_files"          "10-Week10.knit.md"       
## [39] "10-Week10.Rmd"            "11-Week11_files"         
## [41] "11-Week11.knit.md"        "11-Week11.Rmd"           
## [43] "12-Week12_files"          "12-Week12.Rmd"           
## [45] "13-Week13_files"          "13-Week13.Rmd"           
## [47] "14-Week14_files"          "14-Week14.knit.md"       
## [49] "14-Week14.Rmd"            "404.html"                
## [51] "book.bib"                 "ChapterDepot.R"          
## [53] "data"                     "downloads"               
## [55] "header.html"              "index.md"                
## [57] "index.Rmd"                "js"                      
## [59] "LandGenCourse_book.rds"   "LandGenCourse_book.Rproj"
## [61] "LICENSE"                  "ListOfPackages.nb.html"  
## [63] "ListOfPackages.Rmd"       "my_plot.png"             
## [65] "output"                   "README.md"               
## [67] "render1b7c27bb24ce.rds"   "render1e541582584a.rds"  
## [69] "render23ec66f4ddd.rds"    "render2fbc1ad7f46.rds"   
## [71] "render2fd81ad75275.rds"   "render33ed6170c2e4.rds"  
## [73] "render363c1997640c.rds"   "render3894c561f3.rds"    
## [75] "render3b305196d38.rds"    "render65e32e6edd81.rds"  
## [77] "render65ea2d72427d.rds"   "rendera1ac36509ec1.rds"  
## [79] "renderb74a25ab009f.rds"   "renderbedc508fe6b8.rds"  
## [81] "renderf9011b67ac.rds"     "rsconnect"               
## [83] "search_index.json"        "style.css"               
## [85] "toc.css"                  "tutorials"               
## [87] "vignettes"                "Week1_vignette.R"        
## [89] "Week2_bonus_vignette.R"   "Week8_bonus_vignette.R"  
## [91] "Workflow.nb.html"         "Workflow.Rmd"
```

``` r
dir(system.file(package = "LandGenCourse"))
```

```
##  [1] "data"        "DESCRIPTION" "doc"         "extdata"     "help"       
##  [6] "html"        "INDEX"       "Meta"        "NAMESPACE"   "R"          
## [11] "rstudio"
```

``` r
dir(system.file("extdata", package = "LandGenCourse"))
```

```
##  [1] "BashExample.sh"                   "Colortable_LULC.csv"             
##  [3] "CSF_network.csv"                  "dataNm1.str"                     
##  [5] "dataNm10.txt"                     "dataNm2.txt"                     
##  [7] "dModels.rds"                      "EnvironmentalData_8pred.csv"     
##  [9] "ExcelTable.png"                   "Frogs_diversity_allpops.csv"     
## [11] "Index_of_functions_formatted.pdf" "myNotebook.Rmd"                  
## [13] "panel.cor.r"                      "Patch_XY_Dianthus.csv"           
## [15] "pulsatilla_genotypes.csv"         "pulsatilla_momVariables.csv"     
## [17] "pulsatilla_population.csv"        "ralu_coords_allpops.csv"         
## [19] "ralu_dc.csv"                      "RALU_Dps.csv"                    
## [21] "ralu_loci_allpops.csv"            "RALU_Site.csv"                   
## [23] "ralu.loci.csv"                    "ralu.rasters.rds"                
## [25] "RCommands.docx"                   "supplemental_R_functions.R"      
## [27] "Testfile.nb.html"                 "Testfile.Rmd"                    
## [29] "WE12_Fig1.PNG"                    "WE13_Fig1.png"                   
## [31] "WE9_Fig1.png"                     "WE9_Fig2.png"                    
## [33] "Wetlands.csv"                     "wolf_geno_samp_10000.csv"        
## [35] "WWP_environmental_data.txt"       "WWP_phenotype_data.txt"          
## [37] "WWP_SNP_genotypes.txt"
```

``` r
dir(myFile)
```

```
## character(0)
```

- `dir()`: the content of the last folder to which you navigated.
- `dir(here::here())`: the content of your project folder.
- `dir(system.file(package = "LandGenCourse"))`: the content of the package.
- `dir(system.file("extdata", package = "LandGenCourse"))`: the content of the folder 'extdata' in the package.
- `dir(myFile)`: nothing (this is a file, not a folder).

#### 3. Benchmarking file import and export options {-} 

See also Chapter 5 in "Efficient R Programming": https://csgillespie.github.io/efficientR/input-output.html

##### a. Benchmark methods for importing csv files {-}

The file 'myFile' has 2MB and thus a reasonable size to compare the speed of different import and export functions.

Let's benchmark the function `read.csv` used in Week 11. We use the function `microbenchmark` from the package `microbenchmark` to compare the speed of four different functions that can import a 'csv' file. 

Note: Here we execute each function only once to save time, typically you would set `times = 10` or so. Also, `read_csv` will print a warning about a missing column name. This is because the first column here contains the row names and does not have a column name. We'll ignore this here, as we can use the first column as an example to compare how character data are being imported.


``` r
x = myFile
microbenchmark::microbenchmark(times = 1, unit = "ms", 
          read.csv(x), readr::read_csv(x, show_col_types = FALSE), data.table::fread(x),
          rio::import(x))
```

```
## Warning in microbenchmark::microbenchmark(times = 1, unit = "ms", read.csv(x),
## : less accurate nanosecond times to avoid potential integer overflows
```

```
## Unit: milliseconds
##                                        expr        min         lq       mean
##                                 read.csv(x) 2073.90472 2073.90472 2073.90472
##  readr::read_csv(x, show_col_types = FALSE) 1764.02828 1764.02828 1764.02828
##                        data.table::fread(x)  132.43299  132.43299  132.43299
##                              rio::import(x)   89.93768   89.93768   89.93768
##      median         uq        max neval
##  2073.90472 2073.90472 2073.90472     1
##  1764.02828 1764.02828 1764.02828     1
##   132.43299  132.43299  132.43299     1
##    89.93768   89.93768   89.93768     1
```

Would it be faster if we first loaded the packages so that we could call the functions directly?

Note: you can list several independent commands on the same line by separating the with a semi-colon ';'. Also, the chunk setting `message=FALSE` here suppresses the warning message from `read_csv`. 


``` r
library(readr); library(data.table); library(rio); library(microbenchmark)

microbenchmark(times = 1, unit = "ms", 
          read.csv(x), read_csv(x, show_col_types = FALSE), fread(x), import(x))
```

```
## Unit: milliseconds
##                                 expr        min         lq       mean
##                          read.csv(x) 2091.23009 2091.23009 2091.23009
##  read_csv(x, show_col_types = FALSE) 1594.94305 1594.94305 1594.94305
##                             fread(x)   79.47608   79.47608   79.47608
##                            import(x)   82.28983   82.28983   82.28983
##      median         uq        max neval
##  2091.23009 2091.23009 2091.23009     1
##  1594.94305 1594.94305 1594.94305     1
##    79.47608   79.47608   79.47608     1
##    82.28983   82.28983   82.28983     1
```

Yes, the import was faster when the packages were already loaded.

Overall `fread` and `import` were in the order of 50 times faster than `read.csv` and `read_csv`! The two had practically the same speed, which is little surprising: for 'csv' files, `import` uses the function `fread`. 

The beauty of `import` is that it can handle a wide range of file types (and the list keeps growing): csv, xls, xlsx, html, xml, json, feather, R, RData, rda, rds, psv, tsv, sas7bdat, xpt, sav, dta, xpt, por, rec, mtp, syd, dbf, arff, dif, fwf, csv.gz, CSVY, fst, mat, ods, yml, as well as Fortan files and clipboard imports (Mac and Windows). It recognizes the file type from the extension and uses an appropriate import function.

Note: there is also a fuction `Import` in the `car` package that similarly aims to provide an easy way to import various file formats. However, `car::Import` can be very slow (slower than 'read.csv'), whereas `rio::import` is fast. 

##### b. Check handling of character data {-}

The functions differ not only in their speed but also in how they handle text data (character or factor?), missing values etc. 

The first column in 'myFile' is an ID variable that should be used as row names. Let's compare what the four methods did with this. The following code determines, for each import method, the class of the first column (IDs), and the class (or classes) of the resulting object.

Note: here we use double square brackets to subset the first column. Strictly speaking, we interpret 'gen' as a list of vectors. With a data.frame, we could also access the first column by gen[,1]. However, this would not return what we want for 'tbl' of 'data.table' objects. Always double check.


``` r
gen <- read.csv(x); c(class(gen[[1]]), class(gen))
```

```
## [1] "character"  "data.frame"
```

``` r
gen <- read_csv(x, show_col_types = FALSE); c(class(gen[[1]]), class(gen))
```

```
## [1] "character"   "spec_tbl_df" "tbl_df"      "tbl"         "data.frame"
```

``` r
gen <- fread(x); c(class(gen[[1]]), class(gen))
```

```
## [1] "character"  "data.table" "data.frame"
```

``` r
gen <- import(x); c(class(gen[[1]]), class(gen))
```

```
## [1] "character"  "data.frame"
```

- The function `read.csv` interprets any text as 'factor', the other functions use 'character' as default. Always double check!  
- All of these functions have optional arguments for specifying how each column how it should be interpreted (see help files). 
- With the functions `fread` and `import`, you can set the argument 'stringsAsFactors = TRUE' to import all text data as factors.

##### c. Binary files {-}

Binary files are not readable by users (or other software) but provide an efficient way of storing data. Let's compare file size and input/output speed between text files (csv) and different types of binary files (RData, rds, feather). We'll also export the 'csv' file so that we have it in the same location.

First we make sure an `output` folder exists in the R project:


``` r
if(!dir.exists(paste0(here::here(),"/output"))) dir.create(paste0(here::here(),"/output"))
```


``` r
gen <- import(myFile)

export(gen, file.path(here::here(), "output", "gen.csv"))
save(gen, file=file.path(here::here(), "output", "gen.RData"))
saveRDS(gen, file=file.path(here::here(), "output", "gen.rds"))
export(gen, file=file.path(here::here(), "output", "gen.feather"))

c(csv=file.size(file.path(here::here(), "output", "gen.csv")),
  RData=file.size(file.path(here::here(), "output", "gen.RData")),
  rds=file.size(file.path(here::here(), "output", "gen.rds")),
  feather=file.size(file.path(here::here(), "output", "gen.feather")))/10^6
```

```
##      csv    RData      rds  feather 
## 2.002491 0.352069 0.352041 3.681058
```

- The 'csv' file is 2 MB (first row). 
- The R binary files 'RData' and 'rds' are much smaller! 
- In contrast, the 'feather' file (last row) is twice as large here than the 'csv' file, and more than 10 times larger than 'rds'! 

Let's benchmark the import again. We can use the function `import` for all of them. This is so fast that we can actually do it 10 times.


``` r
microbenchmark(times = 10, unit = "ms", 
          csv= rio::import(file.path(here::here(), "output", "gen.csv")),
          RData=rio::import(file.path(here::here(), "output", "gen.RData"),
                            trust=TRUE),
          rds=rio::import(file.path(here::here(), "output", "gen.rds"), 
                          trust=TRUE),
          feather=rio::import(file.path(here::here(), "output", "gen.feather")))
```

```
## Unit: milliseconds
##     expr        min         lq       mean     median         uq        max
##      csv  104.12069  104.79666  106.58840  106.90076  107.22988  109.69181
##    RData   16.93124   17.30704   17.77163   17.58394   18.21482   19.20173
##      rds   16.73419   16.89770   17.45076   17.20237   18.14775   18.57882
##  feather 3137.09229 3199.62942 3558.84196 3501.99768 3879.64985 4221.74134
##  neval cld
##     10  a 
##     10  a 
##     10  a 
##     10   b
```

Look at the column 'mean'. Importing any of the binary files was at least twice as fast as importing the 'csv' file with the underlying function `fread` (which was already 50 times faster than `read.csv`). 

Here's my recommendation for saving R objects/data efficiently:

- If object is not in tabular form: rds (can store any R object)
- If storage space is most important: rds 
- If portability with Python is important: feather
- If being able to read text file is important: csv

Note: the developer of `feather` does not recommend using it for long-term data storage since its stability with future updates to R or Python can't be guaranteed: https://github.com/wesm/feather/issues/183

Why 'rds' and not 'RData'? In practice, the main advantage of 'rds' is convenience when importing data. 

- with 'readRDS', you can directly assign the imported data to an object, and thus choose the object name.
- with 'load', you have to do this in two steps. When using 'load', the loaded object will inherit the name from the object that was stored. 


``` r
# Let's delete any copy of 'gen' from the workspace:
rm(gen)

# Create object 'myData' in a single step from 'rds' file:
myData <- readRDS(file.path(here::here(), "output", "gen.rds"))

# Two steps when importing 'RData': first, load the stored object:
load(file.path(here::here(), "output", "gen.RData")) 
# then assign to the new object 'myData':
myData <- gen
```

Note that when you use `load`, the object name is NOT taken from the file name! This means that you may not know what object you are loading, if the object and file names are different.

Let's test this. Here we save the object 'gen' in file 'gen2.RData', then load it.


``` r
# Export 'gen' to a file with a different name 'gen2.RData':
save(gen, file=file.path(here::here(), "output", "gen2.RData"))
rm(gen)

# Load:
load(file.path(here::here(), "output", "gen2.RData")) 

# What is the name of the loaded object?
exists("gen")
```

```
## [1] TRUE
```

``` r
exists("gen2")
```

```
## [1] FALSE
```

We see that an object 'gen' exists (TRUE), but an object 'gen2' does not exist (FALSE). The name of the loaded object is thus 'gen'. 

##### d. Should you save your R workspace? {-}

When you close RStudio, you may be asked whether you want to save your workspace. What happens when you do this, and should you do so?

- When you save your workspace, all objects from the workspace are saved in one binary file '.RData' (unless you provide a name, like 'myWorkspace.Rdata').
- This may result in a large file!
- There are other downsides: you may accidentally overwrite an object. And your code will not be portable because it depends on a copy of the workspace.
- The general recommendation is to NOT save your workspace, but save your dataset and your R scripts (or Notebooks). This means that you can always recreate all the objects needed.

Also, in the vein of reproducible research, do not save multiple copies of your data set. Instead:

- Save the raw data (with the original file name and extension, e.g. if you downloaded it from the internet - this will help identify the source)
- Save all the data manipulation steps (such as checking for errors, excluding rows, recoding variables, etc.) in an R Notebook and document them (what you are doing, why and when).
- Save the 'clean' dataset (result from data manipulation), preferably as a binary file (especially if it is large). Keep only one copy (you can always recreate it).
- Save your data analysis in another R Notebook (or multiple) that starts with importing the 'clean' dataset. If your code runs quickly, this is sufficient to recreate your results.
- If your code takes a lot of computation time, you may want to export the results (see above).
- Backup your data and scripts (R Notebooks)!
- Use version control for your script (R Notebooks)! See Chapter 0 video 'Version Control 101'. The simplest is to include version numbers in the file name: 'myNotebook_v7.Rmd'. A better way is to use e.g. GitHub.

##### e. Compile your functions {-}

A recommended way of keeping your code tidy is to write functions for anything you will do more than once. 

- Collect your functions in a separate file (R script), e.g. `myFunctions.R`.
- At the beginning of your R Notebook, source the R script with the functions with `source("myFunctions.R")`.
- This will make your code much shorter and thus easier to read.
- If you need to change some code inside your function, you only have to change it in one place, hence there is less risk of mistakes.
- Use some kind of version control for your functions file so that you can go back to an older version, and you always know which is the current version.

To further speed up your code, you can compile your function with 'cmpfun':


``` r
myFunction <- function() {
    sum(rnorm(1000))/1000
}
myFunction.cmp <- compiler::cmpfun(myFunction)

microbenchmark::microbenchmark(myFunction(), myFunction.cmp())
```

```
## Unit: microseconds
##              expr    min      lq     mean  median      uq      max neval cld
##      myFunction() 29.315 30.2785 42.55103 30.9755 31.4675 1176.741   100   a
##  myFunction.cmp() 29.479 30.2170 30.89145 30.6885 31.1805   46.043   100   a
```

**Question**: Which of the following times are most different between the uncompiled and the compiled versions of this simple function? 

- **min**: minimum time across 'neval' replicates
- **Quartiles**: 'lq' = lower quartile (25%), 'median', 'uq' = upper quartile (75%)
- **mean**: mean time across 'neval' replicates
- **max**: maximum time across 'neval' replicates

In this case, compiling mainly reduced the duration of the longest 25% runs (with longer times than the 75% quartile), which brought down the mean processing time.

#### 4. Profiling your code {-}

##### a. Named chunks {-}

An simple way to identify parts of your code that may be slow is to:

- Name each chunk in your R Notebook
- Knit the notebook
- Monitor the R markdown pane while the notebook is knitted: which chunks seem to take a lot of time?

To name a chunk, click on the wheel symbol at the top right of the grey chunk area and enter a one-word name. 

Here's an example of a named chunk: the name 'myChunkName' has been added in the curly brackets `{r, myChunkName}`. You can add a name manually in the same way.



More generally, this is where chunk options are added in the R Notebook. Here's a long list of chunk options: https://yihui.name/knitr/options/

##### b. Profiling some lines of code {-}

RStudio has a built-in menu for profiling. Check it out:

- Select the five lines of code below, from `dat <- data.frame(` until `lm(y ~ x, data=dat)`
- In RStudio's menu, click on 'Profile' > 'Profile Selected Line(s)'.



You can achieve the same with a call to the funciton 'profvis' of the 'profvis' package:


``` r
profvis::profvis({
  dat <- data.frame(
       x = rnorm(5e5), 
       y = rnorm(5e5))   
  mean(dat$x)
  with(dat, cor(x, y))
  lm(y ~ x, data=dat)
})
```

```{=html}
<div class="profvis html-widget html-fill-item" id="htmlwidget-8200f0838a0312caa364" style="width:100%;height:600px;"></div>
<script type="application/json" data-for="htmlwidget-8200f0838a0312caa364">{"x":{"message":{"prof":{"time":[1,2,3,3,4,4,4,4,5,5,6,6],"depth":[1,1,2,1,4,3,2,1,2,1,2,1],"label":["rnorm","rnorm","cor","local","anyDuplicated.default","[.data.frame","na.omit.data.frame","model.frame.default","lm.fit","lm","lm.fit","lm"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null],"memalloc":[169.4871673583984,173.3018646240234,173.3064880371094,173.3064880371094,209.7351303100586,209.7351303100586,209.7351303100586,209.7351303100586,230.7192306518555,230.7192306518555,249.7931518554688,249.7931518554688],"meminc":[0,3.814697265625,0.0046234130859375,0,36.42864227294922,0,0,0,20.98410034179688,0,19.07392120361328,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null]},"interval":10,"files":[],"prof_output":"/var/folders/xf/gplbp61n2dx53pwn1m8rcgj00000gn/T//Rtmpopkiw4/filec37873370bc5.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
```

The results will be opened in a 'Profile' tab. The upper part has two tabs (the lower part is less intuitive to interpret, we'll ignore it here):

- **Flame Graph**: this plots horizontal bars indicating the amount of memory and time used by each line of code, in the original order of the code.
- **Data**: code is sorted by resource use, with the most 'costly' line `qqnorm` at the top of the list. You can click on the triangle before each line to see more detail.

The results may depend on the speed of your computer. Check the sample interval at the bottom of the 'Profile' tab: time was estimated by observing every 10 milliseconds what the computer was doing. Some lines lines must have been too fast to be recorded.

##### c. Converting an R Notebook to an R script {-}

Unfortunately, we can't profile an entire R Notebook (as far as I know). However, we can extract the R code as a script file, then profile the script file. 

Copy an R Notebook file (`Testfile.Rmd`) to the downloads folder. This is just so that we have an example of an `.Rmd` file to extract the code from.



``` r
file.copy(from=system.file("extdata", "Testfile.Rmd", package = "LandGenCourse"),
                     to=file.path(here::here(), "downloads", "Testfile.Rmd"))
```

```
## [1] FALSE
```

Extract the R code from the R Notebook and save it in a script file `Testfile.R` in the downloads folder. You may adapt the infile and outfile to extract the code from any R Notebook saved in your project. 



Let's open the two files and compare them. Note that `file.show` opens a simple text version, whereas `file.edit` shows the colored versions commonly displayed by the RStudio editor.



**Question**: Compare the two files (they should be open, check the tabs of the source pane). 

- How are the chunks from the Notebook file divided in the new script file? 
- What happened to the titles and text?
- What about the header information?

##### d. Profiling an R script {-}

We can now use the function `source` to read and execute the code in the R script. We use the function `Rprof` to start and stop the profiling.



With the function `summaryRprof`, we get a summary by function. First, let's check the total time spent executing the code, and the sample interval (in seconds). Note that profiling works in this way: as the code is executed, R checks at regular time intervals which function is being executed at that very moment. The `summaryRprof` function then tabulates the number of intervals, and thus the time used, by function. 


``` r
#summaryRprof(here::here("downloads/Rprof.out"))[c("sampling.time", "sample.interval")]	
```
The attribute `$by.total` only lists the top-level functions called (not any functions called internally by that top-level function). 


``` r
#summaryRprof(here::here("downloads/Rprof.out"))$by.total
```

Note: The functions are listed by total time, in decreasing order. The two functions `eval` and `source`, are related to evaluating (running) the code from the sourced R script file.

- **total.time**: total time spent executing the function (including functions called by it)
- **total.pct**: (ignore this) percent time spent executing the function (including functions called by it)
- **self.time**: total time spent executing the function itself (excluding functions called by it)
- **self.pct**: percent time spent executing the function itself (excluding functions called by it)


If you need to see a summary with more detail, `$by.self` lists each function used, even internal functions. 

If you want to profile memory use rather than processing time, see here: https://developer.r-project.org/memory-profiling.html


#### 5. Creating and executing a Bash R script {-}

##### a. Run R script directly in the Terminal {-}

Now that we have a stand-alone R script 'myScript.R', we can run it from the command line in the terminal (shell). After navigating to the correct folder, type:

`Rscript myScript.R`

This will source the file and execute the R code. 

- Numerical output will be printed in the termminal. Here, a random number should be returned.
- Best include code in your R script to export any R objects, data tables or figures that you want to retain. These will be saved in the same folder as the R script (unless you specify file paths).

##### b. Create a Bash R script {-}

If you want to run your code on a node or cluster, you may need to take this one step further and include the R code in a bash Rscript. In a bash script, you can add bash commands that govern resource use to submit a job to a node or cluster. Bash scripts are the way of giving instructions to the scheduler of the cluster (e.g. SLURM) for how to manage input and output files.

To execute our R script 'myScript.R' as a Bash script, we need to add a few lines.

- The 'shebang' line `#!/bin/bash` that tells the computer that this is a Bash script, and where to find Bash. Note that here the hashtag symbol does NOT mean that the line is commented out (this line is Unix code, not R code).
- The line `R --slave << EOF` that declares the rest of the file (until `EOF`) as R code.
- The end of file `EOF` marker.

Let's modify the previous code and write it into a Bash file. As an additional challenge, our code contains two figures, which won't be written anywhere unless we change the code to write them into a file:

- We create a graphics file 'my_plot.png' that is 800 pixels wide and 400 pixels high. 
- With `par(mfrow=c(1,2))`, we specify that the two plots should be plotted side-by-side. Then we create the plots. 
- We close the graphics device (png file) with `dev.off`. 

Note: We use single quotes for the file name here, 'my_plot.png', as they are nested within a set of double quotes. R pretty much considers single and double quotes as synonyms, which allows us to nest them either way: '""' or "''".


``` r
myPath <- file.path(here::here(), "output/myBashScript.sh")
fileConn <- file(myPath)
writeLines(c("#!/bin/bash",
             "R --slave << EOF",
             "x <- rnorm(100)",
             "mean(x)",
             "png('my_plot.png', height = 400, width = 800)",
             "par(mfrow=c(1,2))", 
             "hist(x)", 
             "qqnorm(x)",
             "dev.off()",
             "EOF"), fileConn)
close(fileConn)
file.show(myPath)
```

##### c. Executing a Bash R script {-}

On Mac / Unix / Linux, this is straight-forward:

1. **Open terminal**:
    - From the RStudio menu, select 'Tools' > 'Terminal' > 'New Terminal'. This will open an Terminal tab in RStudio. Alternatively, you could select 'Tools' > 'Shell' to open a Shell in a new window outside RStudio.
    - Check the prompt: it should start with the name of your computer, then a colon, then the name of your project folder, then your use name followed '$'.
2. **Navigate to Bash file**:    
    - Enter `ls` to list the content of the project folder.
    - Enter `cd output` to change directory to the subfolder 'output'.
    - Repeat `ls` to list the content of the 'output' folder. The Bash script 'myBashScript.sh' should be listed there.
3. **Execute Bash file**:
    - Enter `chmod +x myBashScript.sh` to change file permission for the script.
    - Enter `./myBashScript.sh` to execute the script.
4. **Find output**:
    - The output from `mean(x)` is printed in the terminal, it should look like this: `[1] -0.07731751`.
    - This is followed 'null device' and the number 1, which tells us that a graphics device has been closed.
    - Enter `ls` to list the content of the project folder. The graphics file 'my_plot.png' should now be listed. 

Use R again to open the graphics file (the code here first checks whether the file exists):


``` r
myPNG <- file.path(here::here(), "output/my_plot.png")
if(file.exists(myPNG))
{
  file.show(myPNG)
}
```

##### d. Moving to a node or cluster? {-}

The example bash file and advice in this section have been provided by Hossam Abdel Moniem, thanks!

Here's an annotated example of a bash file that contains instructions for submitting a job to a single node (a single machine with multiple/many cores). 

Note: A copy of the file 'BashExample.sh' should also have been copied into the downloads folder inside your project folder.   


``` r
writeLines(readLines(system.file("extdata", "BashExample.sh", package = "LandGenCourse")))
```

```
## #!/bin/bash 
## 
## #SBATCH --nodes=1                                 # Number of Nodes 
## #SBATCH --mail-type=ALL                           # Mail events (NONE, BEGIN, END, FAIL, ALL)
## #SBATCH --mail-user=myEmail@gmail.com             # Where to send mail
## #SBATCH --ntasks=1                                # Run a single task
## #SBATCH --cpus-per-task=24                        # Number of CPU cores per task
## #SBATCH --mem-per-cpu=8000			  # allocated memory for the task
## #SBATCH -p nodename				  # name of cluster node
## #SBATCH --requeue                                 # Allow the job to be requeued
## #SBATCH -e myJob.err 				  # File to which STDERR will be written 
## #SBATCH -o myJob.out 				  # File to which STDOUT will be written 
## #SBATCH -J myJob 				  # Job name 
## 
## module load R/MS3.4.1				  # call a preinstalled module(program) on the cluster
## 
## Rscript connect_calc_25.R			  # Run the R script in bash 
## 
```

Instead of including the R code directly in the bash file, the last line here executes an R script with the `Rscript` command.

Notice the second-last line. Obviously, on the node, R and any relevant packages need to be pre-installed. Different users may need different configurations (different packages or versions) installed, hence each installation has a name, which needs to be specified in the bash script.

Note: Make sure that all packages that you need (and their dependencies), as well as the package `unixtools`, have been installed on the node or cluster (i.e., they are part of the installation you will be using). Install `unixtools` with: `install.packages("unixtools",,"http://rforge.net/")`

Further reading:

- Bash script tutorial: https://ryanstutorials.net/bash-scripting-tutorial/
- Scheduling a job with SLURM commands: https://www.rc.fas.harvard.edu/resources/documentation/convenient-slurm-commands/

##### e. Store your session info and package versions {-}

If you use any R packages, load them at the beginning of your script file with `library`. Make sure the packages are installed on the system where you will be running the Bash R script.

A big issue with R is that package updates may make your code break. At least at the end of any project (such as the analyses for a manuscript), save your session information.

Here we use the function `session_info` from the `devtools` package (preferred over the R base function `sessionInfo`). We store the information as an object, 'Session', of class 'session_info' that has two list elements:

- **platform**: Information about R version, your computer's operating system, etc.
- **packages**: List of all packages, including their version, installation date and repository (CRAN, Github etc.). All packages that are currently loaded will be marked with an asterisk.

Platform:


``` r
Session <- devtools::session_info()
Session$platform
```

```
##  setting  value
##  version  R version 4.5.2 (2025-10-31)
##  os       macOS Sequoia 15.7.3
##  system   aarch64, darwin20
##  ui       X11
##  language (EN)
##  collate  en_US.UTF-8
##  ctype    en_US.UTF-8
##  tz       Europe/Zurich
##  date     2026-01-21
##  pandoc   3.4 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/ (via rmarkdown)
##  quarto   1.6.42 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/quarto
```

Packages: here we display only the first six lines, as the list may be long.


``` r
head(Session$packages)
```

```
##  package    * version  date (UTC) lib source
##  abind        1.4-8    2024-09-12 [1] CRAN (R 4.5.0)
##  ade4         1.7-23   2025-02-14 [1] CRAN (R 4.5.0)
##  adegenet     2.1.11   2025-02-06 [1] CRAN (R 4.5.0)
##  ape          5.8-1    2024-12-16 [1] CRAN (R 4.5.0)
##  arrow        22.0.0.1 2025-12-23 [1] CRAN (R 4.5.2)
##  assertthat   0.2.1    2019-03-21 [1] CRAN (R 4.5.0)
## 
##  [1] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library
```

Exporting the session information is a bit tricky because 'Session' is not in tabular format, and what we want to export is the formatted output, not the object itself. 

Also, we will add a time stamp to the file name. This achieves two goals: avoid overwriting earlier files, and keep a record of the date of the session information.

- `capture.output`: captures the output of a function (here: `devtools::session_info()`)
- `writeLines`: writes the captured output into a file, here a text file.
- `Sys.Date`: returns the current date. Here we specify the format as "%Y-%m-%d", i.e., 'Year-month-day'.



More advanced ways for handling the problem of package versions to make sure you can run your code in the future without compatibility issues include:

- Package 'packrat': bundle all your current package version. This can become quite large.
- Package 'checkpoint': access daily snapshots of CRAN for any given date. Note that checkpoint only covers packages from CRAN, not other repositories like GitHub. 

More on the topic: https://timogrossenbacher.ch/2017/07/a-truly-reproducible-r-workflow/

##### f. Potential Windows issues {-}

Check that Bash is installed and ready to use with RStudio:

- In the RStudio menu, select 'Tools' > 'Global options' > 'Terminal'. 
- Make sure some form of Bash (e.g., Git Bash) is listed under 'Shell: New terminals open with:'.

If this does not work, try installing 'Git for Windows', which will also install Bash: http://neondataskills.org/setup/setup-git-bash-R


Here's a detailed multi-part tutorial on running R from the command line, with R scripts and Bash R scripts, with some additional information on Windows:

- **Introduction**: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/01-introduction.Rmd
- **Batch mode**: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/02-batch-mode.Rmd
- **Executing R scripts**: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/03-rscript.Rmd
- **Bash R scripts**: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/04-shell-script.Rmd
- **Redirection**: https://github.com/gastonstat/tutorial-R-noninteractive/blob/master/05-redirection.Rmd

#### 6. Parallelizing code {-}

Note that the following issues may create problems when developing Bash R scripts on Windows that you want to run e.g. on a Linux cluster or another Unix-type system;

- File paths are different, and system files are stored in a different place.
- End-of-line symbols are different

##### a. Replace 'lapply' by 'mclapply' with package 'parallel' {-}

Note: while this will run on a Windows without causing an error, it will only be faster on Mac / Unix.

With the package 'parallel', it is really easy to use all cores of your local machine (as long as you are on a Mac / Unix / Linux system). Let's check the number of cores available:


``` r
library(parallel)
detectCores()
```

```
## [1] 10
```

**Question**: How many cores does your machine have?

Note: Here we check whether the operating system is 'Windows', in which case we set `nCores = 1`. This means that we will use all cores on Mac or Linux, but only one core on Windows. This is to avoid problems on Windows machines.  


``` r
nCores <- detectCores()
if(Sys.info()[['sysname']]=="Windows") nCores = 1
nCores
```

```
## [1] 10
```

- Code your analysis with `lapply` (and related functions).
- Replace `lapply` by `mclapply` (and related functions). Use the argument `mc.cores=detectCores()` to automatically detect the number of cores in your machine. 

NOTE: April 2021: package build does not work with multiple cores, changing nCores to 1.


``` r
x <- gen[,-1]
m1 <- lapply(x, mean, na.rm=TRUE)
#m2 <- mclapply(x, mean, na.rm=TRUE, mc.cores=nCores)  # Use this line when running the code yourself
m2 <- mclapply(x, mean, na.rm=TRUE, mc.cores=1)        # Replace this line with the previous line
```

Let's benchmark four ways of calculating the mean of each column in our example data set 'gen' with 94 rows and 10,000 columns (SNPs):

- **Method 1**: The dedicated function `colMeans` from base R. 
- **Method 2**: A `for` loop.
- **Method 3**: Vectorization with `lapply`
- **Method 4**: Multi-core with `mclapply`.


``` r
method1 <- function(x) {colMeans(x, na.rm=TRUE)}
method2 <- function(x) {for(i in 1:ncol(x)) mean(x[,i], na.rm=TRUE)}
method3 <- function(x) {lapply(x, mean, na.rm=TRUE)}
#method4 <- function(x) {mclapply(x, mean, na.rm=TRUE, mc.cores=nCores)} # Use this line when running the code yourself
method4 <- function(x) {mclapply(x, mean, na.rm=TRUE, mc.cores=1)}  # Replace this line with the previous line

microbenchmark::microbenchmark(times = 10, unit = "ms",
                               method1(x), method2(x), method3(x), method4(x))
```

```
## Unit: milliseconds
##        expr       min        lq      mean    median        uq       max neval
##  method1(x)  26.86086  27.51916  28.33970  28.00536  29.14087  30.16805    10
##  method2(x) 153.91236 160.35403 185.18076 166.44413 217.43903 230.34165    10
##  method3(x)  40.91747  41.06015  54.35233  43.15879  49.61135 100.57525    10
##  method4(x)  40.76876  41.44592  43.36019  42.54568  43.36632  49.56289    10
##  cld
##  a  
##   b 
##    c
##  a c
```

**Question**: Compare the mean

- Which method was the fastest? Can you explain this?
- Which method was slowest?
- Was `mclapply` faster than `lapply` in this example? 

Note: Obviously you might only expect to see a gain in speed if nCores > 1.

##### b. Replace 'for' by 'foreach' with package 'doParallel' {-}

On Windows, it is easier to use the package 'doParallel' with the function 'foreach'. Here's a detailed introduction: http://127.0.0.1:26758/help/library/doParallel/doc/gettingstartedParallel.pdf.	

- Use `makeCluster` to specify the number of cores to be used. The default is half the number of cores. 
- Check number of clusters by printing `cl`.
- Register the cluster with `registerDoParallel`. If you omit this step, the code will not use parallel computing.

The following code is commented out to avoid problems when knitting the Notebook. You may uncomment and run it.


``` r
library(doParallel)
```

```
## Loading required package: foreach
```

```
## Loading required package: iterators
```

``` r
#cl <- makeCluster(2)
#cl
#registerDoParallel(cl)
```

Now we adapt the code in two steps:

- The code is a little different with `foreach` than with `for`, as we use a pipe-like syntax with `%do%`, which means, for each value of *i*, do the following.
- Thus, `%do%` is only a pipe operator, it does not result in parallelisation yet. 
- To make this parallel, replace `%do%` by `%dopar%`.


``` r
m1 <- for(i in 1:ncol(x)) mean(x[,i], na.rm=TRUE)
m2 <- foreach(i = 1:ncol(x)) %do% (mean(x[,i], na.rm=TRUE))
#m3 <- foreach(i = 1:ncol(x)) %dopar% (mean(x[,i], na.rm=TRUE))
```

Let's benchmark this again. We'll only do 3 replicates this time (uncomment before running this code).


``` r
#method1 <- function(x) {colMeans(x, na.rm=TRUE)}
#method2 <- function(x) {for(i in 1:ncol(x)) mean(x[,i], na.rm=TRUE)}
#method3 <- function(x) {lapply(x, mean, na.rm=TRUE)}
#method4 <- function(x) {mclapply(x, mean, na.rm=TRUE, mc.cores=nCores)}
#method5 <- function(x) {foreach(i = 1:ncol(x)) %do% (mean(x[,i], na.rm=TRUE))}
#method6 <- function(x) {foreach(i = 1:ncol(x)) %dopar% (mean(x[,i], na.rm=TRUE))}

#microbenchmark::microbenchmark(times = 3, unit = "ms",method1(x), method2(x), method3(x), method4(x), method5(x), method6(x))
```

Whether parallelisation is faster depends on the type of task and on your system. In this case, both versions that used parallelisation were actually slower than the sequential code (at least on my system). 

Of course, this will not always be the case, it may depend on:

- What type and size of computational task you are running.
- How many nodes you can use in parallel. The initial 'cost' of coordinating the task among cores may not be worth it if you only have two cores to use anyways, but with 20 cores, the gain will be higher.
- Whether all cores are in the same node, or whether you distribute the work among multiple nodes (this will increase the cost of coordination, but potentially also give you access to many more cores).



